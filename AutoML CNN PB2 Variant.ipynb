{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "# from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.integration.torch import is_distributed_trainable\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "# from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "# from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "# from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "# from dragonfly.opt.gp_bandit import CPGPBandit\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# from dragonfly import load_config\n",
    "# from dragonfly.exd.experiment_caller import CPFunctionCaller, EuclideanFunctionCaller\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "from ray.tune.schedulers import pb2\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "from optuna.multi_objective.samplers import MOTPEMultiObjectiveSampler\n",
    "# from optuna.pruners import SuccessiveHalvingPruner\n",
    "\n",
    "# from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, select_length, UCB, standardize, TV_SquaredExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set data and checkpoint locations\n",
    "# p = Path('.')\n",
    "# d = p / 'data'\n",
    "# r = p / 'ray_results'\n",
    "# l = p / 'checkpoints' / 'layers'\n",
    "# n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# # set computation location(s)\n",
    "# gpus = torch.cuda.device_count()\n",
    "# device = \"cuda:0\" if gpus else \"cpu\"\n",
    "\n",
    "# # set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# # cpu_use must be grater than zero\n",
    "# cpu_use = 1 if gpus else 0.5\n",
    "# gpu_use = 0.25 if gpus else 0\n",
    "\n",
    "# # set experiment hyperparameters\n",
    "# num_samples = 2 ** (6 if gpus else 4)\n",
    "# max_time = 10 * (4 if gpus else 1)\n",
    "# gpus_per_trial = 0.5 if gpus else 0\n",
    "\n",
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "dt = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count()\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus#int(cpus/2) if cpus > 1 else cpus\n",
    "cpu_use = 2#2 / gpus if gpus else 1\n",
    "gpu_use = 0.5#gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "# oom = 2 if gpus else 3 # order of magnitude\n",
    "num_samples = 2 / gpu_use#** oom\n",
    "max_time = 10# * oom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neuron configuration we want is dependent upon the number of layers we have, we need to work flatten the feature space a bit. We can reduce the high-dminesional setups to a slightly less high-dminesional string of base-n nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 2, 0), (9, 3, 0), (9, 4, 0), (10, 2, 0), (10, 3, 0), (10, 4, 0), (11, 2, 0), (11, 3, 0), (11, 4, 0), (12, 2, 0), (12, 3, 0), (12, 4, 0), (13, 2, 0), (13, 3, 0), (13, 4, 0), (14, 2, 0), (14, 3, 0), (14, 4, 0), (15, 2, 0), (15, 3, 0), (15, 4, 0), (16, 2, 0), (16, 3, 0), (16, 4, 0), (17, 2, 0), (17, 3, 0), (17, 4, 0), (18, 2, 0), (18, 3, 0), (18, 4, 0), (19, 2, 0), (19, 3, 0), (19, 4, 0), (20, 2, 0), (20, 3, 0), (20, 4, 0), (21, 2, 0), (21, 3, 0), (21, 4, 0), (22, 2, 0), (22, 3, 0), (22, 4, 0), (23, 2, 0), (23, 3, 0), (23, 4, 0), (24, 2, 0), (24, 3, 0), (24, 4, 0), (25, 2, 0), (25, 3, 0), (25, 4, 0), (26, 2, 0), (26, 3, 0), (26, 4, 0), (27, 2, 0), (27, 3, 0), (27, 4, 0), (28, 2, 0), (28, 3, 0), (28, 4, 0), (29, 2, 0), (29, 3, 0), (29, 4, 0), (30, 2, 0), (30, 3, 0), (30, 4, 0), (31, 2, 0), (31, 3, 0), (31, 4, 0), (32, 2, 0), (32, 3, 0), (32, 4, 0), (33, 2, 0), (33, 3, 0), (33, 4, 0), (34, 2, 0), (34, 3, 0), (34, 4, 0), (35, 2, 0), (35, 3, 0), (35, 4, 0), (36, 2, 0), (36, 3, 0), (36, 4, 0), (37, 2, 0), (37, 3, 0), (37, 4, 0), (38, 2, 0), (38, 3, 0), (38, 4, 0), (39, 2, 0), (39, 3, 0), (39, 4, 0), (40, 2, 0), (40, 3, 0), (40, 4, 0), (41, 2, 0), (41, 3, 0), (41, 4, 0), (42, 2, 0), (42, 3, 0), (42, 4, 0), (43, 2, 0), (43, 3, 0), (43, 4, 0), (44, 2, 0), (44, 3, 0), (44, 4, 0), (45, 2, 0), (45, 3, 0), (45, 4, 0), (46, 2, 0), (46, 3, 0), (46, 4, 0), (47, 2, 0), (47, 3, 0), (47, 4, 0), (48, 2, 0), (48, 3, 0), (48, 4, 0), (49, 2, 0), (49, 3, 0), (49, 4, 0), (50, 2, 0), (50, 3, 0), (50, 4, 0), (51, 2, 0), (51, 3, 0), (51, 4, 0), (52, 2, 0), (52, 3, 0), (52, 4, 0), (53, 2, 0), (53, 3, 0), (53, 4, 0), (54, 2, 0), (54, 3, 0), (54, 4, 0), (55, 2, 0), (55, 3, 0), (55, 4, 0), (56, 2, 0), (56, 3, 0), (56, 4, 0), (57, 2, 0), (57, 3, 0), (57, 4, 0), (58, 2, 0), (58, 3, 0), (58, 4, 0), (59, 2, 0), (59, 3, 0), (59, 4, 0), (60, 2, 0), (60, 3, 0), (60, 4, 0), (61, 2, 0), (61, 3, 0), (61, 4, 0), (62, 2, 0), (62, 3, 0), (62, 4, 0), (63, 2, 0), (63, 3, 0), (63, 4, 0), (64, 2, 0), (64, 3, 0), (64, 4, 0), (65, 2, 0), (65, 3, 0), (65, 4, 0), (66, 2, 0), (66, 3, 0), (66, 4, 0), (67, 2, 0), (67, 3, 0), (67, 4, 0), (68, 2, 0), (68, 3, 0), (68, 4, 0), (69, 2, 0), (69, 3, 0), (69, 4, 0), (70, 2, 0), (70, 3, 0), (70, 4, 0), (71, 2, 0), (71, 3, 0), (71, 4, 0), (72, 2, 0), (72, 3, 0), (72, 4, 0), (73, 2, 0), (73, 3, 0), (73, 4, 0), (74, 2, 0), (74, 3, 0), (74, 4, 0), (75, 2, 0), (75, 3, 0), (75, 4, 0), (76, 2, 0), (76, 3, 0), (76, 4, 0), (77, 2, 0), (77, 3, 0), (77, 4, 0), (78, 2, 0), (78, 3, 0), (78, 4, 0), (79, 2, 0), (79, 3, 0), (79, 4, 0), (80, 2, 0), (80, 3, 0), (80, 4, 0), (81, 2, 0), (81, 3, 0), (81, 4, 0), (82, 2, 0), (82, 3, 0), (82, 4, 0), (83, 2, 0), (83, 3, 0), (83, 4, 0), (84, 2, 0), (84, 3, 0), (84, 4, 0), (85, 2, 0), (85, 3, 0), (85, 4, 0), (86, 2, 0), (86, 3, 0), (86, 4, 0), (87, 2, 0), (87, 3, 0), (87, 4, 0), (88, 2, 0), (88, 3, 0), (88, 4, 0), (89, 2, 0), (89, 3, 0), (89, 4, 0), (90, 2, 0), (90, 3, 0), (90, 4, 0), (91, 2, 0), (91, 3, 0), (91, 4, 0), (92, 2, 0), (92, 3, 0), (92, 4, 0), (93, 2, 0), (93, 3, 0), (93, 4, 0), (94, 2, 0), (94, 3, 0), (94, 4, 0), (95, 2, 0), (95, 3, 0), (95, 4, 0), (96, 2, 0), (96, 3, 0), (96, 4, 0), (97, 2, 0), (97, 3, 0), (97, 4, 0), (98, 2, 0), (98, 3, 0), (98, 4, 0), (99, 2, 0), (99, 3, 0), (99, 4, 0), (100, 2, 0), (100, 3, 0), (100, 4, 0), (101, 2, 0), (101, 3, 0), (101, 4, 0), (102, 2, 0), (102, 3, 0), (102, 4, 0), (103, 2, 0), (103, 3, 0), (103, 4, 0), (104, 2, 0), (104, 3, 0), (104, 4, 0), (105, 2, 0), (105, 3, 0), (105, 4, 0), (106, 2, 0), (106, 3, 0), (106, 4, 0), (107, 2, 0), (107, 3, 0), (107, 4, 0), (108, 2, 0), (108, 3, 0), (108, 4, 0), (109, 2, 0), (109, 3, 0), (109, 4, 0), (110, 2, 0), (110, 3, 0), (110, 4, 0), (111, 2, 0), (111, 3, 0), (111, 4, 0), (112, 2, 0), (112, 3, 0), (112, 4, 0), (113, 2, 0), (113, 3, 0), (113, 4, 0), (114, 2, 0), (114, 3, 0), (114, 4, 0), (115, 2, 0), (115, 3, 0), (115, 4, 0), (116, 2, 0), (116, 3, 0), (116, 4, 0), (117, 2, 0), (117, 3, 0), (117, 4, 0), (118, 2, 0), (118, 3, 0), (118, 4, 0), (119, 2, 0), (119, 3, 0), (119, 4, 0), (120, 2, 0), (120, 3, 0), (120, 4, 0), (121, 2, 0), (121, 3, 0), (121, 4, 0), (122, 2, 0), (122, 3, 0), (122, 4, 0), (123, 2, 0), (123, 3, 0), (123, 4, 0), (124, 2, 0), (124, 3, 0), (124, 4, 0), (125, 2, 0), (125, 3, 0), (125, 4, 0), (126, 2, 0), (126, 3, 0), (126, 4, 0), (127, 2, 0), (127, 3, 0), (127, 4, 0), (128, 2, 0), (128, 3, 0), (128, 4, 0), (129, 2, 0), (129, 3, 0), (129, 4, 0), (130, 2, 0), (130, 3, 0), (130, 4, 0), (131, 2, 0), (131, 3, 0), (131, 4, 0), (132, 2, 0), (132, 3, 0), (132, 4, 0), (133, 2, 0), (133, 3, 0), (133, 4, 0), (134, 2, 0), (134, 3, 0), (134, 4, 0), (135, 2, 0), (135, 3, 0), (135, 4, 0), (136, 2, 0), (136, 3, 0), (136, 4, 0), (137, 2, 0), (137, 3, 0), (137, 4, 0), (138, 2, 0), (138, 3, 0), (138, 4, 0), (139, 2, 0), (139, 3, 0), (139, 4, 0), (140, 2, 0), (140, 3, 0), (140, 4, 0), (141, 2, 0), (141, 3, 0), (141, 4, 0), (142, 2, 0), (142, 3, 0), (142, 4, 0), (143, 2, 0), (143, 3, 0), (143, 4, 0), (144, 2, 0), (144, 3, 0), (144, 4, 0), (145, 2, 0), (145, 3, 0), (145, 4, 0), (146, 2, 0), (146, 3, 0), (146, 4, 0), (147, 2, 0), (147, 3, 0), (147, 4, 0), (148, 2, 0), (148, 3, 0), (148, 4, 0), (149, 2, 0), (149, 3, 0), (149, 4, 0), (150, 2, 0), (150, 3, 0), (150, 4, 0), (151, 2, 0), (151, 3, 0), (151, 4, 0), (152, 2, 0), (152, 3, 0), (152, 4, 0), (153, 2, 0), (153, 3, 0), (153, 4, 0), (154, 2, 0), (154, 3, 0), (154, 4, 0), (155, 2, 0), (155, 3, 0), (155, 4, 0), (156, 2, 0), (156, 3, 0), (156, 4, 0), (157, 2, 0), (157, 3, 0), (157, 4, 0), (158, 2, 0), (158, 3, 0), (158, 4, 0), (159, 2, 0), (159, 3, 0), (159, 4, 0), (160, 2, 0), (160, 3, 0), (160, 4, 0), (161, 2, 0), (161, 3, 0), (161, 4, 0), (162, 2, 0), (162, 3, 0), (162, 4, 0), (163, 2, 0), (163, 3, 0), (163, 4, 0), (164, 2, 0), (164, 3, 0), (164, 4, 0), (165, 2, 0), (165, 3, 0), (165, 4, 0), (166, 2, 0), (166, 3, 0), (166, 4, 0), (167, 2, 0), (167, 3, 0), (167, 4, 0), (168, 2, 0), (168, 3, 0), (168, 4, 0), (169, 2, 0), (169, 3, 0), (169, 4, 0), (170, 2, 0), (170, 3, 0), (170, 4, 0), (171, 2, 0), (171, 3, 0), (171, 4, 0), (172, 2, 0), (172, 3, 0), (172, 4, 0), (173, 2, 0), (173, 3, 0), (173, 4, 0), (174, 2, 0), (174, 3, 0), (174, 4, 0), (175, 2, 0), (175, 3, 0), (175, 4, 0), (176, 2, 0), (176, 3, 0), (176, 4, 0), (177, 2, 0), (177, 3, 0), (177, 4, 0), (178, 2, 0), (178, 3, 0), (178, 4, 0), (179, 2, 0), (179, 3, 0), (179, 4, 0), (180, 2, 0), (180, 3, 0), (180, 4, 0), (181, 2, 0), (181, 3, 0), (181, 4, 0), (182, 2, 0), (182, 3, 0), (182, 4, 0), (183, 2, 0), (183, 3, 0), (183, 4, 0), (184, 2, 0), (184, 3, 0), (184, 4, 0), (185, 2, 0), (185, 3, 0), (185, 4, 0), (186, 2, 0), (186, 3, 0), (186, 4, 0), (187, 2, 0), (187, 3, 0), (187, 4, 0), (188, 2, 0), (188, 3, 0), (188, 4, 0), (189, 2, 0), (189, 3, 0), (189, 4, 0), (190, 2, 0), (190, 3, 0), (190, 4, 0), (191, 2, 0), (191, 3, 0), (191, 4, 0), (192, 2, 0), (192, 3, 0), (192, 4, 0), (193, 2, 0), (193, 3, 0), (193, 4, 0), (194, 2, 0), (194, 3, 0), (194, 4, 0), (195, 2, 0), (195, 3, 0), (195, 4, 0), (196, 2, 0), (196, 3, 0), (196, 4, 0), (197, 2, 0), (197, 3, 0), (197, 4, 0), (198, 2, 0), (198, 3, 0), (198, 4, 0), (199, 2, 0), (199, 3, 0), (199, 4, 0), (200, 2, 0), (200, 3, 0), (200, 4, 0), (201, 2, 0), (201, 3, 0), (201, 4, 0), (202, 2, 0), (202, 3, 0), (202, 4, 0), (203, 2, 0), (203, 3, 0), (203, 4, 0), (204, 2, 0), (204, 3, 0), (204, 4, 0), (205, 2, 0), (205, 3, 0), (205, 4, 0), (206, 2, 0), (206, 3, 0), (206, 4, 0), (207, 2, 0), (207, 3, 0), (207, 4, 0), (208, 2, 0), (208, 3, 0), (208, 4, 0), (209, 2, 0), (209, 3, 0), (209, 4, 0), (210, 2, 0), (210, 3, 0), (210, 4, 0), (211, 2, 0), (211, 3, 0), (211, 4, 0), (212, 2, 0), (212, 3, 0), (212, 4, 0), (213, 2, 0), (213, 3, 0), (213, 4, 0), (214, 2, 0), (214, 3, 0), (214, 4, 0), (215, 2, 0), (215, 3, 0), (215, 4, 0), (216, 2, 0), (216, 3, 0), (216, 4, 0), (217, 2, 0), (217, 3, 0), (217, 4, 0), (218, 2, 0), (218, 3, 0), (218, 4, 0), (219, 2, 0), (219, 3, 0), (219, 4, 0), (220, 2, 0), (220, 3, 0), (220, 4, 0), (221, 2, 0), (221, 3, 0), (221, 4, 0), (222, 2, 0), (222, 3, 0), (222, 4, 0), (223, 2, 0), (223, 3, 0), (223, 4, 0), (224, 2, 0), (224, 3, 0), (224, 4, 0), (225, 2, 0), (225, 3, 0), (225, 4, 0), (226, 2, 0), (226, 3, 0), (226, 4, 0), (227, 2, 0), (227, 3, 0), (227, 4, 0), (228, 2, 0), (228, 3, 0), (228, 4, 0), (229, 2, 0), (229, 3, 0), (229, 4, 0), (230, 2, 0), (230, 3, 0), (230, 4, 0), (231, 2, 0), (231, 3, 0), (231, 4, 0), (232, 2, 0), (232, 3, 0), (232, 4, 0), (233, 2, 0), (233, 3, 0), (233, 4, 0), (234, 2, 0), (234, 3, 0), (234, 4, 0), (235, 2, 0), (235, 3, 0), (235, 4, 0), (236, 2, 0), (236, 3, 0), (236, 4, 0), (237, 2, 0), (237, 3, 0), (237, 4, 0), (238, 2, 0), (238, 3, 0), (238, 4, 0), (239, 2, 0), (239, 3, 0), (239, 4, 0), (240, 2, 0), (240, 3, 0), (240, 4, 0), (241, 2, 0), (241, 3, 0), (241, 4, 0), (242, 2, 0), (242, 3, 0), (242, 4, 0)]\n",
      "[(9, 2, 0), (9, 3, 0), (9, 4, 0), (10, 2, 0), (10, 3, 0), (10, 4, 0), (11, 2, 0), (11, 3, 0), (11, 4, 0), (12, 2, 0), (12, 3, 0), (12, 4, 0), (13, 2, 0), (13, 3, 0), (13, 4, 0), (14, 2, 0), (14, 3, 0), (14, 4, 0), (15, 2, 0), (15, 3, 0), (15, 4, 0), (16, 2, 0), (16, 3, 0), (16, 4, 0), (17, 2, 0), (17, 3, 0), (17, 4, 0), (18, 2, 0), (18, 3, 0), (18, 4, 0), (19, 2, 0), (19, 3, 0), (19, 4, 0), (20, 2, 0), (20, 3, 0), (20, 4, 0), (21, 2, 0), (21, 3, 0), (21, 4, 0), (22, 2, 0), (22, 3, 0), (22, 4, 0), (23, 2, 0), (23, 3, 0), (23, 4, 0), (24, 2, 0), (24, 3, 0), (24, 4, 0), (25, 2, 0), (25, 3, 0), (25, 4, 0), (26, 2, 0), (26, 3, 0), (26, 4, 0), (27, 2, 0), (27, 3, 0), (27, 4, 0), (28, 2, 0), (28, 3, 0), (28, 4, 0), (29, 2, 0), (29, 3, 0), (29, 4, 0), (30, 2, 0), (30, 3, 0), (30, 4, 0), (31, 2, 0), (31, 3, 0), (31, 4, 0), (32, 2, 0), (32, 3, 0), (32, 4, 0), (33, 2, 0), (33, 3, 0), (33, 4, 0), (34, 2, 0), (34, 3, 0), (34, 4, 0), (35, 2, 0), (35, 3, 0), (35, 4, 0), (36, 2, 0), (36, 3, 0), (36, 4, 0), (37, 2, 0), (37, 3, 0), (37, 4, 0), (38, 2, 0), (38, 3, 0), (38, 4, 0), (39, 2, 0), (39, 3, 0), (39, 4, 0), (40, 2, 0), (40, 3, 0), (40, 4, 0), (41, 2, 0), (41, 3, 0), (41, 4, 0), (42, 2, 0), (42, 3, 0), (42, 4, 0), (43, 2, 0), (43, 3, 0), (43, 4, 0), (44, 2, 0), (44, 3, 0), (44, 4, 0), (45, 2, 0), (45, 3, 0), (45, 4, 0), (46, 2, 0), (46, 3, 0), (46, 4, 0), (47, 2, 0), (47, 3, 0), (47, 4, 0), (48, 2, 0), (48, 3, 0), (48, 4, 0), (49, 2, 0), (49, 3, 0), (49, 4, 0), (50, 2, 0), (50, 3, 0), (50, 4, 0), (51, 2, 0), (51, 3, 0), (51, 4, 0), (52, 2, 0), (52, 3, 0), (52, 4, 0), (53, 2, 0), (53, 3, 0), (53, 4, 0), (54, 2, 0), (54, 3, 0), (54, 4, 0), (55, 2, 0), (55, 3, 0), (55, 4, 0), (56, 2, 0), (56, 3, 0), (56, 4, 0), (57, 2, 0), (57, 3, 0), (57, 4, 0), (58, 2, 0), (58, 3, 0), (58, 4, 0), (59, 2, 0), (59, 3, 0), (59, 4, 0), (60, 2, 0), (60, 3, 0), (60, 4, 0), (61, 2, 0), (61, 3, 0), (61, 4, 0), (62, 2, 0), (62, 3, 0), (62, 4, 0), (63, 2, 0), (63, 3, 0), (63, 4, 0), (64, 2, 0), (64, 3, 0), (64, 4, 0), (65, 2, 0), (65, 3, 0), (65, 4, 0), (66, 2, 0), (66, 3, 0), (66, 4, 0), (67, 2, 0), (67, 3, 0), (67, 4, 0), (68, 2, 0), (68, 3, 0), (68, 4, 0), (69, 2, 0), (69, 3, 0), (69, 4, 0), (70, 2, 0), (70, 3, 0), (70, 4, 0), (71, 2, 0), (71, 3, 0), (71, 4, 0), (72, 2, 0), (72, 3, 0), (72, 4, 0), (73, 2, 0), (73, 3, 0), (73, 4, 0), (74, 2, 0), (74, 3, 0), (74, 4, 0), (75, 2, 0), (75, 3, 0), (75, 4, 0), (76, 2, 0), (76, 3, 0), (76, 4, 0), (77, 2, 0), (77, 3, 0), (77, 4, 0), (78, 2, 0), (78, 3, 0), (78, 4, 0), (79, 2, 0), (79, 3, 0), (79, 4, 0), (80, 2, 0), (80, 3, 0), (80, 4, 0), (81, 2, 0), (81, 3, 0), (81, 4, 0), (82, 2, 0), (82, 3, 0), (82, 4, 0), (83, 2, 0), (83, 3, 0), (83, 4, 0), (84, 2, 0), (84, 3, 0), (84, 4, 0), (85, 2, 0), (85, 3, 0), (85, 4, 0), (86, 2, 0), (86, 3, 0), (86, 4, 0), (87, 2, 0), (87, 3, 0), (87, 4, 0), (88, 2, 0), (88, 3, 0), (88, 4, 0), (89, 2, 0), (89, 3, 0), (89, 4, 0), (90, 2, 0), (90, 3, 0), (90, 4, 0), (91, 2, 0), (91, 3, 0), (91, 4, 0), (92, 2, 0), (92, 3, 0), (92, 4, 0), (93, 2, 0), (93, 3, 0), (93, 4, 0), (94, 2, 0), (94, 3, 0), (94, 4, 0), (95, 2, 0), (95, 3, 0), (95, 4, 0), (96, 2, 0), (96, 3, 0), (96, 4, 0), (97, 2, 0), (97, 3, 0), (97, 4, 0), (98, 2, 0), (98, 3, 0), (98, 4, 0), (99, 2, 0), (99, 3, 0), (99, 4, 0), (100, 2, 0), (100, 3, 0), (100, 4, 0), (101, 2, 0), (101, 3, 0), (101, 4, 0), (102, 2, 0), (102, 3, 0), (102, 4, 0), (103, 2, 0), (103, 3, 0), (103, 4, 0), (104, 2, 0), (104, 3, 0), (104, 4, 0), (105, 2, 0), (105, 3, 0), (105, 4, 0), (106, 2, 0), (106, 3, 0), (106, 4, 0), (107, 2, 0), (107, 3, 0), (107, 4, 0), (108, 2, 0), (108, 3, 0), (108, 4, 0), (109, 2, 0), (109, 3, 0), (109, 4, 0), (110, 2, 0), (110, 3, 0), (110, 4, 0), (111, 2, 0), (111, 3, 0), (111, 4, 0), (112, 2, 0), (112, 3, 0), (112, 4, 0), (113, 2, 0), (113, 3, 0), (113, 4, 0), (114, 2, 0), (114, 3, 0), (114, 4, 0), (115, 2, 0), (115, 3, 0), (115, 4, 0), (116, 2, 0), (116, 3, 0), (116, 4, 0), (117, 2, 0), (117, 3, 0), (117, 4, 0), (118, 2, 0), (118, 3, 0), (118, 4, 0), (119, 2, 0), (119, 3, 0), (119, 4, 0), (120, 2, 0), (120, 3, 0), (120, 4, 0), (121, 2, 0), (121, 3, 0), (121, 4, 0), (122, 2, 0), (122, 3, 0), (122, 4, 0), (123, 2, 0), (123, 3, 0), (123, 4, 0), (124, 2, 0), (124, 3, 0), (124, 4, 0), (125, 2, 0), (125, 3, 0), (125, 4, 0), (126, 2, 0), (126, 3, 0), (126, 4, 0), (127, 2, 0), (127, 3, 0), (127, 4, 0), (128, 2, 0), (128, 3, 0), (128, 4, 0), (129, 2, 0), (129, 3, 0), (129, 4, 0), (130, 2, 0), (130, 3, 0), (130, 4, 0), (131, 2, 0), (131, 3, 0), (131, 4, 0), (132, 2, 0), (132, 3, 0), (132, 4, 0), (133, 2, 0), (133, 3, 0), (133, 4, 0), (134, 2, 0), (134, 3, 0), (134, 4, 0), (135, 2, 0), (135, 3, 0), (135, 4, 0), (136, 2, 0), (136, 3, 0), (136, 4, 0), (137, 2, 0), (137, 3, 0), (137, 4, 0), (138, 2, 0), (138, 3, 0), (138, 4, 0), (139, 2, 0), (139, 3, 0), (139, 4, 0), (140, 2, 0), (140, 3, 0), (140, 4, 0), (141, 2, 0), (141, 3, 0), (141, 4, 0), (142, 2, 0), (142, 3, 0), (142, 4, 0), (143, 2, 0), (143, 3, 0), (143, 4, 0), (144, 2, 0), (144, 3, 0), (144, 4, 0), (145, 2, 0), (145, 3, 0), (145, 4, 0), (146, 2, 0), (146, 3, 0), (146, 4, 0), (147, 2, 0), (147, 3, 0), (147, 4, 0), (148, 2, 0), (148, 3, 0), (148, 4, 0), (149, 2, 0), (149, 3, 0), (149, 4, 0), (150, 2, 0), (150, 3, 0), (150, 4, 0), (151, 2, 0), (151, 3, 0), (151, 4, 0), (152, 2, 0), (152, 3, 0), (152, 4, 0), (153, 2, 0), (153, 3, 0), (153, 4, 0), (154, 2, 0), (154, 3, 0), (154, 4, 0), (155, 2, 0), (155, 3, 0), (155, 4, 0), (156, 2, 0), (156, 3, 0), (156, 4, 0), (157, 2, 0), (157, 3, 0), (157, 4, 0), (158, 2, 0), (158, 3, 0), (158, 4, 0), (159, 2, 0), (159, 3, 0), (159, 4, 0), (160, 2, 0), (160, 3, 0), (160, 4, 0), (161, 2, 0), (161, 3, 0), (161, 4, 0), (162, 2, 0), (162, 3, 0), (162, 4, 0), (163, 2, 0), (163, 3, 0), (163, 4, 0), (164, 2, 0), (164, 3, 0), (164, 4, 0), (165, 2, 0), (165, 3, 0), (165, 4, 0), (166, 2, 0), (166, 3, 0), (166, 4, 0), (167, 2, 0), (167, 3, 0), (167, 4, 0), (168, 2, 0), (168, 3, 0), (168, 4, 0), (169, 2, 0), (169, 3, 0), (169, 4, 0), (170, 2, 0), (170, 3, 0), (170, 4, 0), (171, 2, 0), (171, 3, 0), (171, 4, 0), (172, 2, 0), (172, 3, 0), (172, 4, 0), (173, 2, 0), (173, 3, 0), (173, 4, 0), (174, 2, 0), (174, 3, 0), (174, 4, 0), (175, 2, 0), (175, 3, 0), (175, 4, 0), (176, 2, 0), (176, 3, 0), (176, 4, 0), (177, 2, 0), (177, 3, 0), (177, 4, 0), (178, 2, 0), (178, 3, 0), (178, 4, 0), (179, 2, 0), (179, 3, 0), (179, 4, 0), (180, 2, 0), (180, 3, 0), (180, 4, 0), (181, 2, 0), (181, 3, 0), (181, 4, 0), (182, 2, 0), (182, 3, 0), (182, 4, 0), (183, 2, 0), (183, 3, 0), (183, 4, 0), (184, 2, 0), (184, 3, 0), (184, 4, 0), (185, 2, 0), (185, 3, 0), (185, 4, 0), (186, 2, 0), (186, 3, 0), (186, 4, 0), (187, 2, 0), (187, 3, 0), (187, 4, 0), (188, 2, 0), (188, 3, 0), (188, 4, 0), (189, 2, 0), (189, 3, 0), (189, 4, 0), (190, 2, 0), (190, 3, 0), (190, 4, 0), (191, 2, 0), (191, 3, 0), (191, 4, 0), (192, 2, 0), (192, 3, 0), (192, 4, 0), (193, 2, 0), (193, 3, 0), (193, 4, 0), (194, 2, 0), (194, 3, 0), (194, 4, 0), (195, 2, 0), (195, 3, 0), (195, 4, 0), (196, 2, 0), (196, 3, 0), (196, 4, 0), (197, 2, 0), (197, 3, 0), (197, 4, 0), (198, 2, 0), (198, 3, 0), (198, 4, 0), (199, 2, 0), (199, 3, 0), (199, 4, 0), (200, 2, 0), (200, 3, 0), (200, 4, 0), (201, 2, 0), (201, 3, 0), (201, 4, 0), (202, 2, 0), (202, 3, 0), (202, 4, 0), (203, 2, 0), (203, 3, 0), (203, 4, 0), (204, 2, 0), (204, 3, 0), (204, 4, 0), (205, 2, 0), (205, 3, 0), (205, 4, 0), (206, 2, 0), (206, 3, 0), (206, 4, 0), (207, 2, 0), (207, 3, 0), (207, 4, 0), (208, 2, 0), (208, 3, 0), (208, 4, 0), (209, 2, 0), (209, 3, 0), (209, 4, 0), (210, 2, 0), (210, 3, 0), (210, 4, 0), (211, 2, 0), (211, 3, 0), (211, 4, 0), (212, 2, 0), (212, 3, 0), (212, 4, 0), (213, 2, 0), (213, 3, 0), (213, 4, 0), (214, 2, 0), (214, 3, 0), (214, 4, 0), (215, 2, 0), (215, 3, 0), (215, 4, 0), (216, 2, 0), (216, 3, 0), (216, 4, 0), (217, 2, 0), (217, 3, 0), (217, 4, 0), (218, 2, 0), (218, 3, 0), (218, 4, 0), (219, 2, 0), (219, 3, 0), (219, 4, 0), (220, 2, 0), (220, 3, 0), (220, 4, 0), (221, 2, 0), (221, 3, 0), (221, 4, 0), (222, 2, 0), (222, 3, 0), (222, 4, 0), (223, 2, 0), (223, 3, 0), (223, 4, 0), (224, 2, 0), (224, 3, 0), (224, 4, 0), (225, 2, 0), (225, 3, 0), (225, 4, 0), (226, 2, 0), (226, 3, 0), (226, 4, 0), (227, 2, 0), (227, 3, 0), (227, 4, 0), (228, 2, 0), (228, 3, 0), (228, 4, 0), (229, 2, 0), (229, 3, 0), (229, 4, 0), (230, 2, 0), (230, 3, 0), (230, 4, 0), (231, 2, 0), (231, 3, 0), (231, 4, 0), (232, 2, 0), (232, 3, 0), (232, 4, 0), (233, 2, 0), (233, 3, 0), (233, 4, 0), (234, 2, 0), (234, 3, 0), (234, 4, 0), (235, 2, 0), (235, 3, 0), (235, 4, 0), (236, 2, 0), (236, 3, 0), (236, 4, 0), (237, 2, 0), (237, 3, 0), (237, 4, 0), (238, 2, 0), (238, 3, 0), (238, 4, 0), (239, 2, 0), (239, 3, 0), (239, 4, 0), (240, 2, 0), (240, 3, 0), (240, 4, 0), (241, 2, 0), (241, 3, 0), (241, 4, 0), (242, 2, 0), (242, 3, 0), (242, 4, 0)]\n",
      "[(9, 2, 0), (9, 3, 0), (9, 4, 0), (10, 2, 0), (10, 3, 0), (10, 4, 0), (11, 2, 0), (11, 3, 0), (11, 4, 0), (12, 2, 0), (12, 3, 0), (12, 4, 0), (13, 2, 0), (13, 3, 0), (13, 4, 0), (14, 2, 0), (14, 3, 0), (14, 4, 0), (15, 2, 0), (15, 3, 0), (15, 4, 0), (16, 2, 0), (16, 3, 0), (16, 4, 0), (17, 2, 0), (17, 3, 0), (17, 4, 0), (18, 2, 0), (18, 3, 0), (18, 4, 0), (19, 2, 0), (19, 3, 0), (19, 4, 0), (20, 2, 0), (20, 3, 0), (20, 4, 0), (21, 2, 0), (21, 3, 0), (21, 4, 0), (22, 2, 0), (22, 3, 0), (22, 4, 0), (23, 2, 0), (23, 3, 0), (23, 4, 0), (24, 2, 0), (24, 3, 0), (24, 4, 0), (25, 2, 0), (25, 3, 0), (25, 4, 0), (26, 2, 0), (26, 3, 0), (26, 4, 0), (27, 2, 0), (27, 3, 0), (27, 4, 0), (28, 2, 0), (28, 3, 0), (28, 4, 0), (29, 2, 0), (29, 3, 0), (29, 4, 0), (30, 2, 0), (30, 3, 0), (30, 4, 0), (31, 2, 0), (31, 3, 0), (31, 4, 0), (32, 2, 0), (32, 3, 0), (32, 4, 0), (33, 2, 0), (33, 3, 0), (33, 4, 0), (34, 2, 0), (34, 3, 0), (34, 4, 0), (35, 2, 0), (35, 3, 0), (35, 4, 0), (36, 2, 0), (36, 3, 0), (36, 4, 0), (37, 2, 0), (37, 3, 0), (37, 4, 0), (38, 2, 0), (38, 3, 0), (38, 4, 0), (39, 2, 0), (39, 3, 0), (39, 4, 0), (40, 2, 0), (40, 3, 0), (40, 4, 0), (41, 2, 0), (41, 3, 0), (41, 4, 0), (42, 2, 0), (42, 3, 0), (42, 4, 0), (43, 2, 0), (43, 3, 0), (43, 4, 0), (44, 2, 0), (44, 3, 0), (44, 4, 0), (45, 2, 0), (45, 3, 0), (45, 4, 0), (46, 2, 0), (46, 3, 0), (46, 4, 0), (47, 2, 0), (47, 3, 0), (47, 4, 0), (48, 2, 0), (48, 3, 0), (48, 4, 0), (49, 2, 0), (49, 3, 0), (49, 4, 0), (50, 2, 0), (50, 3, 0), (50, 4, 0), (51, 2, 0), (51, 3, 0), (51, 4, 0), (52, 2, 0), (52, 3, 0), (52, 4, 0), (53, 2, 0), (53, 3, 0), (53, 4, 0), (54, 2, 0), (54, 3, 0), (54, 4, 0), (55, 2, 0), (55, 3, 0), (55, 4, 0), (56, 2, 0), (56, 3, 0), (56, 4, 0), (57, 2, 0), (57, 3, 0), (57, 4, 0), (58, 2, 0), (58, 3, 0), (58, 4, 0), (59, 2, 0), (59, 3, 0), (59, 4, 0), (60, 2, 0), (60, 3, 0), (60, 4, 0), (61, 2, 0), (61, 3, 0), (61, 4, 0), (62, 2, 0), (62, 3, 0), (62, 4, 0), (63, 2, 0), (63, 3, 0), (63, 4, 0), (64, 2, 0), (64, 3, 0), (64, 4, 0), (65, 2, 0), (65, 3, 0), (65, 4, 0), (66, 2, 0), (66, 3, 0), (66, 4, 0), (67, 2, 0), (67, 3, 0), (67, 4, 0), (68, 2, 0), (68, 3, 0), (68, 4, 0), (69, 2, 0), (69, 3, 0), (69, 4, 0), (70, 2, 0), (70, 3, 0), (70, 4, 0), (71, 2, 0), (71, 3, 0), (71, 4, 0), (72, 2, 0), (72, 3, 0), (72, 4, 0), (73, 2, 0), (73, 3, 0), (73, 4, 0), (74, 2, 0), (74, 3, 0), (74, 4, 0), (75, 2, 0), (75, 3, 0), (75, 4, 0), (76, 2, 0), (76, 3, 0), (76, 4, 0), (77, 2, 0), (77, 3, 0), (77, 4, 0), (78, 2, 0), (78, 3, 0), (78, 4, 0), (79, 2, 0), (79, 3, 0), (79, 4, 0), (80, 2, 0), (80, 3, 0), (80, 4, 0), (81, 2, 0), (81, 3, 0), (81, 4, 0), (82, 2, 0), (82, 3, 0), (82, 4, 0), (83, 2, 0), (83, 3, 0), (83, 4, 0), (84, 2, 0), (84, 3, 0), (84, 4, 0), (85, 2, 0), (85, 3, 0), (85, 4, 0), (86, 2, 0), (86, 3, 0), (86, 4, 0), (87, 2, 0), (87, 3, 0), (87, 4, 0), (88, 2, 0), (88, 3, 0), (88, 4, 0), (89, 2, 0), (89, 3, 0), (89, 4, 0), (90, 2, 0), (90, 3, 0), (90, 4, 0), (91, 2, 0), (91, 3, 0), (91, 4, 0), (92, 2, 0), (92, 3, 0), (92, 4, 0), (93, 2, 0), (93, 3, 0), (93, 4, 0), (94, 2, 0), (94, 3, 0), (94, 4, 0), (95, 2, 0), (95, 3, 0), (95, 4, 0), (96, 2, 0), (96, 3, 0), (96, 4, 0), (97, 2, 0), (97, 3, 0), (97, 4, 0), (98, 2, 0), (98, 3, 0), (98, 4, 0), (99, 2, 0), (99, 3, 0), (99, 4, 0), (100, 2, 0), (100, 3, 0), (100, 4, 0), (101, 2, 0), (101, 3, 0), (101, 4, 0), (102, 2, 0), (102, 3, 0), (102, 4, 0), (103, 2, 0), (103, 3, 0), (103, 4, 0), (104, 2, 0), (104, 3, 0), (104, 4, 0), (105, 2, 0), (105, 3, 0), (105, 4, 0), (106, 2, 0), (106, 3, 0), (106, 4, 0), (107, 2, 0), (107, 3, 0), (107, 4, 0), (108, 2, 0), (108, 3, 0), (108, 4, 0), (109, 2, 0), (109, 3, 0), (109, 4, 0), (110, 2, 0), (110, 3, 0), (110, 4, 0), (111, 2, 0), (111, 3, 0), (111, 4, 0), (112, 2, 0), (112, 3, 0), (112, 4, 0), (113, 2, 0), (113, 3, 0), (113, 4, 0), (114, 2, 0), (114, 3, 0), (114, 4, 0), (115, 2, 0), (115, 3, 0), (115, 4, 0), (116, 2, 0), (116, 3, 0), (116, 4, 0), (117, 2, 0), (117, 3, 0), (117, 4, 0), (118, 2, 0), (118, 3, 0), (118, 4, 0), (119, 2, 0), (119, 3, 0), (119, 4, 0), (120, 2, 0), (120, 3, 0), (120, 4, 0), (121, 2, 0), (121, 3, 0), (121, 4, 0), (122, 2, 0), (122, 3, 0), (122, 4, 0), (123, 2, 0), (123, 3, 0), (123, 4, 0), (124, 2, 0), (124, 3, 0), (124, 4, 0), (125, 2, 0), (125, 3, 0), (125, 4, 0), (126, 2, 0), (126, 3, 0), (126, 4, 0), (127, 2, 0), (127, 3, 0), (127, 4, 0), (128, 2, 0), (128, 3, 0), (128, 4, 0), (129, 2, 0), (129, 3, 0), (129, 4, 0), (130, 2, 0), (130, 3, 0), (130, 4, 0), (131, 2, 0), (131, 3, 0), (131, 4, 0), (132, 2, 0), (132, 3, 0), (132, 4, 0), (133, 2, 0), (133, 3, 0), (133, 4, 0), (134, 2, 0), (134, 3, 0), (134, 4, 0), (135, 2, 0), (135, 3, 0), (135, 4, 0), (136, 2, 0), (136, 3, 0), (136, 4, 0), (137, 2, 0), (137, 3, 0), (137, 4, 0), (138, 2, 0), (138, 3, 0), (138, 4, 0), (139, 2, 0), (139, 3, 0), (139, 4, 0), (140, 2, 0), (140, 3, 0), (140, 4, 0), (141, 2, 0), (141, 3, 0), (141, 4, 0), (142, 2, 0), (142, 3, 0), (142, 4, 0), (143, 2, 0), (143, 3, 0), (143, 4, 0), (144, 2, 0), (144, 3, 0), (144, 4, 0), (145, 2, 0), (145, 3, 0), (145, 4, 0), (146, 2, 0), (146, 3, 0), (146, 4, 0), (147, 2, 0), (147, 3, 0), (147, 4, 0), (148, 2, 0), (148, 3, 0), (148, 4, 0), (149, 2, 0), (149, 3, 0), (149, 4, 0), (150, 2, 0), (150, 3, 0), (150, 4, 0), (151, 2, 0), (151, 3, 0), (151, 4, 0), (152, 2, 0), (152, 3, 0), (152, 4, 0), (153, 2, 0), (153, 3, 0), (153, 4, 0), (154, 2, 0), (154, 3, 0), (154, 4, 0), (155, 2, 0), (155, 3, 0), (155, 4, 0), (156, 2, 0), (156, 3, 0), (156, 4, 0), (157, 2, 0), (157, 3, 0), (157, 4, 0), (158, 2, 0), (158, 3, 0), (158, 4, 0), (159, 2, 0), (159, 3, 0), (159, 4, 0), (160, 2, 0), (160, 3, 0), (160, 4, 0), (161, 2, 0), (161, 3, 0), (161, 4, 0), (162, 2, 0), (162, 3, 0), (162, 4, 0), (163, 2, 0), (163, 3, 0), (163, 4, 0), (164, 2, 0), (164, 3, 0), (164, 4, 0), (165, 2, 0), (165, 3, 0), (165, 4, 0), (166, 2, 0), (166, 3, 0), (166, 4, 0), (167, 2, 0), (167, 3, 0), (167, 4, 0), (168, 2, 0), (168, 3, 0), (168, 4, 0), (169, 2, 0), (169, 3, 0), (169, 4, 0), (170, 2, 0), (170, 3, 0), (170, 4, 0), (171, 2, 0), (171, 3, 0), (171, 4, 0), (172, 2, 0), (172, 3, 0), (172, 4, 0), (173, 2, 0), (173, 3, 0), (173, 4, 0), (174, 2, 0), (174, 3, 0), (174, 4, 0), (175, 2, 0), (175, 3, 0), (175, 4, 0), (176, 2, 0), (176, 3, 0), (176, 4, 0), (177, 2, 0), (177, 3, 0), (177, 4, 0), (178, 2, 0), (178, 3, 0), (178, 4, 0), (179, 2, 0), (179, 3, 0), (179, 4, 0), (180, 2, 0), (180, 3, 0), (180, 4, 0), (181, 2, 0), (181, 3, 0), (181, 4, 0), (182, 2, 0), (182, 3, 0), (182, 4, 0), (183, 2, 0), (183, 3, 0), (183, 4, 0), (184, 2, 0), (184, 3, 0), (184, 4, 0), (185, 2, 0), (185, 3, 0), (185, 4, 0), (186, 2, 0), (186, 3, 0), (186, 4, 0), (187, 2, 0), (187, 3, 0), (187, 4, 0), (188, 2, 0), (188, 3, 0), (188, 4, 0), (189, 2, 0), (189, 3, 0), (189, 4, 0), (190, 2, 0), (190, 3, 0), (190, 4, 0), (191, 2, 0), (191, 3, 0), (191, 4, 0), (192, 2, 0), (192, 3, 0), (192, 4, 0), (193, 2, 0), (193, 3, 0), (193, 4, 0), (194, 2, 0), (194, 3, 0), (194, 4, 0), (195, 2, 0), (195, 3, 0), (195, 4, 0), (196, 2, 0), (196, 3, 0), (196, 4, 0), (197, 2, 0), (197, 3, 0), (197, 4, 0), (198, 2, 0), (198, 3, 0), (198, 4, 0), (199, 2, 0), (199, 3, 0), (199, 4, 0), (200, 2, 0), (200, 3, 0), (200, 4, 0), (201, 2, 0), (201, 3, 0), (201, 4, 0), (202, 2, 0), (202, 3, 0), (202, 4, 0), (203, 2, 0), (203, 3, 0), (203, 4, 0), (204, 2, 0), (204, 3, 0), (204, 4, 0), (205, 2, 0), (205, 3, 0), (205, 4, 0), (206, 2, 0), (206, 3, 0), (206, 4, 0), (207, 2, 0), (207, 3, 0), (207, 4, 0), (208, 2, 0), (208, 3, 0), (208, 4, 0), (209, 2, 0), (209, 3, 0), (209, 4, 0), (210, 2, 0), (210, 3, 0), (210, 4, 0), (211, 2, 0), (211, 3, 0), (211, 4, 0), (212, 2, 0), (212, 3, 0), (212, 4, 0), (213, 2, 0), (213, 3, 0), (213, 4, 0), (214, 2, 0), (214, 3, 0), (214, 4, 0), (215, 2, 0), (215, 3, 0), (215, 4, 0), (216, 2, 0), (216, 3, 0), (216, 4, 0), (217, 2, 0), (217, 3, 0), (217, 4, 0), (218, 2, 0), (218, 3, 0), (218, 4, 0), (219, 2, 0), (219, 3, 0), (219, 4, 0), (220, 2, 0), (220, 3, 0), (220, 4, 0), (221, 2, 0), (221, 3, 0), (221, 4, 0), (222, 2, 0), (222, 3, 0), (222, 4, 0), (223, 2, 0), (223, 3, 0), (223, 4, 0), (224, 2, 0), (224, 3, 0), (224, 4, 0), (225, 2, 0), (225, 3, 0), (225, 4, 0), (226, 2, 0), (226, 3, 0), (226, 4, 0), (227, 2, 0), (227, 3, 0), (227, 4, 0), (228, 2, 0), (228, 3, 0), (228, 4, 0), (229, 2, 0), (229, 3, 0), (229, 4, 0), (230, 2, 0), (230, 3, 0), (230, 4, 0), (231, 2, 0), (231, 3, 0), (231, 4, 0), (232, 2, 0), (232, 3, 0), (232, 4, 0), (233, 2, 0), (233, 3, 0), (233, 4, 0), (234, 2, 0), (234, 3, 0), (234, 4, 0), (235, 2, 0), (235, 3, 0), (235, 4, 0), (236, 2, 0), (236, 3, 0), (236, 4, 0), (237, 2, 0), (237, 3, 0), (237, 4, 0), (238, 2, 0), (238, 3, 0), (238, 4, 0), (239, 2, 0), (239, 3, 0), (239, 4, 0), (240, 2, 0), (240, 3, 0), (240, 4, 0), (241, 2, 0), (241, 3, 0), (241, 4, 0), (242, 2, 0), (242, 3, 0), (242, 4, 0)]\n"
     ]
    }
   ],
   "source": [
    "# define feature space for hashing\n",
    "\n",
    "# get max number of respective layers (exclusive)\n",
    "excl_conv_n_max = 5\n",
    "excl_full_n_max = 5\n",
    "\n",
    "# get number of respective layer attributes (max exclusive)\n",
    "c_min = 3**2 \n",
    "c_max = 3**5\n",
    "k_min = 2\n",
    "k_max = 5\n",
    "m_min = 0\n",
    "m_max = 1\n",
    "f_min = 2**2\n",
    "f_max = 2**6\n",
    "d_min = 5\n",
    "d_max = 11\n",
    "\n",
    "c = c_max - c_min # convolutional layer options\n",
    "k = k_max - k_min # kernel options\n",
    "m = m_max - m_min # max pool layer options\n",
    "f = f_max - f_min # fully connected layer options\n",
    "d = d_max - d_min # dropout layer options\n",
    "\n",
    "# conv = set(range(c_max)) - set(range(c_min))\n",
    "# full = set(range(f_max)) - set(range(f_min))\n",
    "conv = list(range(c_max)[c_min:])\n",
    "kern = list(range(k_max)[k_min:])\n",
    "maxp = list(range(m_max)[m_min:])\n",
    "full = list(range(f_max)[f_min:])\n",
    "drop = list(range(d_max)[d_min:])\n",
    "\n",
    "# c_comb = list(combinations_with_replacement(conv,2))\n",
    "c_comb = []\n",
    "k_comb = []\n",
    "m_comb = []\n",
    "f_comb = []\n",
    "d_comb = []\n",
    "\n",
    "for layers in range(1,4):\n",
    "    cm = list(product(conv,kern,maxp,repeat=1))\n",
    "    print(cm)\n",
    "    c_comb += list(combinations_with_replacement(cm,layers))\n",
    "# print(conv)\n",
    "# print(maxp)\n",
    "# print(c_comb)\n",
    "# pd.DataFrame(c_comb).to_csv(\"c_comb.csv\")\n",
    "for layers in range(1,2):\n",
    "    m_comb += list(combinations_with_replacement(maxp,layers))\n",
    "for layers in range(1,5):\n",
    "    fd = list(product(full,drop))\n",
    "    f_comb += list(combinations_with_replacement(fd,layers))\n",
    "# print(full)\n",
    "# print(drop)\n",
    "# print(f_comb)\n",
    "# pd.DataFrame(f_comb).to_csv(\"f_comb.csv\")\n",
    "for layers in range(1,2):\n",
    "    d_comb += list(combinations_with_replacement(drop,layers))\n",
    "#     print(\"Fully connected layer %s range: %s\" % (layers,len(f_comb)) )\n",
    "#     print(\"\\n\")\n",
    "# [pd.DataFrame(comb).to_csv(\"%s.csv\" % name) for name,comb in zip([\"c_comb\",\"m_comb\",\"f_comb\",\"d_comb\"],[c_comb,m_comb,f_comb,d_comb])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fdb8ab86880e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fdb8ab86880e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def lincombbuild\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# trial value builder\n",
    "def buildtrial(conv_kern=rang,conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conversion from dec to whatever we end up using\n",
    "# most to least significant digit\n",
    "def numberToBase(n, b):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    digits = []\n",
    "    while n:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    rev = digits[::-1]\n",
    "    return rev\n",
    "\n",
    "def feature_spacing():\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "#     conv_combinations = list(combinations([c_comb,m_comb],2))\n",
    "#     print(\"np.shape(conv_combinations[0])\",np.shape(conv_combinations[0]))\n",
    "# #     pd.DataFrame(conv_combinations).to_csv('conv_combinations.csv')\n",
    "#     full_combinations = list(combinations([f_comb,d_comb],2))\n",
    "#     print(\"np.shape(full_combinations[0])\",np.shape(full_combinations[0]))\n",
    "#     pd.DataFrame(full_combinations).to_csv('full_combinations.csv')\n",
    "    unique_combinations = list(product(c_comb,f_comb))\n",
    "    print(\"np.shape(unique_combinations)\",np.shape(unique_combinations))\n",
    "#     pd.DataFrame(unique_combinations).to_csv('unique_combinations.csv')\n",
    "#     [pd.DataFrame(comb).to_csv(\"%s.csv\" % name) for name,comb in zip([\"conv_combinations\",\"full_combinations\",\"unique_combinations\"],[conv_combinations,full_combinations,unique_combinations])]\n",
    "    total_uniques = len(unique_combinations)\n",
    "    total_points = total_uniques**2\n",
    "    total_cvs = len(c_comb)\n",
    "    total_krn = len(k_comb)\n",
    "    total_mxp = len(m_comb)\n",
    "    total_fcs = len(f_comb)\n",
    "    total_drp = len(d_comb)\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    \n",
    "    cf = []\n",
    "    \n",
    "    for layer in [total_cvs,total_mxp,total_fcs,total_drp]:#,total_uniques]:\n",
    "        results = {\n",
    "            \"base\": [1],\n",
    "            \"nodes_req\": [total_uniques],\n",
    "            \"sparcity\": [total_points - total_uniques],\n",
    "            \"max_necc_base_value\":[0],\n",
    "            \"nodes+_req\": [0],\n",
    "            \"subsparcity\": [0],\n",
    "            \"unexplained\":[0],\n",
    "            \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "            \"subsparcity_pcnt\": [0],\n",
    "            \"denoise_pcnt\":[0],\n",
    "            \"complexity\":[0]\n",
    "        }\n",
    "\n",
    "        report = pd.DataFrame(results)\n",
    "    \n",
    "        for base in range(2,101):\n",
    "            results[\"base\"] = [base]\n",
    "            results[\"nodes_req\"] = [math.ceil(math.log(layer,(base)))]\n",
    "            results[\"nodes+_req\"] = [math.floor(math.log(layer,(base)))]\n",
    "            \n",
    "            results[\"sparcity\"] = [base**math.ceil(math.log(layer,base)) - layer]\n",
    "            results[\"subsparcity\"] = [-(base**math.floor(math.log(layer,base)) - layer)]\n",
    "            \n",
    "            results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.ceil(math.log(layer,(base))))*100]\n",
    "            results[\"subsparcity_pcnt\"] = [-((base**math.floor(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.floor(math.log(layer,(base))))*100)]\n",
    "            \n",
    "#             results[\"max_necc_base_value\"] = [numberToBase((results[\"base\"][0]**results[\"nodes+_req\"][0]+results[\"subsparcity\"][0]),results[\"base\"][0])]\n",
    "            results[\"max_necc_base_value\"] = [numberToBase(layer,base)]\n",
    "            results[\"unexplained\"] = [(-(base**math.floor(math.log(layer,base)) - layer))*(math.floor(math.log(layer,(base))))]\n",
    "            \n",
    "            results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(layer,base)))**2)/total_points)*100)]\n",
    "        \n",
    "            results[\"complexity\"] = [results[\"nodes_req\"][0]*(results[\"sparcity\"][0]+1)]\n",
    "\n",
    "            report = report.append(pd.DataFrame(results))\n",
    "            \n",
    "            \n",
    "        report.index = [x for x in range(1, len(report.values)+1)]\n",
    "        report.drop([1],axis=0,inplace=True)\n",
    "        report.sort_values([\"nodes+_req\",\"sparcity\",\"unexplained\",\"subsparcity\",\"sparcity_pcnt\",\"base\"],inplace=True)\n",
    "        \n",
    "        cf.append(report.iloc[0])\n",
    "    \n",
    "    return cf\n",
    "\n",
    "bases = feature_spacing()\n",
    "[print(r,\"\\n\") for r in bases]\n",
    "\n",
    "base_c = bases[0][\"base\"]\n",
    "base_m = bases[1][\"base\"]\n",
    "base_f = bases[2][\"base\"]\n",
    "base_d = bases[3][\"base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For the convolutional layers, base %s seems to allow us to use: \\n1.) the fewest nodes of value 2-100 (inclusive) with \\n2.) the lowest number of invalid configuration indices (sparcity).\" % bases[0][\"base\"])\n",
    "print(\"For the max pooling layers, base %s seems to allow us to use: \\n1.) the fewest nodes of value 2-100 (inclusive) with \\n2.) the lowest number of invalid configuration indices (sparcity).\" % bases[1][\"base\"])\n",
    "print(\"For the linear layers, base %s seems to allow us to use: \\n1.) the fewest nodes of value 2-100 (inclusive) with \\n2.) the lowest number of invalid configuration indices (sparcity).\" % bases[2][\"base\"])\n",
    "print(\"For the dropout layers, base %s seems to allow us to use: \\n1.) the fewest nodes of value 2-100 (inclusive) with \\n2.) the lowest number of invalid configuration indices (sparcity).\" % bases[3][\"base\"])\n",
    "\n",
    "# print(\"We can use the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_to_dec(num_list, base):\n",
    "    num_list = num_list[::-1]\n",
    "    num = 0\n",
    "    for k in range(len(num_list)):\n",
    "        dig = num_list[k]\n",
    "        dig = int(dig)\n",
    "        num += dig*(base**k)\n",
    "    return num\n",
    "\n",
    "def encode(config=[(24, 64),(0, 1),(13, 41),(0, 6)]):\n",
    "    iconv = c_comb.index(config[0])\n",
    "    imxp = m_comb.index(config[1])\n",
    "    ifull = f_comb.index(config[2])\n",
    "    dconv = d_comb.index(config[3])\n",
    "    \n",
    "    conv_hash = numberToBase(iconv,base_c)\n",
    "    maxp_hash = numberToBase(imaxp,base_m)\n",
    "    full_hash = numberToBase(ifull,base_f)\n",
    "    drop_hash = numberToBase(idrop,base_d)\n",
    "    \n",
    "    return [conv_hash,maxp_hash,full_hash,drop_hash]\n",
    "\n",
    "def decode(hash=([1, 7, 5, 0], [1, 0, 1, 0], [2, 9, 7], [6, 9, 7])):\n",
    "    conv = base_to_dec(hash[0], base_c)\n",
    "    maxp = base_to_dec(hash[1], base_m)\n",
    "    full = base_to_dec(hash[2], base_f)\n",
    "    drop = base_to_dec(hash[3], base_d)\n",
    "\n",
    "    \n",
    "    return [c_comb[conv],m_comb[maxp],f_comb[full],d_comb[drop]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=dt.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers=[[6, 16],[0, 1],[120,84],[0, 6]]):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_convs,hidden_maxps,hidden_fcs,hidden_drops = hidden_layers\n",
    "#         print(hidden_convs)\n",
    "#         print(hidden_fcs)\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         uf_input = 0\n",
    "#         layer_list = OrderedDict()\n",
    "        \n",
    "#         layers = []\n",
    "#         pre_flat_size = 32\n",
    "#         in_channels = 3\n",
    "#         out_kernel = None\n",
    "        \n",
    "#         layer_list['conv1'] = nn.Conv2d(3, hidden_convs[0], 5)\n",
    "#         layer_list['pool1'] = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         layer_input = layer_list['conv1'].out_channels\n",
    "        \n",
    "#         for layer_num, channels in enumerate(hidden_convs[1:], 2):\n",
    "#             layer_list[\"conv%s\" % layer_num]  = nn.Conv2d(layer_input, channels, 5)\n",
    "#             layer_list[\"pool%s\" % layer_num] = nn.MaxPool2d(2, 2)\n",
    "#             layer_input = layer_list[\"conv%s\" % layer_num].out_channels\n",
    "        \n",
    "        \n",
    "#         layer_list[\"flat\"] = nn.Flatten()\n",
    "        \n",
    "#         layer_list['fc1'] = nn.Linear(layer_input*5*5, hidden_fcs[0])\n",
    "#         layer_list[\"relu1\"]  = nn.ReLU()\n",
    "        \n",
    "#         layer_input = layer_list['fc1'].out_features\n",
    "#         for (layer_num, features) in enumerate(hidden_fcs[1:], 2):\n",
    "#             layer_list[\"fc%s\" % layer_num]  = nn.Linear(layer_input, features)\n",
    "#             layer_list[\"relu%s\" % layer_num]  = nn.ReLU()\n",
    "#             layer_input = layer_list[\"fc%s\" % layer_num].out_features\n",
    "            \n",
    "        \n",
    "#         layer_list['fco'] = nn.Linear(hidden_fcs[-1], 10)\n",
    "    \n",
    "#         self.layers = nn.Sequential(layer_list)\n",
    "        \n",
    "# #         print(\"New model: %s\" % hidden_layers)\n",
    "#     def forward(self, x):\n",
    "#         x = self.layers(x)\n",
    "#         return x\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "        layers = []\n",
    "        pre_flat_size = 32\n",
    "        in_channels = 3\n",
    "        out_kernel = None\n",
    "\n",
    "        for i in range(hidden_convs):\n",
    "            if pre_flat_size > 7:\n",
    "                out_channels = hidden_convs[i]\n",
    "                kernel_size = 3\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "                pre_flat_size = pre_flat_size - kernel_size+1\n",
    "    #             print(\"post conv: \",pre_flat_size)\n",
    "                if hidden_maxps[i] & pre_flat_size > 3:\n",
    "                    layers.append(nn.MaxPool2d(2, 2))\n",
    "                    pre_flat_size = int(pre_flat_size / 2)\n",
    "    #                 print(\"post pool: \",pre_flat_size)\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "            in_channels = out_channels\n",
    "            out_kernel = kernel_size\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "    #     self.flattening = nn.Sequential(*layers)\n",
    "\n",
    "    #     layers = []\n",
    "    #         print(\"pre_flat_size:\",pre_flat_size)\n",
    "        in_features = in_channels * pre_flat_size**2\n",
    "        for i in range(hidden_fcs):\n",
    "            out_features = hidden_fcs[i]\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "            if hidden_drops[0] % 10:\n",
    "                p = (hidden_drops[0] % 10) / 10\n",
    "                layers.append(nn.Dropout(p))\n",
    "            layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, 10))\n",
    "        layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    #     self.linearizing = nn.Sequential(*layers)\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(neuron_config, checkpoint_dir=None):\n",
    "    print(neuron_config[\"conv_adjusted\"])\n",
    "    print(neuron_config[\"full_adjusted\"])\n",
    "    \n",
    "    data_dir=dt.absolute()\n",
    "    \n",
    "    def cv_discrim(s): return 'conv_subindex_' in s\n",
    "    def fc_discrim(s): return 'full_subindex_' in s\n",
    "    cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "    fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "    \n",
    "#     cfg = decode([cvs, fcs])\n",
    "    cfg = decode([neuron_config[\"conv_adjusted\"], neuron_config[\"full_adjusted\"]])    \n",
    "    net = Net(cfg)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=neuron_config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(neuron_config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(neuron_config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"Model: %s, Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (cfg,epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=(correct / total))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "nodes_c = bases[0][\"nodes_req\"]\n",
    "nodes_m = bases[1][\"nodes_req\"]\n",
    "nodes_f = bases[2][\"nodes_req\"]\n",
    "nodes_d = bases[3][\"nodes_req\"]\n",
    "max_c = bases[0][\"max_necc_base_value\"]\n",
    "max_m = bases[1][\"max_necc_base_value\"]\n",
    "max_f = bases[2][\"max_necc_base_value\"]\n",
    "max_d = bases[3][\"max_necc_base_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def configure_neurons(num_convs,num_fcs):\n",
    "def configure_neurons():\n",
    "    lr = {\n",
    "        \"name\":\"learning rate {10^(-⌊x⌋)\",\n",
    "#         \"bounds\":[1e-4,1e-1]\n",
    "        \"bounds\":[x for x in range(1,4)]\n",
    "    }\n",
    "    batch_size = {\n",
    "        \"name\":\"batch size {2^⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(5,9)]\n",
    "#         \"bounds\":[x for x in range(4,6)]\n",
    "    }\n",
    "    epochs = {\n",
    "        \"name\":\"epochs {10⌊x⌋}\",\n",
    "#         \"bounds\":[x for x in range(2,6)]\n",
    "        \"bounds\":[1,2]\n",
    "    }\n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict = {\"name\": \"neurons\"}\n",
    "    config_space_ray = {\"name\": \"neurons\"}\n",
    "    \n",
    "#     #start ConfigSpace API\n",
    "#     config_space.add_hyperparameter(\n",
    "#         CS.UniformFloatHyperparameter(name=\"lr\", lower=1e-4, upper=1e-1, log=True))\n",
    "#     config_space.add_hyperparameter(\n",
    "#         CS.CategoricalHyperparameter(name=\"batch_size\", choices=[2**x for x in range(2,6)]))\n",
    "#     config_space.add_hyperparameter(\n",
    "#         CS.CategoricalHyperparameter(name=\"epochs\", choices=[10*x for x in range(2,5)]))\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            lr[\"name\"],\n",
    "            lr[\"bounds\"][0],\n",
    "            lr[\"bounds\"][-1],\n",
    "            log=True\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            batch_size[\"name\"], \n",
    "            batch_size[\"bounds\"]\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            epochs[\"name\"], \n",
    "            epochs[\"bounds\"]\n",
    "        ))\n",
    "    \n",
    "#     #start Ray Search Space API\n",
    "#     config_space_dict[\"lr\"] = tune.loguniform(lower=1e-4, upper=1e-1)\n",
    "#     config_space_dict[\"batch_size\"] = tune.choice(categories=[2**x for x in range(2,6)])\n",
    "#     config_space_dict[\"epochs\"] = tune.choice(categories=[10*x for x in range(2,5)])\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_ray[lr[\"name\"]] = tune.loguniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_ray[batch_size[\"name\"]] = tune.choice(batch_size[\"bounds\"])\n",
    "    config_space_ray[epochs[\"name\"]] = tune.choice(categories=epochs[\"bounds\"])\n",
    "    \n",
    "    config_space_dict[\"conv_subindices\"],config_space_dict[\"full_subindices\"] = [],[]\n",
    "    config_space_dict[\"conv_cases\"],config_space_dict[\"full_cases\"] = [],[]\n",
    "    config_space_dict[\"conv_auxilliary\"],config_space_dict[\"full_auxilliary\"] = [],[]        \n",
    "    config_space_dict[\"conv_adjusted\"],config_space_dict[\"full_adjusted\"] = [],[]\n",
    "    config_space_dict[\"maxp_subindices\"],config_space_dict[\"drop_subindices\"] = [],[]\n",
    "    config_space_dict[\"maxp_cases\"],config_space_dict[\"drop_cases\"] = [],[]\n",
    "    config_space_dict[\"maxp_auxilliary\"],config_space_dict[\"drop_auxilliary\"] = [],[]        \n",
    "    config_space_dict[\"maxp_adjusted\"],config_space_dict[\"drop_adjusted\"] = [],[]\n",
    "    \n",
    "    conv_lims,full_lims = [],[]\n",
    "    maxp_lims,drop_lims = [],[]\n",
    "    \n",
    "    for subindex in range(nodes_c):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"conv_subindex_%s\" % subindex\n",
    "        \n",
    "        conv_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_c-1, default_value=subindex%(base_c-1))\n",
    "#         conv_dict_rule = tune.randint(lower=0, upper=base_c-1)\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(conv_rule)\n",
    "#         config_space_dict[rule_name] = conv_dict_rule\n",
    "#         config_space_dict[\"conv_subindices\"].append(conv_dict_rule)\n",
    "    \n",
    "        conv_rules = list(filter(lambda hp: \"conv_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "        conv_dict_rules_keys = list(filter(lambda name: \"conv_subindex_\" in name, config_space_dict.keys()))\n",
    "#         conv_dict_rules = {k:config_space_dict[k] for k in conv_dict_rules_keys if k in config_space_dict}\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "#         rld = deepcopy(config_space_dict)\n",
    "        rd, rdd = {},{}\n",
    "        \n",
    "        for ri,rule in enumerate(conv_rules,1):\n",
    "    \n",
    "            if (len(conv_rules) == 1) & (max_c[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(conv_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_c[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(rule, range(max_c[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "#         for ri,rulekey in enumerate(conv_dict_rules.keys(),1):\n",
    "#             rule = conv_dict_rules[rulekey]\n",
    "            \n",
    "#             if (len(conv_dict_rules) == 1) & (max_c[ri-1] == config_space_dict[rule_name].upper):\n",
    "#                 break\n",
    "#             elif ri != len(conv_dict_rules):\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.randint(lower=0, upper=max_c[ri-1])\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.sample_from(lambda spec: spec.config_space_dict...uniform * 0.01)\n",
    "#             else:\n",
    "#                 rld[rulekey].upper = CS.ForbiddenInClause(rule, range(max_c[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )           \n",
    "    \n",
    "    for subindex in range(nodes_f):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"full_subindex_%s\" % subindex\n",
    "        full_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_f-1, default_value=subindex%(base_f-1))\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(full_rule)\n",
    "    \n",
    "        full_rules = list(filter(lambda hp: \"full_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "        rd = {}\n",
    "        for ri,rule in enumerate(full_rules,1):\n",
    "            if (len(full_rules) == 1) & (max_f[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(full_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_f[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(\n",
    "                        rule,\n",
    "                        range(max_f[ri-1] + 1, rule.upper + 1))\n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    for subindex in range(nodes_m):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"maxp_subindex_%s\" % subindex\n",
    "        \n",
    "        maxp_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_m-1, default_value=subindex%(base_m-1))\n",
    "#         conv_dict_rule = tune.randint(lower=0, upper=base_c-1)\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(maxp_rule)\n",
    "#         config_space_dict[rule_name] = conv_dict_rule\n",
    "#         config_space_dict[\"conv_subindices\"].append(conv_dict_rule)\n",
    "    \n",
    "        maxp_rules = list(filter(lambda hp: \"maxp_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "        maxp_dict_rules_keys = list(filter(lambda name: \"maxp_subindex_\" in name, config_space_dict.keys()))\n",
    "#         conv_dict_rules = {k:config_space_dict[k] for k in conv_dict_rules_keys if k in config_space_dict}\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "#         rld = deepcopy(config_space_dict)\n",
    "        rd, rdd = {},{}\n",
    "        \n",
    "        for ri,rule in enumerate(maxp_rules,1):\n",
    "    \n",
    "            if (len(maxp_rules) == 1) & (max_m[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(maxp_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_m[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(rule, range(max_m[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "#         for ri,rulekey in enumerate(conv_dict_rules.keys(),1):\n",
    "#             rule = conv_dict_rules[rulekey]\n",
    "            \n",
    "#             if (len(conv_dict_rules) == 1) & (max_c[ri-1] == config_space_dict[rule_name].upper):\n",
    "#                 break\n",
    "#             elif ri != len(conv_dict_rules):\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.randint(lower=0, upper=max_c[ri-1])\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.sample_from(lambda spec: spec.config_space_dict...uniform * 0.01)\n",
    "#             else:\n",
    "#                 rld[rulekey].upper = CS.ForbiddenInClause(rule, range(max_c[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )           \n",
    "    \n",
    "    for subindex in range(nodes_d):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"drop_subindex_%s\" % subindex\n",
    "        drop_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_d-1, default_value=subindex%(base_d-1))\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(drop_rule)\n",
    "    \n",
    "        drop_rules = list(filter(lambda hp: \"drop_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "        rd = {}\n",
    "        for ri,rule in enumerate(drop_rules,1):\n",
    "            if (len(drop_rules) == 1) & (max_d[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(drop_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_d[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(\n",
    "                        rule,\n",
    "                        range(max_d[ri-1] + 1, rule.upper + 1))\n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    # add nodes to config space\n",
    "    [config_space_dict[\"conv_subindices\"].append(tune.randint(lower=0, upper=base_c-1)) for n in range(nodes_c)]\n",
    "    [config_space_dict[\"maxp_subindices\"].append(tune.randint(lower=0, upper=base_m-1)) for n in range(nodes_m)]\n",
    "    [config_space_dict[\"full_subindices\"].append(tune.randint(lower=0, upper=base_f-1)) for n in range(nodes_f)]\n",
    "    [config_space_dict[\"drop_subindices\"].append(tune.randint(lower=0, upper=base_d-1)) for n in range(nodes_d)]\n",
    "    \n",
    "    for i,ceiling in enumerate(max_c):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"conv_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.conv_subindices[i] >= max_c[i] & config_space_dict[\"conv_cases\"][i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"conv_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.conv_subindices[i] >= max_c[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"conv_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"conv_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.conv_auxilliary[i] if spec.config.conv_cases[i] else spec.config.conv_subindices[i]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    for i,ceiling in enumerate(max_f):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"full_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.full_subindices[i] >= max_f[i] & spec.config.full_cases[i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"full_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.full_subindices[i] >= max_f[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"full_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"full_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.full_auxilliary[i] if spec.config.full_cases[i] else spec.config.full_subindices[i]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for i,ceiling in enumerate(max_m):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"maxp_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.maxp_subindices[i] >= max_m[i] & config_space_dict[\"maxp_cases\"][i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"maxp_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.maxp_subindices[i] >= max_m[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"maxp_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"maxp_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.maxp_auxilliary[i] if spec.config.maxp_cases[i] else spec.config.maxp_subindices[i]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    for i,ceiling in enumerate(max_d):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"drop_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.drop_subindices[i] >= max_d[i] & spec.config.drop_cases[i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"drop_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.drop_subindices[i] >= max_d[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"drop_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"drop_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.drop_auxilliary[i] if spec.config.drop_cases[i] else spec.config.drop_subindices[i]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return config_space,config_space_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = configure_neurons()\n",
    "cfl = [{v:cn[1][v]} for v in cn[1]]\n",
    "[print(v,\": \",cn[1][v]) for v in cn[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons(checkpoint_dir=None):    \n",
    "    neuron_config_space = configure_neurons()[1]\n",
    "    parameter_columns = [\"batch_size\", \"lr\", \"epochs\", \"conv_adjusted\", \"full_adjusted\"]\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "#     config = {\n",
    "#     \"LiNO3_vol\": tune.uniform(0, 7),\n",
    "#     \"Li2SO4_vol\": tune.uniform(0, 7),\n",
    "#     \"NaClO4_vol\": tune.uniform(0, 7)\n",
    "#     }\n",
    "    \n",
    "#     #pre-load data to avoid races\n",
    "#     load_data()\n",
    "    \n",
    "#     param_dict = {\"domain\": cfl}\n",
    "#     domain_config = load_config(param_dict)\n",
    "#     print(domain_config)\n",
    "#     domain, domain_orderings = domain_config.domain, domain_config.domain_orderings\n",
    "\n",
    "    # define the hpo search algorithm BO\n",
    "#     func_caller = EuclideanFunctionCaller(None, domain_config.domain.list_of_domains[0])\n",
    "#     optimizer = EuclideanGPBandit(func_caller, ask_tell_mode=True)\n",
    "#     bo_search_alg = DragonflySearch(optimizer, **experiment_metrics)\n",
    "    \n",
    "    scheduler = PB2(\n",
    "        time_attr=\"training_iteration\",\n",
    "        perturbation_interval=1,#max_time/10,\n",
    "#         quantile_fraction=0.5,\n",
    "        hyperparam_bounds=neuron_config_space,\n",
    "        synch=True,\n",
    "        **experiment_metrics)\n",
    "#     search = DragonflySearch(\n",
    "#         optimizer=\"bandit\",\n",
    "#         domain=\"euclidean\",\n",
    "# #         space=neuron_config_space,\n",
    "# #         space=cfl,\n",
    "# #         max_concurrent=8,\n",
    "#         **experiment_metrics)\n",
    "#     search = ConcurrencyLimiter(search, max_concurrent=8)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "#         parameter_columns=neuron_config_space.get_hyperparameter_names(),\n",
    "        parameter_columns=parameter_columns,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar),\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        config=param_dict,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    \n",
    "    def cv_discrim(s): return 'conv_subindex_' in s\n",
    "    def fc_discrim(s): return 'full_subindex_' in s\n",
    "    def other_discrim(s): return 'subindex' not in s\n",
    "    best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "    best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "    best_other = [best_trial.config[hp] for hp in list(filter(other_discrim, best_trial.config.keys()))]\n",
    "\n",
    "    cfg = decode([best_cvs, best_fcs])\n",
    "    \n",
    "    conv_report = [\"Connolutional Layer %s: %s\" % (i,c) for i,c in enumerate(cfg[0],1)]\n",
    "    full_report = [\"Fully-connected Layer %s: %s\" % (i,f) for i,f in enumerate(cfg[1],1)]\n",
    "    other_report = [\"%s: %s\" % (hp,f) for (hp,f) in zip([\"Batch Size\",\"Epochs\",\"Learning Rate\"],best_other)]\n",
    "\n",
    "#     print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial config:\")\n",
    "    [print(best) for best in [conv_report,full_report,other_report]]\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = Net(cfg)\n",
    "    best_training_hyperparameters = zip([\"Batch Size\",\"Epochs\",\"Learning Rate\"],best_other)\n",
    "    \n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    \n",
    "    if checkpoint_dir != None:\n",
    "        tune.report(accuracy=test_acc)\n",
    "    \n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return (best_trained_model.state_dict(), dict(best_training_hyperparameters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,trainers = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
