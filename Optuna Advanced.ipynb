{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Advanced Configurations\n",
    "\n",
    "## Defining Parameter Spaces\n",
    "\n",
    "Optuna supports five kinds of parameters.\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    def objective(trial):\n",
    "        # Categorical parameter\n",
    "        optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])\n",
    "\n",
    "        # Int parameter\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "        # Uniform parameter\n",
    "        dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)\n",
    "\n",
    "        # Loguniform parameter\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "        # Discrete-uniform parameter\n",
    "        drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1)\n",
    "\n",
    "        ...\n",
    "\n",
    "## Branches and Loops\n",
    "\n",
    "You can use branches or loops depending on the parameter values.\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    def objective(trial):\n",
    "        classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n",
    "        if classifier_name == 'SVC':\n",
    "            svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
    "            classifier_obj = sklearn.svm.SVC(C=svc_c)\n",
    "        else:\n",
    "            rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
    "            classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth)\n",
    "\n",
    "        ...\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "    def create_model(trial):\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            n_units = int(trial.suggest_loguniform('n_units_l{}'.format(i), 4, 128))\n",
    "            layers.append(L.Linear(None, n_units))\n",
    "            layers.append(F.relu)\n",
    "        layers.append(L.Linear(None, 10))\n",
    "\n",
    "        return chainer.Sequential(*layers)\n",
    "\n",
    "Please also refer to `examples <https://github.com/optuna/optuna/tree/master/examples>`_.\n",
    "\n",
    "\n",
    "### Note on the Number of Parameters\n",
    "\n",
    "The difficulty of optimization increases roughly exponentially with regard to the number of parameters. That is, the number of necessary trials increases exponentially when you increase the number of parameters, so it is recommended to not add unimportant parameters.\n",
    "\n",
    "\n",
    "## Arguments for `Study.optimize`\n",
    "\n",
    "The method :func:`~optuna.study.Study.optimize` (and ``optuna study optimize`` CLI command as well)\n",
    "has several useful options such as ``timeout``.\n",
    "For details, please refer to the API reference for :func:`~optuna.study.Study.optimize`.\n",
    "\n",
    "**FYI**: If you give neither ``n_trials`` nor ``timeout`` options, the optimization continues until it receives a termination signal such as Ctrl+C or SIGTERM.\n",
    "This is useful for use cases such as when it is hard to estimate the computational costs required to optimize your objective function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 28 * 28\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
