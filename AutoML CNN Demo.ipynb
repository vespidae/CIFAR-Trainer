{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import permutations\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "## set number (or fraction) of GPUs (per training loop) you'd like to utilize if any at all\n",
    "cpu_use = 1\n",
    "gpu_use = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define feature space for hashing\n",
    "def feature_spacing():\n",
    "    conv = set(range(3**5)) - set(range(3**2))\n",
    "    full = set(range(2**5)) - set(range(2**2))\n",
    "    \n",
    "    c = 3**5 - 3**2\n",
    "    f = 2**5 - 2**2\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "    unique_combinations = []\n",
    "    total_uniques = 0\n",
    "    total_points = 1\n",
    "    \n",
    "    # do combo\n",
    "#     for combo in product(conv,conv,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "\n",
    "    for ls in range(0,4):\n",
    "#         print(ls)\n",
    "        unique_combinations.append((c**2)*(f*(f+1)**ls))\n",
    "        total_uniques += (c**2)*f*((f+1)**ls)\n",
    "#         total_points = ((c**2)*f*((f+1)**ls))\n",
    "    \n",
    "    total_uniques -= ((c**2)*f)\n",
    "    total_points = total_uniques**2\n",
    "#     print(\"number of combos: %s\" % [\"%s-fc model: %s\" % (l,v) for l,v in enumerate(unique_combinations, 1)])\n",
    "#     print(\"total uniques:\",total_uniques)\n",
    "#     print(\"number of points/indices (with sparicities/noise): %s\" % total_points)\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    results = {\n",
    "        \"base\": [1],\n",
    "        \"nodes_req\": [total_uniques],\n",
    "        \"sparcity\": [total_points - total_uniques],\n",
    "        \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "        \"denoise_pcnt\":[0]\n",
    "    }\n",
    "    \n",
    "    report = pd.DataFrame(results)\n",
    "    \n",
    "#     print(report.to_string())\n",
    "    \n",
    "    for base in range(2,11):\n",
    "        results[\"base\"] = [base]\n",
    "        results[\"nodes_req\"] = [math.ceil(math.log(total_uniques,(base)))]\n",
    "# #         print(\"number of base %s complex nodes required:\" % (base), math.ceil(math.log(total_uniques,(base))))\n",
    "#         print(\"number of base %s complex nodes required:\" % (base), results[\"nodes_req\"])\n",
    "        results[\"sparcity\"] = [base**math.ceil(math.log(total_uniques,base)) - total_uniques]\n",
    "# #         print(\"sparcity:\",base**math.ceil(math.log(total_uniques,base)) - total_uniques,'points')\n",
    "#         print(\"sparcity:\",results[\"sparcity\"],'points')\n",
    "        results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(total_uniques,(base))) - base**math.log(total_uniques,(base)))/(base**math.ceil(math.log(total_uniques,(base))))*100]\n",
    "# #         print(\"sparcity percentage:\",(base**math.ceil(math.log(total_uniques,(base))) - base**math.log(total_uniques,(base)))/(base**math.ceil(math.log(total_uniques,(base))))*100,'%')\n",
    "#         print(\"sparcity percentage:\",results[\"sparcity percentage\"],'%')\n",
    "#         print(\"%s root-%s nodes per layer\" % (math.ceil(math.log(total_uniques,base+1)),base+1))\n",
    "#         print(\"\\n\")\n",
    "        results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(total_uniques,base)))**2)/total_points)*100)]\n",
    "# #         print(\"noise reduced from total points:\",math.floor(((total_points-(math.ceil(math.log(total_uniques,base)))**2)/total_points)*100),'%')\n",
    "#         print(\"noise reduced from total points:\",results[\"denoise_pcnt\"],'%')\n",
    "    \n",
    "        report = report.append(pd.DataFrame(results))\n",
    "#     for root in range(1,8):\n",
    "#         print(\"ceilinged %s-root (%s-value per number component) of combos with complex numbers: %s\\n\" % (root*2, root+1, [[math.ceil(combo**(1/(root*2))),\"sparsity: %s%s\" % ((math.ceil(combo**(1/(root*2))) - combo**(1/(root*2)))/combo**(1/(root*2))*100,'%')] for combo in unique_combinations]))\n",
    "    \n",
    "    print(report.sort_values([\"sparcity_pcnt\",\"nodes_req\",\"base\"]).to_string())\n",
    "#     report.head()\n",
    "    \n",
    "#     print(len(product(conv,conv,full)))\n",
    "#     print(f1)\n",
    "feature_spacing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base = 8\n",
    "c = 3**5 - 3**2\n",
    "f = 2**5 - 2**2\n",
    "def decode(code=None):\n",
    "    conv = []\n",
    "    full = []\n",
    "    \n",
    "    print(math.ceil(math.log(c,base)))\n",
    "    print(base**math.ceil(math.log(c,base)) - c)\n",
    "    print(math.ceil(math.log(f,base)))\n",
    "    print(base**math.ceil(math.log(f,base)) - f)\n",
    "    \n",
    "    model = [conv,full]\n",
    "#     return model\n",
    "    print()\n",
    "    \n",
    "decode()\n",
    "[print(math.log(278,b)) for b in range(2,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# math.sqrt(3**5)\n",
    "poss = ((2**5 - 2**2)+1)**3\n",
    "print(\"%s possibilities\" % poss)\n",
    "[print(\"%s root-%s nodes per layer\" % (math.ceil(math.log(poss,root)),root)) for root in range(2,5)]\n",
    "[print(\"%s root-%s nodes per layer\" % (math.log(poss,root),root)) for root in range(2,5)]\n",
    "# print(math.ceil(poss**(1/2)))\n",
    "# print(math.log(poss,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2**5 - 2**2)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = ['1', '2', '3']\n",
    "b = ['1', '2', '3']\n",
    "c = ['1', '2', '3']\n",
    "d = ['1', '2', '3']\n",
    "\n",
    "# for r in product(product(a, b, d),c): print(r)\n",
    "r = [comb for comb in product(a, b, d)]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers=[[6, 16],[120,84]]):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_convs,hidden_fcs = hidden_layers\n",
    "        print(hidden_convs)\n",
    "        print(hidden_fcs)\n",
    "        uf_input = 0\n",
    "        layer_list = OrderedDict()\n",
    "        \n",
    "        layer_list['conv1'] = nn.Conv2d(3, hidden_convs[0], 5)\n",
    "        layer_list['pool1'] = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        layer_input = layer_list['conv1'].out_channels\n",
    "        \n",
    "        for layer_num, channels in enumerate(hidden_convs[1:], 2):\n",
    "            layer_list[\"conv%s\" % layer_num]  = nn.Conv2d(layer_input, channels, 5)\n",
    "            layer_list[\"pool%s\" % layer_num] = nn.MaxPool2d(2, 2)\n",
    "            layer_input = layer_list[\"conv%s\" % layer_num].out_channels\n",
    "        \n",
    "        \n",
    "        layer_list[\"flat\"] = nn.Flatten()\n",
    "        \n",
    "        layer_list['fc1'] = nn.Linear(layer_input*5*5, hidden_fcs[0])\n",
    "        layer_list[\"relu1\"]  = nn.ReLU()\n",
    "        \n",
    "        layer_input = layer_list['fc1'].out_features\n",
    "        for (layer_num, features) in enumerate(hidden_fcs[1:], 2):\n",
    "            layer_list[\"fc%s\" % layer_num]  = nn.Linear(layer_input, features)\n",
    "            layer_list[\"relu%s\" % layer_num]  = nn.ReLU()\n",
    "            layer_input = layer_list[\"fc%s\" % layer_num].out_features\n",
    "            \n",
    "        \n",
    "        layer_list['fco'] = nn.Linear(hidden_fcs[-1], 10)\n",
    "    \n",
    "        self.layers = nn.Sequential(layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(neuron_config, checkpoint_dir=None):\n",
    "    \n",
    "    data_dir=d.absolute()\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "    fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "#     cvs = neuron_config[\"cvs\"]\n",
    "#     fcs = neuron_config[\"fcs\"]\n",
    "    \n",
    "    net = Net([cvs, fcs])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=neuron_config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(neuron_config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(neuron_config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=(correct / total))\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "def configure_neurons(num_convs,num_fcs):\n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    \n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(name=\"lr\", lower=1e-4, upper=1e-1, log=True))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"batch_size\", choices=[4, 8, 16, 32]))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"epochs\", choices=[20, 30, 40]))\n",
    "    \n",
    "    for hidden in range(2):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"cv%s\" % hidden, lower=3, upper=3**4))\n",
    "    \n",
    "    for hidden in range(num_fcs):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"fc%s\" % hidden, lower=2**2, upper=2**6))\n",
    "        \n",
    "    return config_space\n",
    "\n",
    "# def configure_neurons():\n",
    "#     config_space = {\n",
    "#         \"batch_size_seed\": tune.randint(2, 6),\n",
    "#         \"cv_seed\": tune.grid_search([2]),\n",
    "#         \"fc_seed\": tune.randint(2, 4),\n",
    "        \n",
    "#         \"lr\": tune.loguniform(1e-4,1e-1),\n",
    "#         \"batch_size\": tune.sample_from(lambda spec: 2**spec.config.batch_size_seed),\n",
    "#         \"epochs\": tune.qrandint(20, 40, 10),\n",
    "        \n",
    "#         \"cvs\": tune.sample_from(lambda spec: [tune.randint(3, 3**4) for layer in range(spec.config.cv_seed)]),\n",
    "#         \"fcs\": tune.sample_from(lambda spec: [tune.randint(2**2, 2**4) for layer in range(spec.config.fc_seed)])        \n",
    "#     }\n",
    "        \n",
    "#     return config_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron_config_space = configure_neurons()\n",
    "print(neuron_config_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons(layer_config, checkpoint_dir=None):\n",
    "    num_samples=40\n",
    "    max_num_epochs=40\n",
    "    gpus_per_trial=1\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        gpu_use = 0\n",
    "        num_samples = int(num_samples / 4)\n",
    "        max_num_epochs = int(max_num_epochs / 4)\n",
    "    \n",
    "#     print(layer_config)\n",
    "    \n",
    "    neuron_config_space = configure_neurons(layer_config[\"num_convs\"], layer_config[\"num_fcs\"])\n",
    "#     neuron_config_space = configure_neurons()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    scheduler = HyperBandForBOHB(\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        max_t=20,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        neuron_config_space,\n",
    "        max_concurrent=8,\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        **experiment_metrics)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "#         parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\", \"epochs\"],\n",
    "        parameter_columns=neuron_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar),\n",
    "        verbose=2,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "#         config=neuron_config_space,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "    best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "# #     best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    \n",
    "#     best_trained_model = Net(best_trial.config[\"cvs\"], best_trial.config[\"fcs\"])\n",
    "    best_trained_model = Net([best_cvs, best_fcs])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    \n",
    "    if checkpoint_dir != None:\n",
    "        tune.report(accuracy=test_acc)\n",
    "    \n",
    "#     with tune.checkpoint_dir(\"nodes\") as checkpoint_dir:\n",
    "#         path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         torch.save(best_trained_model.state_dict(), path)\n",
    "    \n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform layer count trials\n",
    "def search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir=d.absolute()\n",
    "    load_data(data_dir)\n",
    "    layer_config_space = CS.ConfigurationSpace()\n",
    "\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.Constant(\"num_convs\", value=2))\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.UniformIntegerHyperparameter(\"num_fcs\", lower=2, upper=2**2))\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "\n",
    "    scheduler = HyperBandForBOHB(\n",
    "        max_t=max_num_epochs,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        layer_config_space,\n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "    reporter = CLIReporter(\n",
    "#         overwrite=True,\n",
    "        parameter_columns=layer_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(search_neurons),\n",
    "        verbose=2,\n",
    "        name=\"layers\",\n",
    "        local_dir=r.absolute(),\n",
    "#         config=layer_config_space,\n",
    "        resources_per_trial={\"gpu\": gpus_per_trial},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net([best_trial.config[\"num_convs\"], best_trial.config[\"num_fcs\"]])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"),map_location=torch.device('cpu'))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "model = Net()\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/31.9 GiB<br>Using HyperBand: num_stopped=9 total_brackets=1\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=12, completed=100.0%): {TERMINATED: 10} <br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/11.18 GiB heap, 0.0/3.86 GiB objects<br>Result logdir: /home/francisn/Source/CIFAR-Trainer/ray_results/neurons<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  cv0</th><th style=\"text-align: right;\">  cv1</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  fc0</th><th style=\"text-align: right;\">  fc1</th><th style=\"text-align: right;\">  fc2</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DEFAULT_9a6c2a5a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   20</td><td style=\"text-align: right;\">   81</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">   42</td><td style=\"text-align: right;\">   26</td><td style=\"text-align: right;\">0.000271309</td><td style=\"text-align: right;\">1.22829 </td><td style=\"text-align: right;\">  0.557225</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_9b0b3d0c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   63</td><td style=\"text-align: right;\">   33</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">0.0352562  </td><td style=\"text-align: right;\">2.26892 </td><td style=\"text-align: right;\">  0.13095 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_9b119f44</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">   33</td><td style=\"text-align: right;\">   58</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   33</td><td style=\"text-align: right;\">   13</td><td style=\"text-align: right;\">   12</td><td style=\"text-align: right;\">0.00339777 </td><td style=\"text-align: right;\">0.888526</td><td style=\"text-align: right;\">  0.68905 </td><td style=\"text-align: right;\">                   8</td></tr>\n",
       "<tr><td>DEFAULT_9b1758ee</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   30</td><td style=\"text-align: right;\">   34</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">   57</td><td style=\"text-align: right;\">    4</td><td style=\"text-align: right;\">0.000171372</td><td style=\"text-align: right;\">2.31667 </td><td style=\"text-align: right;\">  0.1004  </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_9b20bd08</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   48</td><td style=\"text-align: right;\">   11</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">0.0605843  </td><td style=\"text-align: right;\">2.31241 </td><td style=\"text-align: right;\">  0.100175</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_1ea9a9b4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   51</td><td style=\"text-align: right;\">    7</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">   36</td><td style=\"text-align: right;\">   46</td><td style=\"text-align: right;\">0.00104955 </td><td style=\"text-align: right;\">1.29708 </td><td style=\"text-align: right;\">  0.53485 </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_3ffabea0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   65</td><td style=\"text-align: right;\">   58</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">   24</td><td style=\"text-align: right;\">   27</td><td style=\"text-align: right;\">0.00026537 </td><td style=\"text-align: right;\">0.616328</td><td style=\"text-align: right;\">  0.7841  </td><td style=\"text-align: right;\">                  12</td></tr>\n",
       "<tr><td>DEFAULT_501cade8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   21</td><td style=\"text-align: right;\">    4</td><td style=\"text-align: right;\">   24</td><td style=\"text-align: right;\">0.000378915</td><td style=\"text-align: right;\">2.2467  </td><td style=\"text-align: right;\">  0.17485 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_8b21a2e0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   52</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   18</td><td style=\"text-align: right;\">   59</td><td style=\"text-align: right;\">    9</td><td style=\"text-align: right;\">0.0281438  </td><td style=\"text-align: right;\">2.32141 </td><td style=\"text-align: right;\">  0.09945 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_a8b791ac</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   70</td><td style=\"text-align: right;\">   18</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">0.000372243</td><td style=\"text-align: right;\">1.87975 </td><td style=\"text-align: right;\">  0.315375</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 4<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DEFAULT_9a6c2a5a</td><td style=\"text-align: right;\">           1</td><td>/home/francisn/Source/CIFAR-Trainer/ray_results/neurons/DEFAULT_9a6c2a5a_1_batch_size=4,cv0=20,cv1=81,epochs=20,fc0=19,fc1=42,fc2=26,lr=0.00027131_2020-12-11_12-45-05/error.txt</td></tr>\n",
       "<tr><td>DEFAULT_9b0b3d0c</td><td style=\"text-align: right;\">           2</td><td>/home/francisn/Source/CIFAR-Trainer/ray_results/neurons/DEFAULT_9b0b3d0c_2_batch_size=16,cv0=63,cv1=33,epochs=20,fc0=44,fc1=16,fc2=37,lr=0.035256_2020-12-11_12-45-06/error.txt </td></tr>\n",
       "<tr><td>DEFAULT_9b119f44</td><td style=\"text-align: right;\">           2</td><td>/home/francisn/Source/CIFAR-Trainer/ray_results/neurons/DEFAULT_9b119f44_3_batch_size=32,cv0=33,cv1=58,epochs=30,fc0=33,fc1=13,fc2=12,lr=0.0033978_2020-12-11_12-45-06/error.txt</td></tr>\n",
       "<tr><td>DEFAULT_9b1758ee</td><td style=\"text-align: right;\">           1</td><td>/home/francisn/Source/CIFAR-Trainer/ray_results/neurons/DEFAULT_9b1758ee_4_batch_size=16,cv0=30,cv1=34,epochs=40,fc0=37,fc1=57,fc2=4,lr=0.00017137_2020-12-11_12-45-06/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-11 13:58:50,731\tINFO tune.py:439 -- Total run time: 4429.28 seconds (4425.35 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'batch_size': 4, 'cv0': 65, 'cv1': 58, 'epochs': 30, 'fc0': 56, 'fc1': 24, 'fc2': 27, 'lr': 0.00026536970873218964}\n",
      "Best trial final validation loss: 0.6163276231823954\n",
      "Best trial final validation accuracy: 0.7841\n",
      "[65, 58]\n",
      "[56, 24, 27]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.6643\n",
      "\n",
      "Processed in 74.33882950544357 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-12 02:38:37,803\tWARNING worker.py:1091 -- The node with node id f4a1eb8672b49703a544416e0591c57a2031f699 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m F1212 02:39:25.464478  1366  1366 node_manager.cc:777]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor: GCS didn't receive heartbeats within timeout 30000 ms. This is likely since the machine or raylet became overloaded.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9a24d3d  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9a261ac  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9a24a19  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9a24c31  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d99d92b9  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d96eabf4  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d96eadfc  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d97db5dc  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d97db8c6  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d97e554a  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d97e6f1b  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9d2770f  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9d28c11  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d9d29c42  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d963ccbc  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d81902e1  (unknown)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m     @     0x7f94d964f621  (unknown)\n"
     ]
    }
   ],
   "source": [
    "layer_config_space = {}\n",
    "\n",
    "# for hp in [\"num_convs\",\"num_fcs\"]:\n",
    "#     layer_config_space[hp] = np.random.randint(2,2**3)\n",
    "# layer_config_space[\"num_convs\"] = np.random.randint(2,3)\n",
    "layer_config_space[\"num_convs\"] = 2\n",
    "layer_config_space[\"num_fcs\"] = np.random.randint(3,2**2)\n",
    "\n",
    "cpu_use = 1\n",
    "gpu_use = 0.25\n",
    "# data_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/data/\")\n",
    "# checkpoint_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/checkpoints\")\n",
    "print(\"Resource usage can be viewed at 127.0.0.1:8265\")\n",
    "start = time.time()\n",
    "model = search_neurons(layer_config_space)\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\nProcessed in %s minutes\\n\" % ((end-start)/60,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf ./data/* ./ray_results/layers/* ./ray_results/neurons/* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
