{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial # for trials\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "\n",
    "import os # for paths\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "import numpy as np # for accuracy math\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "# from ray.tune.integration.torch import is_distributed_trainable\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "# from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "# from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import BoTorchSampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count() # number of cpu cores\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus\n",
    "cpu_use = 1 # number of cpu cores to dedicate to 1 series of trials\n",
    "gpu_use = gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "oom = 8 if gpus else 2 # order of magnitude\n",
    "num_samples = 2 ** oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    n_convs = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    n_fulls = trial.suggest_int(\"n_full_layers\", 1, 4)\n",
    "\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(n_convs):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "            kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(n_fulls):\n",
    "        out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "            p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(arch):\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(arch[\"n_conv_layers\"]):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = arch[\"n_conv_channels_c%s\" % i]\n",
    "            kernel_size = arch[\"kernel_size_c%s\" % i]\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if arch[\"has_max_pool_c%s\" % i] & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(arch[\"n_full_layers\"]):\n",
    "        out_features = arch[\"n_l_units_l%s\" % i]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if arch[\"has_dropout_l%s\" % i]:\n",
    "            p = arch[\"dropout_l%s\" % i]\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(non_arch_config,trial):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "\n",
    "    net = define_model(trial) if type(trial) == optuna.trial.Trial else Net(trial.params)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", trial.number, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                net.state_dict()\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "    return [loss,accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model nn based on HPO\n",
    "def model_cifar(non_arch_config,arch_config):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "    \n",
    "    print(arch_config)\n",
    "    net = Net(arch_config)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "    if gpus > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", Test, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "        \n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    _, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "nodes_c = bases[0][\"nodes_req\"]\n",
    "nodes_f = bases[1][\"nodes_req\"]\n",
    "max_c = bases[0][\"max_necc_base_value\"]\n",
    "max_f = bases[1][\"max_necc_base_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_training_hyperparameters():\n",
    "    lr = {\n",
    "        \"name\":\"learning rate {10^(-⌊x⌋)\",\n",
    "        \"bounds\":[x for x in range(1,4)]\n",
    "    }\n",
    "    batch_size = {\n",
    "        \"name\":\"batch size {2^⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(6,9)]\n",
    "    }\n",
    "    epochs = {\n",
    "        \"name\":\"epochs {10⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(2,6)]\n",
    "    }\n",
    "    \n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict,config_space_ray = {},{}\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            lr[\"name\"],\n",
    "            lr[\"bounds\"][0],\n",
    "            lr[\"bounds\"][-1],\n",
    "            log=True\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            batch_size[\"name\"], \n",
    "            batch_size[\"bounds\"]\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            epochs[\"name\"], \n",
    "            epochs[\"bounds\"]\n",
    "        ))\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_ray[lr[\"name\"]] = tune.loguniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_ray[batch_size[\"name\"]] = tune.choice(batch_size[\"bounds\"])\n",
    "    config_space_ray[epochs[\"name\"]] = tune.choice(categories=epochs[\"bounds\"])\n",
    "    \n",
    "    #start Dragonfly Search Space API\n",
    "    param_list = [\n",
    "        {\n",
    "            \"name\": lr[\"name\"], \n",
    "            \"type\": \"float\", \n",
    "            \"min\": lr[\"bounds\"][0], \n",
    "            \"max\": lr[\"bounds\"][-1]\n",
    "        },\n",
    "        {\n",
    "            \"name\": batch_size[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(2**x) for x in batch_size[\"bounds\"]])\n",
    "        },\n",
    "        {\n",
    "            \"name\": epochs[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(10*x) for x in epochs[\"bounds\"]])\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    #start BayesOpt Search Space API\n",
    "    config_space_dict[lr[\"name\"]] = tune.uniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_dict[batch_size[\"name\"]] = tune.uniform(lower=batch_size[\"bounds\"][0], upper=batch_size[\"bounds\"][-1])\n",
    "    config_space_dict[epochs[\"name\"]] = tune.uniform(lower=epochs[\"bounds\"][0], upper=epochs[\"bounds\"][-1])\n",
    "    \n",
    "    #start Discrete Search Search Space API\n",
    "    param_dict = {p[\"name\"]:p[\"bounds\"] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    #start PB2 Space API\n",
    "    min_max_param_dict = {p[\"name\"]:[p[\"bounds\"][0], p[\"bounds\"][-1]] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    return config_space_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(search_training_hyperparameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "def nas_report(study,trial):\n",
    "    best_session = study.best_trials[0]\n",
    "    print(\"Trial stats (#{}):    Loss={}    Accuracy={}\".format(trial.number,*(list(best_session.values))))\n",
    "    print(\"Best params so far (#{}):    {}\".format(best_session.number,best_session.params))\n",
    "\n",
    "    finished_trials = list(filter(\n",
    "        (lambda trial: trial.state.is_finished()),\n",
    "        study.trials\n",
    "    ))\n",
    "\n",
    "    model_state = {}\n",
    "    with tune.checkpoint_dir(step=best_session.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state = torch.load(path)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                best_session.params,\n",
    "                model_state\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "\n",
    "    \n",
    "    result_zip = zip([\"loss\",\"accuracy\"], list(best_session.values))\n",
    "    results = {p:v for p,v in result_zip}\n",
    "    tune.report(**results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neural_arch(non_arch_config,checkpoint_dir=None):\n",
    "\n",
    "    optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions=[\"minimize\",\"maximize\"],\n",
    "        study_name=str(non_arch_config),\n",
    "        sampler=BoTorchSampler(),\n",
    "        pruner=SuccessiveHalvingPruner(),\n",
    "        storage='sqlite:///na.db',\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        partial(train_cifar, non_arch_config),\n",
    "        n_trials=oom,\n",
    "#         n_jobs=2,\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[nas_report]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons():\n",
    "    neuron_config_space = search_training_hyperparameters()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "\n",
    "    hpn = list(neuron_config_space.keys())\n",
    "    \n",
    "    #pre-load data to avoid races\n",
    "    load_data()\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=oom,\n",
    "        reduction_factor=2,\n",
    "#         grace_period=3,\n",
    "        **experiment_metrics)\n",
    "    search = BayesOptSearch(\n",
    "        **experiment_metrics)\n",
    "    search = ConcurrencyLimiter(\n",
    "        search,\n",
    "        max_concurrent=max_concurrent_trials)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=hpn,\n",
    "#         max_progress_rows=num_samples,\n",
    "        max_report_frequency=10,\n",
    "        **experiment_metrics)\n",
    "    result = tune.run(\n",
    "        search_neural_arch,\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "#         max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        config=neuron_config_space,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    \n",
    "\n",
    "    print(\"Best training hyperparameters: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    arch_state, model_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "\n",
    "    best_trained_model = Net(arch_state)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
