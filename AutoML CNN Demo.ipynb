{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial # for trials\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "\n",
    "import os # for paths\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "import numpy as np # for accuracy math\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "# from ray.tune.integration.torch import is_distributed_trainable\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "# from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "# from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import BoTorchSampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count() # number of cpu cores\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus\n",
    "cpu_use = 1 # number of cpu cores to dedicate to 1 series of trials\n",
    "gpu_use = gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "oom = 5 if gpus else 2 # order of magnitude\n",
    "num_samples = 2 ** oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    n_convs = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    n_fulls = trial.suggest_int(\"n_full_layers\", 1, 4)\n",
    "\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(n_convs):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "            kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(n_fulls):\n",
    "        out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "            p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(arch):\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(arch[\"n_conv_layers\"]):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = arch[\"n_conv_channels_c%s\" % i]\n",
    "            kernel_size = arch[\"kernel_size_c%s\" % i]\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if arch[\"has_max_pool_c%s\" % i] & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(arch[\"n_full_layers\"]):\n",
    "        out_features = arch[\"n_l_units_l%s\" % i]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if arch[\"has_dropout_l%s\" % i]:\n",
    "            p = arch[\"dropout_l%s\" % i]\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(non_arch_config,trial):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "\n",
    "    net = define_model(trial) if type(trial) == optuna.trial.Trial else Net(trial.params)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", trial.number, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                net.state_dict()\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "    return [loss,accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model nn based on HPO\n",
    "def model_cifar(non_arch_config,arch_config):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "    \n",
    "    print(arch_config)\n",
    "    net = Net(arch_config)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "    if gpus > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", Test, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "        \n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    _, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_training_hyperparameters():\n",
    "    lr = {\n",
    "        \"name\":\"learning rate {10^(-⌊x⌋)\",\n",
    "        \"bounds\":[x for x in range(1,4)]\n",
    "    }\n",
    "    batch_size = {\n",
    "        \"name\":\"batch size {2^⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(6,9)]\n",
    "    }\n",
    "    epochs = {\n",
    "        \"name\":\"epochs {10⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(2,6)]\n",
    "    }\n",
    "    \n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict,config_space_ray = {},{}\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            lr[\"name\"],\n",
    "            lr[\"bounds\"][0],\n",
    "            lr[\"bounds\"][-1],\n",
    "            log=True\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            batch_size[\"name\"], \n",
    "            batch_size[\"bounds\"]\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            epochs[\"name\"], \n",
    "            epochs[\"bounds\"]\n",
    "        ))\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_ray[lr[\"name\"]] = tune.loguniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_ray[batch_size[\"name\"]] = tune.choice(batch_size[\"bounds\"])\n",
    "    config_space_ray[epochs[\"name\"]] = tune.choice(categories=epochs[\"bounds\"])\n",
    "    \n",
    "    #start Dragonfly Search Space API\n",
    "    param_list = [\n",
    "        {\n",
    "            \"name\": lr[\"name\"], \n",
    "            \"type\": \"float\", \n",
    "            \"min\": lr[\"bounds\"][0], \n",
    "            \"max\": lr[\"bounds\"][-1]\n",
    "        },\n",
    "        {\n",
    "            \"name\": batch_size[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(2**x) for x in batch_size[\"bounds\"]])\n",
    "        },\n",
    "        {\n",
    "            \"name\": epochs[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(10*x) for x in epochs[\"bounds\"]])\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    #start BayesOpt Search Space API\n",
    "    config_space_dict[lr[\"name\"]] = tune.uniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_dict[batch_size[\"name\"]] = tune.uniform(lower=batch_size[\"bounds\"][0], upper=batch_size[\"bounds\"][-1])\n",
    "    config_space_dict[epochs[\"name\"]] = tune.uniform(lower=epochs[\"bounds\"][0], upper=epochs[\"bounds\"][-1])\n",
    "    \n",
    "    #start Discrete Search Search Space API\n",
    "    param_dict = {p[\"name\"]:p[\"bounds\"] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    #start PB2 Space API\n",
    "    min_max_param_dict = {p[\"name\"]:[p[\"bounds\"][0], p[\"bounds\"][-1]] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    return config_space_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "def nas_report(study,trial):\n",
    "    best_session = study.best_trials[0]\n",
    "    print(\"Trial stats (#{}):    Loss={}    Accuracy={}\".format(trial.number,*(list(best_session.values))))\n",
    "    print(\"Best params so far (#{}):    {}\".format(best_session.number,best_session.params))\n",
    "\n",
    "    finished_trials = list(filter(\n",
    "        (lambda trial: trial.state.is_finished()),\n",
    "        study.trials\n",
    "    ))\n",
    "\n",
    "    model_state = {}\n",
    "    with tune.checkpoint_dir(step=best_session.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state = torch.load(path)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                best_session.params,\n",
    "                model_state\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "\n",
    "    \n",
    "    result_zip = zip([\"loss\",\"accuracy\"], list(best_session.values))\n",
    "    results = {p:v for p,v in result_zip}\n",
    "    tune.report(**results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neural_arch(non_arch_config,checkpoint_dir=None):\n",
    "\n",
    "    optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions=[\"minimize\",\"maximize\"],\n",
    "        study_name=str(non_arch_config),\n",
    "        sampler=BoTorchSampler(),\n",
    "        pruner=SuccessiveHalvingPruner(),\n",
    "        storage='sqlite:///na.db',\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        partial(train_cifar, non_arch_config),\n",
    "        n_trials=oom,\n",
    "#         n_jobs=2,\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[nas_report]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons():\n",
    "    neuron_config_space = search_training_hyperparameters()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "\n",
    "    hpn = list(neuron_config_space.keys())\n",
    "    \n",
    "    #pre-load data to avoid races\n",
    "    load_data()\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=oom,\n",
    "        reduction_factor=2,\n",
    "#         grace_period=3,\n",
    "        **experiment_metrics)\n",
    "    search = BayesOptSearch(\n",
    "        **experiment_metrics)\n",
    "    search = ConcurrencyLimiter(\n",
    "        search,\n",
    "        max_concurrent=max_concurrent_trials)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=hpn,\n",
    "#         max_progress_rows=num_samples,\n",
    "        max_report_frequency=10,\n",
    "        **experiment_metrics)\n",
    "    result = tune.run(\n",
    "        search_neural_arch,\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        config=neuron_config_space,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    \n",
    "\n",
    "    print(\"Best training hyperparameters: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    first, second = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "    \n",
    "    arch_state,model_state = {}\n",
    "    if (type(second) == tuple): \n",
    "        arch_state,model_state = second\n",
    "    else:\n",
    "        arch_state,model_state = first,second\n",
    "\n",
    "    best_trained_model = Net(arch_state)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\n"
     ]
    }
   ],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.9/31.3 GiB<br>Using AsyncHyperBand: num_stopped=31\n",
       "Bracket: Iter 4.000: 0.8808 | Iter 2.000: 0.7991125 | Iter 1.000: 0.6227750000000001<br>Resources requested: 0/8 CPUs, 0.0/2 GPUs, 0.0/17.53 GiB heap, 0.0/6.05 GiB objects (0/1.0 accelerator_type:RTX)<br>Current best trial: 6f545958 with accuracy=0.99985 and parameters={'learning rate {10^(-⌊x⌋)': 2.2022300234864174, 'batch size {2^⌊x⌋}': 6.116167224336399, 'epochs {10⌊x⌋}': 4.598528437324806}<br>Result logdir: /home/grottesco/Source/CIFAR-Trainer/ray_results/neurons<br>Number of trials: 32/32 (1 ERROR, 31 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  learning rate {10^(-⌊x⌋)</th><th style=\"text-align: right;\">  batch size {2^⌊x⌋}</th><th style=\"text-align: right;\">  epochs {10⌊x⌋}</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>search_neural_arch_6f4699f8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.46399</td><td style=\"text-align: right;\">             6.74908</td><td style=\"text-align: right;\">         4.85214</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2972.37 </td><td style=\"text-align: right;\">2.30277   </td><td style=\"text-align: right;\">  0.09955 </td></tr>\n",
       "<tr><td>search_neural_arch_6f507946</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.31199</td><td style=\"text-align: right;\">             7.19732</td><td style=\"text-align: right;\">         2.46806</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         448.912</td><td style=\"text-align: right;\">1.25881   </td><td style=\"text-align: right;\">  0.5493  </td></tr>\n",
       "<tr><td>search_neural_arch_6f545958</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.20223</td><td style=\"text-align: right;\">             6.11617</td><td style=\"text-align: right;\">         4.59853</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        5756    </td><td style=\"text-align: right;\">0.00533684</td><td style=\"text-align: right;\">  0.99985 </td></tr>\n",
       "<tr><td>search_neural_arch_6f570ac2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.93982</td><td style=\"text-align: right;\">             7.41615</td><td style=\"text-align: right;\">         2.06175</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2227.37 </td><td style=\"text-align: right;\">1.79122   </td><td style=\"text-align: right;\">  0.361975</td></tr>\n",
       "<tr><td>search_neural_arch_6f589446</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.36365</td><td style=\"text-align: right;\">             7.66489</td><td style=\"text-align: right;\">         2.63702</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        2308.8  </td><td style=\"text-align: right;\">0.60581   </td><td style=\"text-align: right;\">  0.79185 </td></tr>\n",
       "<tr><td>search_neural_arch_6f5de2d4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.04951</td><td style=\"text-align: right;\">             6.36681</td><td style=\"text-align: right;\">         2.91273</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        2403.21 </td><td style=\"text-align: right;\">0.598727  </td><td style=\"text-align: right;\">  0.798175</td></tr>\n",
       "<tr><td>search_neural_arch_6f5ece06</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.22371</td><td style=\"text-align: right;\">             6.86389</td><td style=\"text-align: right;\">         2.87369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1187.75 </td><td style=\"text-align: right;\">0.881522  </td><td style=\"text-align: right;\">  0.697325</td></tr>\n",
       "<tr><td>search_neural_arch_6f62e23e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.73272</td><td style=\"text-align: right;\">             6.27899</td><td style=\"text-align: right;\">         2.87643</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         728.784</td><td style=\"text-align: right;\">1.68346   </td><td style=\"text-align: right;\">  0.395025</td></tr>\n",
       "<tr><td>search_neural_arch_7c56e804</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.39935</td><td style=\"text-align: right;\">             6.91214</td><td style=\"text-align: right;\">         4.35553</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4021.26 </td><td style=\"text-align: right;\">1.17313   </td><td style=\"text-align: right;\">  0.59085 </td></tr>\n",
       "<tr><td>search_neural_arch_231cea94</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.0929 </td><td style=\"text-align: right;\">             7.02847</td><td style=\"text-align: right;\">         3.77724</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         793.711</td><td style=\"text-align: right;\">1.07031   </td><td style=\"text-align: right;\">  0.633575</td></tr>\n",
       "<tr><td>search_neural_arch_d78f62f8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.97179</td><td style=\"text-align: right;\">             7.94271</td><td style=\"text-align: right;\">         3.89719</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1122.42 </td><td style=\"text-align: right;\">1.51655   </td><td style=\"text-align: right;\">  0.460125</td></tr>\n",
       "<tr><td>search_neural_arch_d7970602</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.17303</td><td style=\"text-align: right;\">             6.32157</td><td style=\"text-align: right;\">         3.19716</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        9026.14 </td><td style=\"text-align: right;\">0.279558  </td><td style=\"text-align: right;\">  0.9034  </td></tr>\n",
       "<tr><td>search_neural_arch_d7a1a62a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.28557</td><td style=\"text-align: right;\">             6.65046</td><td style=\"text-align: right;\">         3.43732</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        7932.3  </td><td style=\"text-align: right;\">0.44752   </td><td style=\"text-align: right;\">  0.844225</td></tr>\n",
       "<tr><td>search_neural_arch_d7a7fc1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.44259</td><td style=\"text-align: right;\">             7.88598</td><td style=\"text-align: right;\">         3.10373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         788.2  </td><td style=\"text-align: right;\">1.20804   </td><td style=\"text-align: right;\">  0.577775</td></tr>\n",
       "<tr><td>search_neural_arch_d7b18d92</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.82645</td><td style=\"text-align: right;\">             7.78819</td><td style=\"text-align: right;\">         4.58453</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1787.12 </td><td style=\"text-align: right;\">0.826863  </td><td style=\"text-align: right;\">  0.711   </td></tr>\n",
       "<tr><td>search_neural_arch_d7baad8c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.92601</td><td style=\"text-align: right;\">             6.66464</td><td style=\"text-align: right;\">         4.03973</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        8409.53 </td><td style=\"text-align: right;\">0.268052  </td><td style=\"text-align: right;\">  0.91735 </td></tr>\n",
       "<tr><td>search_neural_arch_d7c33e20</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.1914 </td><td style=\"text-align: right;\">             6.18886</td><td style=\"text-align: right;\">         4.4038 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2438.17 </td><td style=\"text-align: right;\">0.806906  </td><td style=\"text-align: right;\">  0.7198  </td></tr>\n",
       "<tr><td>search_neural_arch_d7cd6a58</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.51515</td><td style=\"text-align: right;\">             6.80607</td><td style=\"text-align: right;\">         4.52208</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        8693.98 </td><td style=\"text-align: right;\">0.357147  </td><td style=\"text-align: right;\">  0.8808  </td></tr>\n",
       "<tr><td>search_neural_arch_afaa393c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.95228</td><td style=\"text-align: right;\">             7.22068</td><td style=\"text-align: right;\">         2.66437</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         794.737</td><td style=\"text-align: right;\">1.92065   </td><td style=\"text-align: right;\">  0.296675</td></tr>\n",
       "<tr><td>search_neural_arch_76cabcda</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.18331</td><td style=\"text-align: right;\">             6.08172</td><td style=\"text-align: right;\">         3.46666</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         816.277</td><td style=\"text-align: right;\">1.53429   </td><td style=\"text-align: right;\">  0.446775</td></tr>\n",
       "<tr><td>search_neural_arch_8d35092a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.18768</td><td style=\"text-align: right;\">             6.61657</td><td style=\"text-align: right;\">         4.58325</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4829.37 </td><td style=\"text-align: right;\">0.684468  </td><td style=\"text-align: right;\">  0.75935 </td></tr>\n",
       "<tr><td>search_neural_arch_031afdde</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.91539</td><td style=\"text-align: right;\">             7.66086</td><td style=\"text-align: right;\">         3.91788</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1266.05 </td><td style=\"text-align: right;\">1.30587   </td><td style=\"text-align: right;\">  0.535275</td></tr>\n",
       "<tr><td>search_neural_arch_60b7628e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.00896</td><td style=\"text-align: right;\">             6.53642</td><td style=\"text-align: right;\">         2.76586</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         897.3  </td><td style=\"text-align: right;\">2.00171   </td><td style=\"text-align: right;\">  0.2516  </td></tr>\n",
       "<tr><td>search_neural_arch_8729add6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.84973</td><td style=\"text-align: right;\">             7.22471</td><td style=\"text-align: right;\">         2.18454</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1142.74 </td><td style=\"text-align: right;\">1.10684   </td><td style=\"text-align: right;\">  0.611975</td></tr>\n",
       "<tr><td>search_neural_arch_7a32f852</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.25597</td><td style=\"text-align: right;\">             7.1847 </td><td style=\"text-align: right;\">         4.15886</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1805.41 </td><td style=\"text-align: right;\">1.80138   </td><td style=\"text-align: right;\">  0.3486  </td></tr>\n",
       "<tr><td>search_neural_arch_f9c66018</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.8206 </td><td style=\"text-align: right;\">             7.89143</td><td style=\"text-align: right;\">         3.35427</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2097.6  </td><td style=\"text-align: right;\">0.601059  </td><td style=\"text-align: right;\">  0.80005 </td></tr>\n",
       "<tr><td>search_neural_arch_330da3da</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.20547</td><td style=\"text-align: right;\">             6.13478</td><td style=\"text-align: right;\">         4.57674</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1106.17 </td><td style=\"text-align: right;\">1.55248   </td><td style=\"text-align: right;\">  0.46155 </td></tr>\n",
       "<tr><td>search_neural_arch_b09072c2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.75703</td><td style=\"text-align: right;\">             6.67266</td><td style=\"text-align: right;\">         2.17438</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2316.02 </td><td style=\"text-align: right;\">0.818461  </td><td style=\"text-align: right;\">  0.72015 </td></tr>\n",
       "<tr><td>search_neural_arch_ca0d97d4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   1.64155</td><td style=\"text-align: right;\">             7.76056</td><td style=\"text-align: right;\">         4.51311</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1120.08 </td><td style=\"text-align: right;\">2.07969   </td><td style=\"text-align: right;\">  0.20485 </td></tr>\n",
       "<tr><td>search_neural_arch_de5870be</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.01047</td><td style=\"text-align: right;\">             6.6582 </td><td style=\"text-align: right;\">         4.8892 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3865.34 </td><td style=\"text-align: right;\">1.01131   </td><td style=\"text-align: right;\">  0.6432  </td></tr>\n",
       "<tr><td>search_neural_arch_cefed664</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">                   2.87178</td><td style=\"text-align: right;\">             7.05351</td><td style=\"text-align: right;\">         2.66373</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         930.53 </td><td style=\"text-align: right;\">1.52962   </td><td style=\"text-align: right;\">  0.4544  </td></tr>\n",
       "<tr><td>search_neural_arch_6814eb2e</td><td>ERROR     </td><td>     </td><td style=\"text-align: right;\">                   2.27685</td><td style=\"text-align: right;\">             7.53766</td><td style=\"text-align: right;\">         2.2449 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         688.909</td><td style=\"text-align: right;\">0.515731  </td><td style=\"text-align: right;\">  0.827525</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>search_neural_arch_6814eb2e</td><td style=\"text-align: right;\">           1</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_6814eb2e_31_batch size {2^⌊x⌋}=7.5377,epochs {10⌊x⌋}=2.2449,learning rate {10^(-⌊x⌋)=2.2769_2021-01-30_03-16-09/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [search_neural_arch_6814eb2e])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97e1e1f81862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c8e3eb1d0702>\u001b[0m in \u001b[0;36msearch_neurons\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmax_report_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         **experiment_metrics)\n\u001b[0;32m---> 28\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msearch_neural_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [search_neural_arch_6814eb2e])"
     ]
    }
   ],
   "source": [
    "model = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
