{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/dragonfly/utils/oper_utils.py:30: UserWarning: cannot import name 'direct' from 'dragonfly.utils.direct_fortran' (/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/dragonfly/utils/direct_fortran/__init__.py)\n",
      "Could not import Fortran direct library. Dragonfly can still be used, but might be slightly slower. To get rid of this warning, install a numpy compatible Fortran compiler (e.g. gfortran) and the python-dev package and reinstall Dragonfly.\n",
      "  warn('%s\\n%s'%(e, fortran_err_msg))\n"
     ]
    }
   ],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from dragonfly.opt.multiobjective_gp_bandit import CPMultiObjectiveGPBandit\n",
    "\n",
    "from dragonfly.opt.gp_bandit import EuclideanGPBandit\n",
    "from dragonfly.exd.experiment_caller import EuclideanFunctionCaller\n",
    "from dragonfly.opt.gp_bandit import CPGPBandit\n",
    "from dragonfly.exd.experiment_caller import CPFunctionCaller\n",
    "from dragonfly.opt.gp_bandit import EuclideanMFGPFitter\n",
    "from dragonfly.exd.experiment_caller import CPMultiFunctionCaller\n",
    "from dragonfly import load_config\n",
    "\n",
    "from  ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "gpus = torch.cuda.device_count()\n",
    "device = \"cuda:0\" if gpus else \"cpu\"\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "cpu_use = 1 if gpus else 0.5\n",
    "gpu_use = 0.25 if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "num_samples = 2 ** (6 if gpus else 4)\n",
    "max_time = 10 * (4 if gpus else 1)\n",
    "gpus_per_trial = 1 if gpus else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neuron configuration we want is dependent upon the number of layers we have, we need to work flatten the feature space a bit. We can reduce the high-dminesional setups to a slightly less high-dminesional string of base-n nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base                              13\n",
      "nodes_req                          4\n",
      "sparcity                        1066\n",
      "max_necc_base_value    [12, 6, 9, 0]\n",
      "nodes+_req                         3\n",
      "subsparcity                    25298\n",
      "unexplained                    75894\n",
      "sparcity_pcnt                3.73236\n",
      "subsparcity_pcnt             1151.48\n",
      "denoise_pcnt                   -1500\n",
      "complexity                      4268\n",
      "Name: 13, dtype: object \n",
      "\n",
      "base                                  15\n",
      "nodes_req                              5\n",
      "sparcity                          124000\n",
      "max_necc_base_value    [12, 8, 3, 13, 5]\n",
      "nodes+_req                             4\n",
      "subsparcity                       584750\n",
      "unexplained                      2339000\n",
      "sparcity_pcnt                    16.3292\n",
      "subsparcity_pcnt                 1155.06\n",
      "denoise_pcnt                       -2400\n",
      "complexity                        620005\n",
      "Name: 15, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define feature space for hashing\n",
    "c_min = 3**2\n",
    "c_max = 3**5\n",
    "f_min = 2**2\n",
    "f_max = 2**6\n",
    "\n",
    "c = c_max - c_min\n",
    "f = f_max - f_min\n",
    "\n",
    "# conv = set(range(c_max)) - set(range(c_min))\n",
    "# full = set(range(f_max)) - set(range(f_min))\n",
    "conv = range(c_max)[c_min:]\n",
    "full = range(f_max)[f_min:]\n",
    "\n",
    "c_comb = list(combinations_with_replacement(conv,2))\n",
    "f_comb = []\n",
    "for layers in range(1,5):\n",
    "    f_comb += list(combinations_with_replacement(full,layers))\n",
    "#     print(\"Fully connected layer %s range: %s\" % (layers,len(f_comb)) )\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# for conversion from dec to whatever we end up using\n",
    "# most to least significant digit\n",
    "def numberToBase(n, b):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    digits = []\n",
    "    while n:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    rev = digits[::-1]\n",
    "    return rev\n",
    "\n",
    "def feature_spacing():\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "    unique_combinations = list(combinations([c_comb,f_comb],2))\n",
    "    total_uniques = len(unique_combinations)\n",
    "    total_points = total_uniques**2\n",
    "    total_cvs = len(c_comb)\n",
    "    total_fcs = len(f_comb)\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    \n",
    "    cf = []\n",
    "    \n",
    "    for layer in [total_cvs,total_fcs]:#,total_uniques]:\n",
    "        results = {\n",
    "            \"base\": [1],\n",
    "            \"nodes_req\": [total_uniques],\n",
    "            \"sparcity\": [total_points - total_uniques],\n",
    "            \"max_necc_base_value\":[0],\n",
    "            \"nodes+_req\": [0],\n",
    "            \"subsparcity\": [0],\n",
    "            \"unexplained\":[0],\n",
    "            \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "            \"subsparcity_pcnt\": [0],\n",
    "            \"denoise_pcnt\":[0],\n",
    "            \"complexity\":[0]\n",
    "        }\n",
    "\n",
    "        report = pd.DataFrame(results)\n",
    "    \n",
    "        for base in range(2,17):\n",
    "            results[\"base\"] = [base]\n",
    "            results[\"nodes_req\"] = [math.ceil(math.log(layer,(base)))]\n",
    "            results[\"nodes+_req\"] = [math.floor(math.log(layer,(base)))]\n",
    "            \n",
    "            results[\"sparcity\"] = [base**math.ceil(math.log(layer,base)) - layer]\n",
    "            results[\"subsparcity\"] = [-(base**math.floor(math.log(layer,base)) - layer)]\n",
    "            \n",
    "            results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.ceil(math.log(layer,(base))))*100]\n",
    "            results[\"subsparcity_pcnt\"] = [-((base**math.floor(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.floor(math.log(layer,(base))))*100)]\n",
    "            \n",
    "#             results[\"max_necc_base_value\"] = [numberToBase((results[\"base\"][0]**results[\"nodes+_req\"][0]+results[\"subsparcity\"][0]),results[\"base\"][0])]\n",
    "            results[\"max_necc_base_value\"] = [numberToBase(layer,base)]\n",
    "            results[\"unexplained\"] = [(-(base**math.floor(math.log(layer,base)) - layer))*(math.floor(math.log(layer,(base))))]\n",
    "            \n",
    "            results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(layer,base)))**2)/total_points)*100)]\n",
    "        \n",
    "            results[\"complexity\"] = [results[\"nodes_req\"][0]*(results[\"sparcity\"][0]+1)]\n",
    "\n",
    "            report = report.append(pd.DataFrame(results))\n",
    "            \n",
    "            \n",
    "        report.index = [x for x in range(1, len(report.values)+1)]\n",
    "        report.drop([1],axis=0,inplace=True)\n",
    "        report.sort_values([\"sparcity\",\"unexplained\",\"nodes+_req\",\"subsparcity\",\"sparcity_pcnt\",\"base\"],inplace=True)\n",
    "        \n",
    "        cf.append(report.iloc[0])\n",
    "    \n",
    "    return cf\n",
    "\n",
    "bases = feature_spacing()\n",
    "[print(r,\"\\n\") for r in bases]\n",
    "\n",
    "base_c = bases[0][\"base\"]\n",
    "base_f = bases[1][\"base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the convolutional layers, base 13 seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\n",
      "For the linear layers, base 15 seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\n"
     ]
    }
   ],
   "source": [
    "print(\"For the convolutional layers, base %s seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\" % bases[0][\"base\"])\n",
    "print(\"For the linear layers, base %s seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\" % bases[1][\"base\"])\n",
    "\n",
    "# print(\"We can use the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_to_dec(num_list, base):\n",
    "    num_list = num_list[::-1]\n",
    "    num = 0\n",
    "    for k in range(len(num_list)):\n",
    "        dig = num_list[k]\n",
    "        dig = int(dig)\n",
    "        num += dig*(base**k)\n",
    "    return num\n",
    "\n",
    "def encode(config=[(24, 64),(13, 41)]):\n",
    "    iconv = c_comb.index(config[0])\n",
    "    ifull = f_comb.index(config[1])\n",
    "    \n",
    "    conv_hash = numberToBase(iconv,base_c)\n",
    "    full_hash = numberToBase(ifull,base_f)\n",
    "    \n",
    "    return [conv_hash,full_hash]\n",
    "\n",
    "def decode(hash=([1, 7, 5, 0], [2, 9, 7])):\n",
    "    conv = base_to_dec(hash[0], base_c)\n",
    "    full = base_to_dec(hash[1], base_f)\n",
    "\n",
    "    \n",
    "    return [c_comb[conv],f_comb[full]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers=[[6, 16],[120,84]]):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_convs,hidden_fcs = hidden_layers\n",
    "#         print(hidden_convs)\n",
    "#         print(hidden_fcs)\n",
    "        uf_input = 0\n",
    "        layer_list = OrderedDict()\n",
    "        \n",
    "        layer_list['conv1'] = nn.Conv2d(3, hidden_convs[0], 5)\n",
    "        layer_list['pool1'] = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        layer_input = layer_list['conv1'].out_channels\n",
    "        \n",
    "        for layer_num, channels in enumerate(hidden_convs[1:], 2):\n",
    "            layer_list[\"conv%s\" % layer_num]  = nn.Conv2d(layer_input, channels, 5)\n",
    "            layer_list[\"pool%s\" % layer_num] = nn.MaxPool2d(2, 2)\n",
    "            layer_input = layer_list[\"conv%s\" % layer_num].out_channels\n",
    "        \n",
    "        \n",
    "        layer_list[\"flat\"] = nn.Flatten()\n",
    "        \n",
    "        layer_list['fc1'] = nn.Linear(layer_input*5*5, hidden_fcs[0])\n",
    "        layer_list[\"relu1\"]  = nn.ReLU()\n",
    "        \n",
    "        layer_input = layer_list['fc1'].out_features\n",
    "        for (layer_num, features) in enumerate(hidden_fcs[1:], 2):\n",
    "            layer_list[\"fc%s\" % layer_num]  = nn.Linear(layer_input, features)\n",
    "            layer_list[\"relu%s\" % layer_num]  = nn.ReLU()\n",
    "            layer_input = layer_list[\"fc%s\" % layer_num].out_features\n",
    "            \n",
    "        \n",
    "        layer_list['fco'] = nn.Linear(hidden_fcs[-1], 10)\n",
    "    \n",
    "        self.layers = nn.Sequential(layer_list)\n",
    "        \n",
    "#         print(\"New model: %s\" % hidden_layers)\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(neuron_config, checkpoint_dir=None):\n",
    "    print(neuron_config[\"conv_adjusted\"])\n",
    "    print(neuron_config[\"full_adjusted\"])\n",
    "    \n",
    "    data_dir=d.absolute()\n",
    "    \n",
    "    def cv_discrim(s): return 'conv_subindex_' in s\n",
    "    def fc_discrim(s): return 'full_subindex_' in s\n",
    "    cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "    fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "    \n",
    "#     cfg = decode([cvs, fcs])\n",
    "    cfg = decode([neuron_config[\"conv_adjusted\"], neuron_config[\"full_adjusted\"]])    \n",
    "    net = Net(cfg)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=neuron_config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(neuron_config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(neuron_config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"Model: %s, Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (cfg,epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=(correct / total))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "nodes_c = bases[0][\"nodes_req\"]\n",
    "nodes_f = bases[1][\"nodes_req\"]\n",
    "max_c = bases[0][\"max_necc_base_value\"]\n",
    "max_f = bases[1][\"max_necc_base_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def configure_neurons(num_convs,num_fcs):\n",
    "def configure_neurons():\n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict = {\"name\": \"neurons\"}\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(name=\"lr\", lower=1e-4, upper=1e-1, log=True))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"batch_size\", choices=[2**x for x in range(2,6)]))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"epochs\", choices=[10*x for x in range(2,5)]))\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_dict[\"lr\"] = tune.loguniform(lower=1e-4, upper=1e-1)\n",
    "    config_space_dict[\"batch_size\"] = tune.choice(categories=[2**x for x in range(2,6)])\n",
    "    config_space_dict[\"epochs\"] = tune.choice(categories=[10*x for x in range(2,5)])\n",
    "    \n",
    "    config_space_dict[\"conv_subindices\"],config_space_dict[\"full_subindices\"] = [],[]\n",
    "    config_space_dict[\"conv_cases\"],config_space_dict[\"full_cases\"] = [],[]\n",
    "    config_space_dict[\"conv_auxilliary\"],config_space_dict[\"full_auxilliary\"] = [],[]        \n",
    "    config_space_dict[\"conv_adjusted\"],config_space_dict[\"full_adjusted\"] = [],[]\n",
    "    \n",
    "    conv_lims,full_lims = [],[]\n",
    "    \n",
    "    for subindex in range(nodes_c):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"conv_subindex_%s\" % subindex\n",
    "        \n",
    "        conv_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_c-1, default_value=subindex%(base_c-1))\n",
    "#         conv_dict_rule = tune.randint(lower=0, upper=base_c-1)\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(conv_rule)\n",
    "#         config_space_dict[rule_name] = conv_dict_rule\n",
    "#         config_space_dict[\"conv_subindices\"].append(conv_dict_rule)\n",
    "    \n",
    "        conv_rules = list(filter(lambda hp: \"conv_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "        conv_dict_rules_keys = list(filter(lambda name: \"conv_subindex_\" in name, config_space_dict.keys()))\n",
    "#         conv_dict_rules = {k:config_space_dict[k] for k in conv_dict_rules_keys if k in config_space_dict}\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "#         rld = deepcopy(config_space_dict)\n",
    "        rd, rdd = {},{}\n",
    "        \n",
    "        for ri,rule in enumerate(conv_rules,1):\n",
    "    \n",
    "            if (len(conv_rules) == 1) & (max_c[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(conv_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_c[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(rule, range(max_c[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "#         for ri,rulekey in enumerate(conv_dict_rules.keys(),1):\n",
    "#             rule = conv_dict_rules[rulekey]\n",
    "            \n",
    "#             if (len(conv_dict_rules) == 1) & (max_c[ri-1] == config_space_dict[rule_name].upper):\n",
    "#                 break\n",
    "#             elif ri != len(conv_dict_rules):\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.randint(lower=0, upper=max_c[ri-1])\n",
    "#                 rld[\"%s_cond\" % rulekey] = tune.sample_from(lambda spec: spec.config_space_dict...uniform * 0.01)\n",
    "#             else:\n",
    "#                 rld[rulekey].upper = CS.ForbiddenInClause(rule, range(max_c[ri-1] + 1, rule.upper + 1))\n",
    "        \n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )           \n",
    "    \n",
    "    for subindex in range(nodes_f):\n",
    "        # define hyperparameter reference attributes\n",
    "        rule_name = \"full_subindex_%s\" % subindex\n",
    "        full_rule = CS.UniformIntegerHyperparameter(rule_name, lower=0, upper=base_f-1, default_value=subindex%(base_f-1))\n",
    "        \n",
    "        # add hyperparameter to collections\n",
    "        config_space.add_hyperparameter(full_rule)\n",
    "    \n",
    "        full_rules = list(filter(lambda hp: \"full_subindex_\" in hp.name, config_space.get_hyperparameters()))\n",
    "    \n",
    "        # build banlist from collections\n",
    "        rl = deepcopy(config_space)\n",
    "        rd = {}\n",
    "        for ri,rule in enumerate(full_rules,1):\n",
    "            if (len(full_rules) == 1) & (max_f[ri-1] == config_space.get_hyperparameter(rule_name).upper):\n",
    "                break\n",
    "            elif ri != len(full_rules):\n",
    "                rd[rule.name] = CS.ForbiddenEqualsClause(rule, max_f[ri-1])\n",
    "            else:\n",
    "                rd[rule.name] = CS.ForbiddenInClause(\n",
    "                        rule,\n",
    "                        range(max_f[ri-1] + 1, rule.upper + 1))\n",
    "        # package banlist for addition to config space\n",
    "        # add banlist to collection\n",
    "        if rd.values():\n",
    "            config_space.add_forbidden_clause(\n",
    "                CS.ForbiddenAndConjunction(\n",
    "                    *rd.values()\n",
    "                )\n",
    "            )           \n",
    "        \n",
    "    # add nodes to config space\n",
    "    [config_space_dict[\"conv_subindices\"].append(tune.randint(lower=0, upper=base_c-1)) for n in range(nodes_c)]\n",
    "    [config_space_dict[\"full_subindices\"].append(tune.randint(lower=0, upper=base_f-1)) for n in range(nodes_f)]\n",
    "    \n",
    "    for i,ceiling in enumerate(max_c):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"conv_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.conv_subindices[i] >= max_c[i] & config_space_dict[\"conv_cases\"][i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"conv_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.conv_subindices[i] >= max_c[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"conv_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"conv_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.conv_auxilliary[i] if spec.config.conv_cases[i] else spec.config.conv_subindices[i]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    for i,ceiling in enumerate(max_f):\n",
    "        # add cases wherein subindex must be switched to auxilliary\n",
    "        if i:\n",
    "            config_space_dict[\"full_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.full_subindices[i] >= max_f[i] & spec.config.full_cases[i-1])\n",
    "            )\n",
    "        else:\n",
    "            config_space_dict[\"full_cases\"].append(\n",
    "                tune.sample_from(lambda spec: spec.config.full_subindices[i] >= max_f[i])\n",
    "            )\n",
    "            \n",
    "        # add auxilliary values\n",
    "        config_space_dict[\"full_auxilliary\"].append(tune.randint(lower=0, upper=ceiling))\n",
    "        \n",
    "        # add subindex swticher\n",
    "        config_space_dict[\"full_adjusted\"].append(\n",
    "            tune.sample_from(lambda spec: \n",
    "                spec.config.full_auxilliary[i] if spec.config.full_cases[i] else spec.config.full_subindices[i]\n",
    "            )\n",
    "        )\n",
    "            \n",
    "#     [config_space_dict[\"conv_auxilliary\"].append(tune.randint(lower=0, upper=ceiling)) for i,ceiling in max_c]\n",
    "#     [config_space_dict[\"full_auxilliary\"].append(tune.randint(lower=0, upper=ceiling)) for i,ceiling in max_f]\n",
    "    \n",
    "#     for i,ceiling in enumerate(max_c):\n",
    "#         config_space_dict[\"conv_adjusted\"].append(\n",
    "#             tune.sample_from(lambda spec: \n",
    "#                 if spec.config.conv_cases[i] ? spec.config.conv_auxilliary[i] : spec.config.conv_subindices[i]\n",
    "#             )\n",
    "#         )\n",
    "#     for i,ceiling in enumerate(max_f):\n",
    "#         config_space_dict[\"full_adjusted\"].append(\n",
    "#             tune.sample_from(lambda spec: \n",
    "#                 if spec.config.full_cases[i] ? spec.config.full_auxilliary[i] : spec.config.full_subindices[i]\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "    return config_space,config_space_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  neurons\n",
      "lr :  <ray.tune.sample.Float object at 0x7f67b54b3fa0>\n",
      "batch_size :  <ray.tune.sample.Categorical object at 0x7f677a4602b0>\n",
      "epochs :  <ray.tune.sample.Categorical object at 0x7f677a460cd0>\n",
      "conv_subindices :  [<ray.tune.sample.Integer object at 0x7f677a460fd0>, <ray.tune.sample.Integer object at 0x7f6733a73850>, <ray.tune.sample.Integer object at 0x7f670d9f3070>, <ray.tune.sample.Integer object at 0x7f670d9f3880>]\n",
      "full_subindices :  [<ray.tune.sample.Integer object at 0x7f670d9f3cd0>, <ray.tune.sample.Integer object at 0x7f6757e2f310>, <ray.tune.sample.Integer object at 0x7f6757e2f280>, <ray.tune.sample.Integer object at 0x7f6757e2fee0>, <ray.tune.sample.Integer object at 0x7f6757e2faf0>]\n",
      "conv_cases :  [<ray.tune.sample.Function object at 0x7f677040f250>, <ray.tune.sample.Function object at 0x7f6757e2f1f0>, <ray.tune.sample.Function object at 0x7f6757e2f6d0>, <ray.tune.sample.Function object at 0x7f6757e2f8b0>]\n",
      "full_cases :  [<ray.tune.sample.Function object at 0x7f6757e2f190>, <ray.tune.sample.Function object at 0x7f6757e2fbe0>, <ray.tune.sample.Function object at 0x7f672a2136a0>, <ray.tune.sample.Function object at 0x7f672a213100>, <ray.tune.sample.Function object at 0x7f672a213400>]\n",
      "conv_auxilliary :  [<ray.tune.sample.Integer object at 0x7f6757e2f760>, <ray.tune.sample.Integer object at 0x7f6757e2f460>, <ray.tune.sample.Integer object at 0x7f6757e2fa30>, <ray.tune.sample.Integer object at 0x7f6757e2fbb0>]\n",
      "full_auxilliary :  [<ray.tune.sample.Integer object at 0x7f6757e2f1c0>, <ray.tune.sample.Integer object at 0x7f672a213e80>, <ray.tune.sample.Integer object at 0x7f672a2135e0>, <ray.tune.sample.Integer object at 0x7f672a213760>, <ray.tune.sample.Integer object at 0x7f672a213b50>]\n",
      "conv_adjusted :  [<ray.tune.sample.Function object at 0x7f6757e2fd60>, <ray.tune.sample.Function object at 0x7f6757e2f970>, <ray.tune.sample.Function object at 0x7f6757e2f2e0>, <ray.tune.sample.Function object at 0x7f6757e2f610>]\n",
      "full_adjusted :  [<ray.tune.sample.Function object at 0x7f6757e2ffa0>, <ray.tune.sample.Function object at 0x7f6757e2f370>, <ray.tune.sample.Function object at 0x7f672a213370>, <ray.tune.sample.Function object at 0x7f672a2133d0>, <ray.tune.sample.Function object at 0x7f672a213d00>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn = configure_neurons()\n",
    "cfl = [{v:cn[1][v]} for v in cn[1]]\n",
    "[print(v,\": \",cn[1][v]) for v in cn[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons(checkpoint_dir=None):    \n",
    "    neuron_config_space = configure_neurons()[1]\n",
    "    parameter_columns = [\"batch_size\", \"lr\", \"epochs\", \"conv_adjusted\", \"full_adjusted\"]\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    config = {\n",
    "    \"LiNO3_vol\": tune.uniform(0, 7),\n",
    "    \"Li2SO4_vol\": tune.uniform(0, 7),\n",
    "    \"NaClO4_vol\": tune.uniform(0, 7)\n",
    "    }\n",
    "    \n",
    "    #pre-load data to avoid races\n",
    "    load_data()\n",
    "    \n",
    "    param_dict = {\"domain\": cfl}\n",
    "    domain_config = load_config(param_dict)\n",
    "    print(domain_config)\n",
    "#     domain, domain_orderings = domain_config.domain, domain_config.domain_orderings\n",
    "\n",
    "    # define the hpo search algorithm BO\n",
    "#     func_caller = EuclideanFunctionCaller(None, domain_config.domain.list_of_domains[0])\n",
    "#     optimizer = EuclideanGPBandit(func_caller, ask_tell_mode=True)\n",
    "#     bo_search_alg = DragonflySearch(optimizer, **experiment_metrics)\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_time,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = DragonflySearch(\n",
    "        optimizer=\"bandit\",\n",
    "        domain=\"euclidean\",\n",
    "#         space=neuron_config_space,\n",
    "#         space=cfl,\n",
    "#         max_concurrent=8,\n",
    "        **experiment_metrics)\n",
    "    search = ConcurrencyLimiter(search, max_concurrent=8)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "#         parameter_columns=neuron_config_space.get_hyperparameter_names(),\n",
    "        parameter_columns=parameter_columns,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar),\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        config=param_dict,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    \n",
    "    def cv_discrim(s): return 'conv_subindex_' in s\n",
    "    def fc_discrim(s): return 'full_subindex_' in s\n",
    "    def other_discrim(s): return 'subindex' not in s\n",
    "    best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "    best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "    best_other = [best_trial.config[hp] for hp in list(filter(other_discrim, best_trial.config.keys()))]\n",
    "\n",
    "    cfg = decode([best_cvs, best_fcs])\n",
    "    \n",
    "    conv_report = [\"Connolutional Layer %s: %s\" % (i,c) for i,c in enumerate(cfg[0],1)]\n",
    "    full_report = [\"Fully-connected Layer %s: %s\" % (i,f) for i,f in enumerate(cfg[1],1)]\n",
    "    other_report = [\"%s: %s\" % (hp,f) for (hp,f) in zip([\"Batch Size\",\"Epochs\",\"Learning Rate\"],best_other)]\n",
    "\n",
    "#     print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial config:\")\n",
    "    [print(best) for best in [conv_report,full_report,other_report]]\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = Net(cfg)\n",
    "    best_training_hyperparameters = zip([\"Batch Size\",\"Epochs\",\"Learning Rate\"],best_other)\n",
    "    \n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    \n",
    "    if checkpoint_dir != None:\n",
    "        tune.report(accuracy=test_acc)\n",
    "    \n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return (best_trained_model.state_dict(), dict(best_training_hyperparameters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\n"
     ]
    }
   ],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-6a1041daa5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-148-1e1c0d495c6d>\u001b[0m in \u001b[0;36msearch_neurons\u001b[0;34m(checkpoint_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"domain\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcfl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdomain_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     domain, domain_orderings = domain_config.domain, domain_config.domain_orderings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/dragonfly/exd/cp_domain_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_params, config_file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;34m\"\"\" Loads configuration from parameters. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mconfig_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preprocess_config_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mdomain_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0mdomain_constraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'domain_constraints'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/dragonfly/exd/cp_domain_utils.py\u001b[0m in \u001b[0;36m_preprocess_config_params\u001b[0;34m(config_params)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no_name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Process the domain variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   config_params['domain'] = _preprocess_domain_parameters(config_params['domain'],\n\u001b[0m\u001b[1;32m    104\u001b[0m                               var_prefix='domvar_')\n\u001b[1;32m    105\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'domain_constraints'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/dragonfly/exd/cp_domain_utils.py\u001b[0m in \u001b[0;36m_preprocess_domain_parameters\u001b[0;34m(domain_parameters, var_prefix)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'kernel'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mvar_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'min'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'max'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'bounds'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "model,trainers = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dragonfly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-932195d8ad4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "a = [2**x for x in range(2,7)]\n",
    "a[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
