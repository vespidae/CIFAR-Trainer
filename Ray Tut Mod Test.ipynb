{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import permutations\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "## set number (or fraction) of GPUs (per training loop) you'd like to utilize if any at all\n",
    "cpu_use = 1\n",
    "gpu_use = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define feature space for hashing\n",
    "def feature_spacing():\n",
    "    conv = set(range(3**5)) - set(range(3**2))\n",
    "    full = set(range(2**5)) - set(range(2**2))\n",
    "    \n",
    "    c = 3**5 - 3**2\n",
    "    f = 2**5 - 2**2\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "    unique_combinations = []\n",
    "    total_uniques = 0\n",
    "    total_points = 1\n",
    "    \n",
    "    # do combo\n",
    "#     for combo in product(conv,conv,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "        \n",
    "#     for combo in product(conv,conv,full,full,full,full):\n",
    "#         unique_combinations.append(combo)\n",
    "\n",
    "    for ls in range(0,4):\n",
    "#         print(ls)\n",
    "        unique_combinations.append((c**2)*(f*(f+1)**ls))\n",
    "        total_uniques += (c**2)*f*((f+1)**ls)\n",
    "#         total_points = ((c**2)*f*((f+1)**ls))\n",
    "    \n",
    "    total_uniques -= ((c**2)*f)\n",
    "    total_points = total_uniques**2\n",
    "#     print(\"number of combos: %s\" % [\"%s-fc model: %s\" % (l,v) for l,v in enumerate(unique_combinations, 1)])\n",
    "#     print(\"total uniques:\",total_uniques)\n",
    "#     print(\"number of points/indices (with sparicities/noise): %s\" % total_points)\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    results = {\n",
    "        \"base\": [1],\n",
    "        \"nodes_req\": [total_uniques],\n",
    "        \"sparcity\": [total_points - total_uniques],\n",
    "        \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "        \"denoise_pcnt\":[0]\n",
    "    }\n",
    "    \n",
    "    report = pd.DataFrame(results)\n",
    "    \n",
    "#     print(report.to_string())\n",
    "    \n",
    "    for base in range(2,11):\n",
    "        results[\"base\"] = [base]\n",
    "        results[\"nodes_req\"] = [math.ceil(math.log(total_uniques,(base)))]\n",
    "# #         print(\"number of base %s complex nodes required:\" % (base), math.ceil(math.log(total_uniques,(base))))\n",
    "#         print(\"number of base %s complex nodes required:\" % (base), results[\"nodes_req\"])\n",
    "        results[\"sparcity\"] = [base**math.ceil(math.log(total_uniques,base)) - total_uniques]\n",
    "# #         print(\"sparcity:\",base**math.ceil(math.log(total_uniques,base)) - total_uniques,'points')\n",
    "#         print(\"sparcity:\",results[\"sparcity\"],'points')\n",
    "        results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(total_uniques,(base))) - base**math.log(total_uniques,(base)))/(base**math.ceil(math.log(total_uniques,(base))))*100]\n",
    "# #         print(\"sparcity percentage:\",(base**math.ceil(math.log(total_uniques,(base))) - base**math.log(total_uniques,(base)))/(base**math.ceil(math.log(total_uniques,(base))))*100,'%')\n",
    "#         print(\"sparcity percentage:\",results[\"sparcity percentage\"],'%')\n",
    "#         print(\"%s root-%s nodes per layer\" % (math.ceil(math.log(total_uniques,base+1)),base+1))\n",
    "#         print(\"\\n\")\n",
    "        results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(total_uniques,base)))**2)/total_points)*100)]\n",
    "# #         print(\"noise reduced from total points:\",math.floor(((total_points-(math.ceil(math.log(total_uniques,base)))**2)/total_points)*100),'%')\n",
    "#         print(\"noise reduced from total points:\",results[\"denoise_pcnt\"],'%')\n",
    "    \n",
    "        report = report.append(pd.DataFrame(results))\n",
    "#     for root in range(1,8):\n",
    "#         print(\"ceilinged %s-root (%s-value per number component) of combos with complex numbers: %s\\n\" % (root*2, root+1, [[math.ceil(combo**(1/(root*2))),\"sparsity: %s%s\" % ((math.ceil(combo**(1/(root*2))) - combo**(1/(root*2)))/combo**(1/(root*2))*100,'%')] for combo in unique_combinations]))\n",
    "    \n",
    "    print(report.sort_values([\"sparcity_pcnt\",\"nodes_req\",\"base\"]).to_string())\n",
    "#     report.head()\n",
    "    \n",
    "#     print(len(product(conv,conv,full)))\n",
    "#     print(f1)\n",
    "feature_spacing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base = 8\n",
    "c = 3**5 - 3**2\n",
    "f = 2**5 - 2**2\n",
    "def decode(code=None):\n",
    "    conv = []\n",
    "    full = []\n",
    "    \n",
    "    print(math.ceil(math.log(c,base)))\n",
    "    print(base**math.ceil(math.log(c,base)) - c)\n",
    "    print(math.ceil(math.log(f,base)))\n",
    "    print(base**math.ceil(math.log(f,base)) - f)\n",
    "    \n",
    "    model = [conv,full]\n",
    "#     return model\n",
    "    print()\n",
    "    \n",
    "decode()\n",
    "[print(math.log(278,b)) for b in range(2,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# math.sqrt(3**5)\n",
    "poss = ((2**5 - 2**2)+1)**3\n",
    "print(\"%s possibilities\" % poss)\n",
    "[print(\"%s root-%s nodes per layer\" % (math.ceil(math.log(poss,root)),root)) for root in range(2,5)]\n",
    "[print(\"%s root-%s nodes per layer\" % (math.log(poss,root),root)) for root in range(2,5)]\n",
    "# print(math.ceil(poss**(1/2)))\n",
    "# print(math.log(poss,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2**5 - 2**2)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = ['1', '2', '3']\n",
    "b = ['1', '2', '3']\n",
    "c = ['1', '2', '3']\n",
    "d = ['1', '2', '3']\n",
    "\n",
    "# for r in product(product(a, b, d),c): print(r)\n",
    "r = [comb for comb in product(a, b, d)]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers=[[6, 16],[120,84]]):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_convs,hidden_fcs = hidden_layers\n",
    "        print(hidden_convs)\n",
    "        print(hidden_fcs)\n",
    "        uf_input = 0\n",
    "        layer_list = OrderedDict()\n",
    "        \n",
    "        layer_list['conv1'] = nn.Conv2d(3, hidden_convs[0], 5)\n",
    "        layer_list['pool1'] = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        layer_input = layer_list['conv1'].out_channels\n",
    "        \n",
    "        for layer_num, channels in enumerate(hidden_convs[1:], 2):\n",
    "            layer_list[\"conv%s\" % layer_num]  = nn.Conv2d(layer_input, channels, 5)\n",
    "            layer_list[\"pool%s\" % layer_num] = nn.MaxPool2d(2, 2)\n",
    "            layer_input = layer_list[\"conv%s\" % layer_num].out_channels\n",
    "        \n",
    "        \n",
    "        layer_list[\"flat\"] = nn.Flatten()\n",
    "        \n",
    "        layer_list['fc1'] = nn.Linear(layer_input*5*5, hidden_fcs[0])\n",
    "        layer_list[\"relu1\"]  = nn.ReLU()\n",
    "        \n",
    "        layer_input = layer_list['fc1'].out_features\n",
    "        for (layer_num, features) in enumerate(hidden_fcs[1:], 2):\n",
    "            layer_list[\"fc%s\" % layer_num]  = nn.Linear(layer_input, features)\n",
    "            layer_list[\"relu%s\" % layer_num]  = nn.ReLU()\n",
    "            layer_input = layer_list[\"fc%s\" % layer_num].out_features\n",
    "            \n",
    "        \n",
    "        layer_list['fco'] = nn.Linear(hidden_fcs[-1], 10)\n",
    "    \n",
    "        self.layers = nn.Sequential(layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(neuron_config, checkpoint_dir=None):\n",
    "    \n",
    "    data_dir=d.absolute()\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "    fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "#     cvs = neuron_config[\"cvs\"]\n",
    "#     fcs = neuron_config[\"fcs\"]\n",
    "    \n",
    "    net = Net([cvs, fcs])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=neuron_config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(neuron_config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(neuron_config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=(correct / total))\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "def configure_neurons(num_convs,num_fcs):\n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    \n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(name=\"lr\", lower=1e-4, upper=1e-1, log=True))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"batch_size\", choices=[4, 8, 16, 32]))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"epochs\", choices=[20, 30, 40]))\n",
    "    \n",
    "    for hidden in range(2):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"cv%s\" % hidden, lower=3, upper=3**4))\n",
    "    \n",
    "    for hidden in range(num_fcs):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"fc%s\" % hidden, lower=2**2, upper=2**6))\n",
    "        \n",
    "    return config_space\n",
    "\n",
    "# def configure_neurons():\n",
    "#     config_space = {\n",
    "#         \"batch_size_seed\": tune.randint(2, 6),\n",
    "#         \"cv_seed\": tune.grid_search([2]),\n",
    "#         \"fc_seed\": tune.randint(2, 4),\n",
    "        \n",
    "#         \"lr\": tune.loguniform(1e-4,1e-1),\n",
    "#         \"batch_size\": tune.sample_from(lambda spec: 2**spec.config.batch_size_seed),\n",
    "#         \"epochs\": tune.qrandint(20, 40, 10),\n",
    "        \n",
    "#         \"cvs\": tune.sample_from(lambda spec: [tune.randint(3, 3**4) for layer in range(spec.config.cv_seed)]),\n",
    "#         \"fcs\": tune.sample_from(lambda spec: [tune.randint(2**2, 2**4) for layer in range(spec.config.fc_seed)])        \n",
    "#     }\n",
    "        \n",
    "#     return config_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron_config_space = configure_neurons()\n",
    "print(neuron_config_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons(layer_config, checkpoint_dir=None):\n",
    "    num_samples=20\n",
    "    max_num_epochs=20\n",
    "    gpus_per_trial=1\n",
    "    \n",
    "#     print(layer_config)\n",
    "    \n",
    "    neuron_config_space = configure_neurons(layer_config[\"num_convs\"], layer_config[\"num_fcs\"])\n",
    "#     neuron_config_space = configure_neurons()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    scheduler = HyperBandForBOHB(\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        max_t=10,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        neuron_config_space,\n",
    "        max_concurrent=4,\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        **experiment_metrics)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "#         parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\", \"epochs\"],\n",
    "        parameter_columns=neuron_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar),\n",
    "        verbose=2,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "#         config=neuron_config_space,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "    best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "# #     best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    \n",
    "#     best_trained_model = Net(best_trial.config[\"cvs\"], best_trial.config[\"fcs\"])\n",
    "    best_trained_model = Net([best_cvs, best_fcs])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    \n",
    "    if checkpoint_dir != None:\n",
    "        tune.report(accuracy=test_acc)\n",
    "    \n",
    "#     with tune.checkpoint_dir(\"nodes\") as checkpoint_dir:\n",
    "#         path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         torch.save(best_trained_model.state_dict(), path)\n",
    "    \n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform layer count trials\n",
    "def search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir=d.absolute()\n",
    "    load_data(data_dir)\n",
    "    layer_config_space = CS.ConfigurationSpace()\n",
    "\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.Constant(\"num_convs\", value=2))\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.UniformIntegerHyperparameter(\"num_fcs\", lower=2, upper=2**2))\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "\n",
    "    scheduler = HyperBandForBOHB(\n",
    "        max_t=max_num_epochs,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        layer_config_space,\n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "    reporter = CLIReporter(\n",
    "#         overwrite=True,\n",
    "        parameter_columns=layer_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(search_neurons),\n",
    "        verbose=2,\n",
    "        name=\"layers\",\n",
    "        local_dir=r.absolute(),\n",
    "#         config=layer_config_space,\n",
    "        resources_per_trial={\"gpu\": gpus_per_trial},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net([best_trial.config[\"num_convs\"], best_trial.config[\"num_fcs\"]])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"),map_location=torch.device('cpu'))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "model = Net()\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/31.3 GiB<br>Using HyperBand: num_stopped=13 total_brackets=4\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=6, completed=100.0%): {TERMINATED: 8} \n",
       "  Bracket(Max Size (n)=2, Milestone (r)=6, completed=100.0%): {TERMINATED: 6} \n",
       "  Bracket(Max Size (n)=2, Milestone (r)=5, completed=100.0%): {TERMINATED: 4} \n",
       "  Bracket(Max Size (n)=4, Milestone (r)=10, completed=100.0%): {TERMINATED: 2} <br>Resources requested: 0/8 CPUs, 0.0/2 GPUs, 0.0/17.24 GiB heap, 0.0/5.96 GiB objects (0/1.0 accelerator_type:RTX)<br>Result logdir: /home/grottesco/Source/CIFAR-Trainer/ray_results/neurons<br>Number of trials: 20/20 (20 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  cv0</th><th style=\"text-align: right;\">  cv1</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  fc0</th><th style=\"text-align: right;\">  fc1</th><th style=\"text-align: right;\">  fc2</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">      loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DEFAULT_89d5bed6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   53</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   63</td><td style=\"text-align: right;\">   41</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">0.00948259 </td><td style=\"text-align: right;\">  1.33842 </td><td style=\"text-align: right;\">  0.5566  </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_89d84a20</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   78</td><td style=\"text-align: right;\">   28</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    9</td><td style=\"text-align: right;\">   62</td><td style=\"text-align: right;\">   43</td><td style=\"text-align: right;\">0.00069557 </td><td style=\"text-align: right;\">  1.36644 </td><td style=\"text-align: right;\">  0.502175</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_89d97e9a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   65</td><td style=\"text-align: right;\">    7</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">   41</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">0.000173371</td><td style=\"text-align: right;\">  1.94433 </td><td style=\"text-align: right;\">  0.281075</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_89da71c4</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   46</td><td style=\"text-align: right;\">   74</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">   43</td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">0.000466858</td><td style=\"text-align: right;\">  2.11741 </td><td style=\"text-align: right;\">  0.227175</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_968620d0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">    7</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">    7</td><td style=\"text-align: right;\">   36</td><td style=\"text-align: right;\">   45</td><td style=\"text-align: right;\">0.00522748 </td><td style=\"text-align: right;\">  1.54912 </td><td style=\"text-align: right;\">  0.432675</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_96b7e070</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   77</td><td style=\"text-align: right;\">   55</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   47</td><td style=\"text-align: right;\">   30</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">0.000629648</td><td style=\"text-align: right;\">  0.800531</td><td style=\"text-align: right;\">  0.721275</td><td style=\"text-align: right;\">                   6</td></tr>\n",
       "<tr><td>DEFAULT_9c4bf936</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   54</td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   61</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">   59</td><td style=\"text-align: right;\">0.00213932 </td><td style=\"text-align: right;\">  1.60591 </td><td style=\"text-align: right;\">  0.40175 </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_a2b253b0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   47</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   34</td><td style=\"text-align: right;\">   15</td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">0.0540373  </td><td style=\"text-align: right;\">  2.30542 </td><td style=\"text-align: right;\">  0.099875</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_a9879574</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   46</td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   27</td><td style=\"text-align: right;\">    4</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">0.00414631 </td><td style=\"text-align: right;\">  2.30536 </td><td style=\"text-align: right;\">  0.100425</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_aeb0d47a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   48</td><td style=\"text-align: right;\">   75</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">   28</td><td style=\"text-align: right;\">   23</td><td style=\"text-align: right;\">0.00434361 </td><td style=\"text-align: right;\">  2.30045 </td><td style=\"text-align: right;\">  0.103075</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_b5ab0dc2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   19</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   12</td><td style=\"text-align: right;\">    5</td><td style=\"text-align: right;\">   29</td><td style=\"text-align: right;\">0.00598311 </td><td style=\"text-align: right;\">  1.84774 </td><td style=\"text-align: right;\">  0.362525</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_e30d25de</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    8</td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   54</td><td style=\"text-align: right;\">   63</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">0.000709276</td><td style=\"text-align: right;\">  1.15047 </td><td style=\"text-align: right;\">  0.58915 </td><td style=\"text-align: right;\">                   6</td></tr>\n",
       "<tr><td>DEFAULT_e55380ae</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">   29</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   20</td><td style=\"text-align: right;\">   29</td><td style=\"text-align: right;\">   25</td><td style=\"text-align: right;\">0.01359    </td><td style=\"text-align: right;\">  2.29324 </td><td style=\"text-align: right;\">  0.1208  </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_eae18cf0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">   27</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">   40</td><td style=\"text-align: right;\">0.000923698</td><td style=\"text-align: right;\">  1.19325 </td><td style=\"text-align: right;\">  0.573625</td><td style=\"text-align: right;\">                   6</td></tr>\n",
       "<tr><td>DEFAULT_fbafdaa0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">   79</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">   41</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">0.000114274</td><td style=\"text-align: right;\">  2.26167 </td><td style=\"text-align: right;\">  0.19215 </td><td style=\"text-align: right;\">                   5</td></tr>\n",
       "<tr><td>DEFAULT_275ec97c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   71</td><td style=\"text-align: right;\">   50</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   62</td><td style=\"text-align: right;\">   27</td><td style=\"text-align: right;\">   23</td><td style=\"text-align: right;\">0.0991235  </td><td style=\"text-align: right;\">nan       </td><td style=\"text-align: right;\">  0.099825</td><td style=\"text-align: right;\">                   5</td></tr>\n",
       "<tr><td>DEFAULT_279e9868</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   51</td><td style=\"text-align: right;\">   77</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">   61</td><td style=\"text-align: right;\">    7</td><td style=\"text-align: right;\">    6</td><td style=\"text-align: right;\">0.0128619  </td><td style=\"text-align: right;\">  2.30422 </td><td style=\"text-align: right;\">  0.10085 </td><td style=\"text-align: right;\">                   5</td></tr>\n",
       "<tr><td>DEFAULT_59231bfc</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   67</td><td style=\"text-align: right;\">   46</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   28</td><td style=\"text-align: right;\">   44</td><td style=\"text-align: right;\">   14</td><td style=\"text-align: right;\">0.0012783  </td><td style=\"text-align: right;\">  0.942796</td><td style=\"text-align: right;\">  0.670025</td><td style=\"text-align: right;\">                   5</td></tr>\n",
       "<tr><td>DEFAULT_719ec3e8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">   49</td><td style=\"text-align: right;\">   29</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">   28</td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">   34</td><td style=\"text-align: right;\">0.00013174 </td><td style=\"text-align: right;\">  1.95809 </td><td style=\"text-align: right;\">  0.26655 </td><td style=\"text-align: right;\">                  10</td></tr>\n",
       "<tr><td>DEFAULT_77b66da8</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">   31</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">   17</td><td style=\"text-align: right;\">    9</td><td style=\"text-align: right;\">0.0166097  </td><td style=\"text-align: right;\">  2.30773 </td><td style=\"text-align: right;\">  0.0996  </td><td style=\"text-align: right;\">                  10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-06 22:33:22,510\tINFO tune.py:439 -- Total run time: 609.36 seconds (609.31 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'batch_size': 4, 'cv0': 77, 'cv1': 55, 'epochs': 40, 'fc0': 47, 'fc1': 30, 'fc2': 44, 'lr': 0.0006296477601315882}\n",
      "Best trial final validation loss: 0.8005313697248697\n",
      "Best trial final validation accuracy: 0.721275\n",
      "[77, 55]\n",
      "[47, 30, 44]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.6493\n",
      "\n",
      "Processed in 10.221303872267406 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_config_space = {}\n",
    "\n",
    "# for hp in [\"num_convs\",\"num_fcs\"]:\n",
    "#     layer_config_space[hp] = np.random.randint(2,2**3)\n",
    "# layer_config_space[\"num_convs\"] = np.random.randint(2,3)\n",
    "layer_config_space[\"num_convs\"] = 2\n",
    "layer_config_space[\"num_fcs\"] = np.random.randint(3,2**2)\n",
    "\n",
    "cpu_use = 1\n",
    "gpu_use = 0.5\n",
    "# data_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/data/\")\n",
    "# checkpoint_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/checkpoints\")\n",
    "start = time.time()\n",
    "model = search_neurons(layer_config_space)\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\nProcessed in %s minutes\\n\" % ((end-start)/60,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
