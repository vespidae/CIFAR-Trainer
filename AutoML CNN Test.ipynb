{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "## set number (or fraction) of GPUs (per training loop) you'd like to utilize if any at all\n",
    "cpu_use = 1\n",
    "gpu_use = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neuron configuration we want is dependent upon the number of layers we have, we need to work flatten the feature space a bit. We can reduce the high-dminesional setups to a slightly less high-dminesional string of base-n nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base                              13\n",
      "nodes_req                          4\n",
      "sparcity                        1066\n",
      "max_necc_base_value    [12, 6, 9, 0]\n",
      "nodes+_req                         3\n",
      "subsparcity                    25298\n",
      "unexplained                    75894\n",
      "sparcity_pcnt                3.73236\n",
      "subsparcity_pcnt             1151.48\n",
      "denoise_pcnt                   -1500\n",
      "complexity                      4268\n",
      "Name: 13, dtype: object \n",
      "\n",
      "base                                    6\n",
      "nodes_req                               6\n",
      "sparcity                             6946\n",
      "max_necc_base_value    [5, 0, 3, 5, 0, 2]\n",
      "nodes+_req                              5\n",
      "subsparcity                         31934\n",
      "unexplained                        159670\n",
      "sparcity_pcnt                     14.8877\n",
      "subsparcity_pcnt                  410.674\n",
      "denoise_pcnt                        -3500\n",
      "complexity                          41682\n",
      "Name: 6, dtype: object \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define feature space for hashing\n",
    "\n",
    "c_min = 3**2\n",
    "c_max = 3**5\n",
    "f_min = 2**2\n",
    "f_max = 2**6\n",
    "\n",
    "c = c_max - c_min\n",
    "f = f_max - f_min\n",
    "\n",
    "# conv = set(range(c_max)) - set(range(c_min))\n",
    "# full = set(range(f_max)) - set(range(f_min))\n",
    "conv = range(c_max)[c_min:]\n",
    "full = range(f_max)[f_min:]\n",
    "\n",
    "c_comb = list(combinations_with_replacement(conv,2))\n",
    "f_comb = []\n",
    "for layers in range(1,4):\n",
    "    f_comb += list(combinations_with_replacement(full,layers))\n",
    "\n",
    "# for conversion from dec to whatever we end up using\n",
    "# most to least significant digit\n",
    "def numberToBase(n, b):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    digits = []\n",
    "    while n:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    rev = digits[::-1]\n",
    "    return rev\n",
    "\n",
    "def feature_spacing():\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "    unique_combinations = list(combinations([c_comb,f_comb],2))\n",
    "    total_uniques = len(unique_combinations)\n",
    "    total_points = total_uniques**2\n",
    "    total_cvs = len(c_comb)\n",
    "    total_fcs = len(f_comb)\n",
    "    \n",
    "#     print(total_cvs)\n",
    "#     print(total_fcs)\n",
    "    \n",
    "#     print(c_comb[np.random.randint(0,len(c_comb))])\n",
    "#     print(f_comb[np.random.randint(0,len(f_comb))])\n",
    "\n",
    "#     for ls in range(0,4):\n",
    "#         unique_combinations.append((c**2)*(f*(f+1)**ls))\n",
    "#         total_uniques += (c**2)*f*((f+1)**ls)\n",
    "#         total_fcs += f*((f+1)**ls)\n",
    "    \n",
    "#     total_uniques -= ((c**2)*f)\n",
    "#     total_points = total_uniques**2\n",
    "    \n",
    "#     print(\"number of combos: %s\" % [\"%s-fc model: %s\" % (l,v) for l,v in enumerate(unique_combinations, 1)])\n",
    "#     print(\"total uniques:\",total_uniques)\n",
    "#     print(\"number of points/indices (with sparicities/noise): %s\" % total_points)\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "#     results = {\n",
    "#         \"base\": [1],\n",
    "#         \"nodes_req\": [total_uniques],\n",
    "#         \"sparcity\": [total_points - total_uniques],\n",
    "#         \"max_necc_base_value\":[0],\n",
    "#         \"nodes+_req\": [0],\n",
    "#         \"subsparcity\": [0],\n",
    "#         \"unexplained\":[0],\n",
    "#         \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "#         \"subsparcity_pcnt\": [0],\n",
    "#         \"denoise_pcnt\":[0],\n",
    "#         \"complexity\":[0]\n",
    "#     }\n",
    "    \n",
    "    cf = []\n",
    "#     print(report.to_string())\n",
    "    for layer in [total_cvs,total_fcs]:#,total_uniques]:\n",
    "        results = {\n",
    "            \"base\": [1],\n",
    "            \"nodes_req\": [total_uniques],\n",
    "            \"sparcity\": [total_points - total_uniques],\n",
    "            \"max_necc_base_value\":[0],\n",
    "            \"nodes+_req\": [0],\n",
    "            \"subsparcity\": [0],\n",
    "            \"unexplained\":[0],\n",
    "            \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "            \"subsparcity_pcnt\": [0],\n",
    "            \"denoise_pcnt\":[0],\n",
    "            \"complexity\":[0]\n",
    "        }\n",
    "\n",
    "        report = pd.DataFrame(results)\n",
    "    \n",
    "        for base in range(2,17):\n",
    "            results[\"base\"] = [base]\n",
    "            results[\"nodes_req\"] = [math.ceil(math.log(layer,(base)))]\n",
    "            results[\"nodes+_req\"] = [math.floor(math.log(layer,(base)))]\n",
    "            \n",
    "            results[\"sparcity\"] = [base**math.ceil(math.log(layer,base)) - layer]\n",
    "            results[\"subsparcity\"] = [-(base**math.floor(math.log(layer,base)) - layer)]\n",
    "            \n",
    "            results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.ceil(math.log(layer,(base))))*100]\n",
    "            results[\"subsparcity_pcnt\"] = [-((base**math.floor(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.floor(math.log(layer,(base))))*100)]\n",
    "            \n",
    "#             results[\"max_necc_base_value\"] = [numberToBase((results[\"base\"][0]**results[\"nodes+_req\"][0]+results[\"subsparcity\"][0]),results[\"base\"][0])]\n",
    "            results[\"max_necc_base_value\"] = [numberToBase(layer,base)]\n",
    "            results[\"unexplained\"] = [(-(base**math.floor(math.log(layer,base)) - layer))*(math.floor(math.log(layer,(base))))]\n",
    "            \n",
    "            results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(layer,base)))**2)/total_points)*100)]\n",
    "        \n",
    "            results[\"complexity\"] = [results[\"nodes_req\"][0]*(results[\"sparcity\"][0]+1)]\n",
    "\n",
    "            report = report.append(pd.DataFrame(results))\n",
    "            \n",
    "            \n",
    "        report.index = [x for x in range(1, len(report.values)+1)]\n",
    "#         report.set_index(range(len(report)),inplace=True)\n",
    "        report.drop([1],axis=0,inplace=True)\n",
    "#         print(\"value: %s \\n\" % layer)\n",
    "        report.sort_values([\"sparcity\",\"unexplained\",\"nodes+_req\",\"subsparcity\",\"sparcity_pcnt\",\"base\"],inplace=True)\n",
    "#         print(report)\n",
    "#         print(report.to_string(),\"\\n\")\n",
    "        \n",
    "#         print(report.max())\n",
    "#         print(report.min())\n",
    "        \n",
    "#         report_norm = (report + -1 * report.mean()) / (report.max() + -1 * report.min())\n",
    "#         print(report_norm.to_string(),\"\\n\")\n",
    "        \n",
    "        cf.append(report.iloc[0])\n",
    "    \n",
    "    return cf\n",
    "\n",
    "[print(r,\"\\n\") for r in feature_spacing()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the convolutional layers, base 9 seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\n",
    "For the linear layers, base 16 seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\n",
    "\n",
    "We can use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = feature_spacing()\n",
    "\n",
    "base_c = bases[0][\"base\"]\n",
    "base_f = bases[1][\"base\"]\n",
    "\n",
    "def base_to_dec(num_list, base):\n",
    "    num_list = num_list[::-1]\n",
    "    num = 0\n",
    "    for k in range(len(num_list)):\n",
    "        dig = num_list[k]\n",
    "#         if dig.isdigit():\n",
    "#             dig = int(dig)\n",
    "        dig = int(dig)\n",
    "#         else:    #Assuming its either number or alphabet only\n",
    "#             dig = ord(dig.upper())-ord('A')+10\n",
    "        num += dig*(base**k)\n",
    "    return num\n",
    "\n",
    "def encode(config=[(24, 64),(13, 18, 41)]):\n",
    "    iconv = c_comb.index(config[0])\n",
    "    ifull = f_comb.index(config[1])\n",
    "    \n",
    "    conv_hash = numberToBase(iconv,base_c)\n",
    "    full_hash = numberToBase(ifull,base_f)\n",
    "    \n",
    "    return [conv_hash,full_hash]\n",
    "\n",
    "# print([(24, 64),(13, 18, 41)])\n",
    "# print(\"to\")\n",
    "# print(encode([(24, 64),(13, 18, 41)]))\n",
    "\n",
    "def decode(hash=([1, 7, 5, 0], [2, 0, 4, 3, 4, 4])):\n",
    "    conv = base_to_dec(hash[0], base_c)\n",
    "    full = base_to_dec(hash[1], base_f)\n",
    "\n",
    "    \n",
    "    return [c_comb[conv],f_comb[full]]\n",
    "\n",
    "\n",
    "# print([[1, 7, 5, 0], [2, 0, 4, 3, 4, 4]])\n",
    "# print(\"to\")\n",
    "# print(decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers=[[6, 16],[120,84]]):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_convs,hidden_fcs = hidden_layers\n",
    "        print(hidden_convs)\n",
    "        print(hidden_fcs)\n",
    "        uf_input = 0\n",
    "        layer_list = OrderedDict()\n",
    "        \n",
    "        layer_list['conv1'] = nn.Conv2d(3, hidden_convs[0], 5)\n",
    "        layer_list['pool1'] = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        layer_input = layer_list['conv1'].out_channels\n",
    "        \n",
    "        for layer_num, channels in enumerate(hidden_convs[1:], 2):\n",
    "            layer_list[\"conv%s\" % layer_num]  = nn.Conv2d(layer_input, channels, 5)\n",
    "            layer_list[\"pool%s\" % layer_num] = nn.MaxPool2d(2, 2)\n",
    "            layer_input = layer_list[\"conv%s\" % layer_num].out_channels\n",
    "        \n",
    "        \n",
    "        layer_list[\"flat\"] = nn.Flatten()\n",
    "        \n",
    "        layer_list['fc1'] = nn.Linear(layer_input*5*5, hidden_fcs[0])\n",
    "        layer_list[\"relu1\"]  = nn.ReLU()\n",
    "        \n",
    "        layer_input = layer_list['fc1'].out_features\n",
    "        for (layer_num, features) in enumerate(hidden_fcs[1:], 2):\n",
    "            layer_list[\"fc%s\" % layer_num]  = nn.Linear(layer_input, features)\n",
    "            layer_list[\"relu%s\" % layer_num]  = nn.ReLU()\n",
    "            layer_input = layer_list[\"fc%s\" % layer_num].out_features\n",
    "            \n",
    "        \n",
    "        layer_list['fco'] = nn.Linear(hidden_fcs[-1], 10)\n",
    "    \n",
    "        self.layers = nn.Sequential(layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(neuron_config, checkpoint_dir=None):\n",
    "    \n",
    "    data_dir=d.absolute()\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "    fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "#     cvs = neuron_config[\"cvs\"]\n",
    "#     fcs = neuron_config[\"fcs\"]\n",
    "    \n",
    "    net = Net([cvs, fcs])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=neuron_config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(neuron_config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(neuron_config[\"epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=(correct / total))\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "nodes_c = bases[0][\"nodes_req\"]\n",
    "nodes_f = bases[1][\"nodes_req\"]\n",
    "max_c = bases[0][\"max_necc_base_value\"]\n",
    "max_f = bases[1][\"max_necc_base_value\"]\n",
    "\n",
    "def configure_neurons(num_convs,num_fcs):\n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    \n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(name=\"lr\", lower=1e-4, upper=1e-1, log=True))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"batch_size\", choices=[4, 8, 16, 32]))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(name=\"epochs\", choices=[20, 30, 40]))\n",
    "    \n",
    "    conv_rules,full_rules = []\n",
    "    \n",
    "    for subindex in range(nodes_c):\n",
    "        conv_rules += CS.UniformIntegerHyperparameter(\"conv_subindex_%s\" % subindex, lower=0, upper=base_c-1)\n",
    "#         config_space.add_hyperparameter(\n",
    "#             CS.UniformIntegerHyperparameter(\"conv_subindex_%s\" % subindex, lower=0, upper=base_c-1))\n",
    "        config_space.add_hyperparameter(conv_rules[subindex])\n",
    "    \n",
    "    for subindex in range(nodes_f):\n",
    "        full_rules += CS.UniformIntegerHyperparameter(\"full_subindex_%s\" % subindex, lower=0, upper=base_f-1)\n",
    "#         config_space.add_hyperparameter(\n",
    "#             CS.UniformIntegerHyperparameter(\"full_subindex_%s\" % subindex, lower=0, upper=base_f-1))\n",
    "        config_space.add_hyperparameter(full_rules[subindex])\n",
    "        \n",
    "    #Define max subindices\n",
    "    \n",
    "    max_conv_index = [[CS.ForbiddenEqualsClause(conv_rules[0],max_c[0] + 1)]]\n",
    "    max_full_index = [[CS.ForbiddenEqualsClause(full_rules[0],max_f[0] + 1)]]\n",
    "    \n",
    "    for subindex,max_necc in enumerate(1,max_c):\n",
    "        banlist = [CS.ForbiddenEqualsClause(max_conv_index[0][0].hyperparameter,max_conv_index[0][0].value - 1)]\n",
    "        for fbd in max_conv_index:\n",
    "#             banlist += CS.ForbiddenEqualsClause(fbd.hyperparameter,fbd.value - 1)\n",
    "            banlist += CS.ForbiddenEqualsClause(conv_rules[subindex],max_necc + 1)\n",
    "        max_conv_index += CS.ForbiddenEqualsClause(conv_rules[subindex],max_necc + 1)\n",
    "    \n",
    "    for hidden in range(2):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"cv%s\" % hidden, lower=3, upper=3**4))\n",
    "    \n",
    "    for hidden in range(num_fcs):\n",
    "        config_space.add_hyperparameter(\n",
    "            CS.UniformIntegerHyperparameter(\"fc%s\" % hidden, lower=2**2, upper=2**4))\n",
    "        \n",
    "    return config_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron_config_space = configure_neurons()\n",
    "print(neuron_config_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons(layer_config, checkpoint_dir=None):\n",
    "    num_samples=10\n",
    "    max_num_epochs=10\n",
    "    gpus_per_trial=1\n",
    "    \n",
    "#     print(layer_config)\n",
    "    \n",
    "    neuron_config_space = configure_neurons(layer_config[\"num_convs\"], layer_config[\"num_fcs\"])\n",
    "#     neuron_config_space = configure_neurons()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    scheduler = HyperBandForBOHB(\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        max_t=10,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        neuron_config_space,\n",
    "        max_concurrent=4,\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "        **experiment_metrics)\n",
    "    reporter = CLIReporter(\n",
    "#         overwrite=True,\n",
    "#         parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\", \"epochs\"],\n",
    "        parameter_columns=neuron_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar),\n",
    "        verbose=2,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "#         config=neuron_config_space,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    \n",
    "    def cv_discrim(s): return 'cv' in s\n",
    "    def fc_discrim(s): return 'fc' in s\n",
    "    best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "    best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "# #     best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    \n",
    "#     best_trained_model = Net(best_trial.config[\"cvs\"], best_trial.config[\"fcs\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    \n",
    "    if checkpoint_dir != None:\n",
    "        tune.report(accuracy=test_acc)\n",
    "    \n",
    "#     with tune.checkpoint_dir(\"nodes\") as checkpoint_dir:\n",
    "#         path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         torch.save(best_trained_model.state_dict(), path)\n",
    "    \n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perform layer count trials\n",
    "def search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir=d.absolute()\n",
    "    load_data(data_dir)\n",
    "    layer_config_space = CS.ConfigurationSpace()\n",
    "\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.Constant(\"num_convs\", value=2))\n",
    "    layer_config_space.add_hyperparameter(\n",
    "        CS.UniformIntegerHyperparameter(\"num_fcs\", lower=2, upper=2**2))\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "\n",
    "    scheduler = HyperBandForBOHB(\n",
    "        max_t=max_num_epochs,\n",
    "        reduction_factor=2,\n",
    "        **experiment_metrics)\n",
    "    search = TuneBOHB(\n",
    "        layer_config_space,\n",
    "        max_concurrent=4,\n",
    "        **experiment_metrics)\n",
    "    reporter = CLIReporter(\n",
    "#         overwrite=True,\n",
    "        parameter_columns=layer_config_space.get_hyperparameter_names(),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(search_neurons),\n",
    "        verbose=2,\n",
    "        name=\"layers\",\n",
    "        local_dir=r.absolute(),\n",
    "#         config=layer_config_space,\n",
    "        resources_per_trial={\"gpu\": gpus_per_trial},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net([best_trial.config[\"num_convs\"], best_trial.config[\"num_fcs\"]])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"),map_location=torch.device('cpu'))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "model = Net()\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_config_space = {}\n",
    "\n",
    "# for hp in [\"num_convs\",\"num_fcs\"]:\n",
    "#     layer_config_space[hp] = np.random.randint(2,2**3)\n",
    "# layer_config_space[\"num_convs\"] = np.random.randint(2,3)\n",
    "layer_config_space[\"num_convs\"] = 2\n",
    "layer_config_space[\"num_fcs\"] = np.random.randint(2,2**2)\n",
    "\n",
    "# cpu_use = 1\n",
    "# gpu_use = 0\n",
    "# data_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/data/\")\n",
    "# checkpoint_dir = os.path.abspath(\"/home/grottesco/Source/RayTuneTut/checkpoints\")\n",
    "print(\"Resource usage can be viewed at 127.0.0.1:8265\")\n",
    "    \n",
    "model = search_neurons(layer_config_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf ./data/* ./ray_results/layers/* ./ray_results/neurons/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
