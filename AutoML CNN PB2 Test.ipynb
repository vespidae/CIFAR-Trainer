{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "# from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.integration.torch import is_distributed_trainable\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "# from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "# from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "# from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "# from dragonfly.opt.gp_bandit import CPGPBandit\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# from dragonfly import load_config\n",
    "# from dragonfly.exd.experiment_caller import CPFunctionCaller, EuclideanFunctionCaller\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "from ray.tune.schedulers import pb2\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "from optuna.multi_objective.samplers import MOTPEMultiObjectiveSampler\n",
    "from optuna.integration import BoTorchSampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "\n",
    "# from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, select_length, UCB, standardize, TV_SquaredExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count() # number of cpu cores\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus\n",
    "cpu_use = 1 # number of cpu cores to dedicate to 1 series of trials\n",
    "gpu_use = gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "oom = 3#8 if gpus else 2 # order of magnitude\n",
    "num_samples = 2 ** oom\n",
    "# max_time = 10# * oom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neuron configuration we want is dependent upon the number of layers we have, we need to work flatten the feature space a bit. We can reduce the high-dminesional setups to a slightly less high-dminesional string of base-n nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define feature space for hashing\n",
    "c_min = 3**2\n",
    "c_max = 3**5\n",
    "f_min = 2**2\n",
    "f_max = 2**6\n",
    "\n",
    "c = c_max - c_min\n",
    "f = f_max - f_min\n",
    "\n",
    "# conv = set(range(c_max)) - set(range(c_min))\n",
    "# full = set(range(f_max)) - set(range(f_min))\n",
    "conv = range(c_max)[c_min:]\n",
    "full = range(f_max)[f_min:]\n",
    "\n",
    "c_comb = list(combinations_with_replacement(conv,2))\n",
    "f_comb = []\n",
    "for layers in range(1,5):\n",
    "    f_comb += list(combinations_with_replacement(full,layers))\n",
    "#     print(\"Fully connected layer %s range: %s\" % (layers,len(f_comb)) )\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# for conversion from dec to whatever we end up using\n",
    "# most to least significant digit\n",
    "def numberToBase(n, b):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    digits = []\n",
    "    while n:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    rev = digits[::-1]\n",
    "    return rev\n",
    "\n",
    "def feature_spacing():\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "    unique_combinations = list(combinations([c_comb,f_comb],2))\n",
    "    total_uniques = len(unique_combinations)\n",
    "    total_points = total_uniques**2\n",
    "    total_cvs = len(c_comb)\n",
    "    total_fcs = len(f_comb)\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    \n",
    "    cf = []\n",
    "    \n",
    "    for layer in [total_cvs,total_fcs]:#,total_uniques]:\n",
    "        results = {\n",
    "            \"base\": [1],\n",
    "            \"nodes_req\": [total_uniques],\n",
    "            \"sparcity\": [total_points - total_uniques],\n",
    "            \"max_necc_base_value\":[0],\n",
    "            \"nodes+_req\": [0],\n",
    "            \"subsparcity\": [0],\n",
    "            \"unexplained\":[0],\n",
    "            \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "            \"subsparcity_pcnt\": [0],\n",
    "            \"denoise_pcnt\":[0],\n",
    "            \"complexity\":[0]\n",
    "        }\n",
    "\n",
    "        report = pd.DataFrame(results)\n",
    "    \n",
    "        for base in range(2,17):\n",
    "            results[\"base\"] = [base]\n",
    "            results[\"nodes_req\"] = [math.ceil(math.log(layer,(base)))]\n",
    "            results[\"nodes+_req\"] = [math.floor(math.log(layer,(base)))]\n",
    "            \n",
    "            results[\"sparcity\"] = [base**math.ceil(math.log(layer,base)) - layer]\n",
    "            results[\"subsparcity\"] = [-(base**math.floor(math.log(layer,base)) - layer)]\n",
    "            \n",
    "            results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.ceil(math.log(layer,(base))))*100]\n",
    "            results[\"subsparcity_pcnt\"] = [-((base**math.floor(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.floor(math.log(layer,(base))))*100)]\n",
    "            \n",
    "#             results[\"max_necc_base_value\"] = [numberToBase((results[\"base\"][0]**results[\"nodes+_req\"][0]+results[\"subsparcity\"][0]),results[\"base\"][0])]\n",
    "            results[\"max_necc_base_value\"] = [numberToBase(layer,base)]\n",
    "            results[\"unexplained\"] = [(-(base**math.floor(math.log(layer,base)) - layer))*(math.floor(math.log(layer,(base))))]\n",
    "            \n",
    "            results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(layer,base)))**2)/total_points)*100)]\n",
    "        \n",
    "            results[\"complexity\"] = [results[\"nodes_req\"][0]*(results[\"sparcity\"][0]+1)]\n",
    "\n",
    "            report = report.append(pd.DataFrame(results))\n",
    "            \n",
    "            \n",
    "        report.index = [x for x in range(1, len(report.values)+1)]\n",
    "        report.drop([1],axis=0,inplace=True)\n",
    "        report.sort_values([\"sparcity\",\"unexplained\",\"nodes+_req\",\"subsparcity\",\"sparcity_pcnt\",\"base\"],inplace=True)\n",
    "        \n",
    "        cf.append(report.iloc[0])\n",
    "    \n",
    "    return cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bases = feature_spacing()\n",
    "[print(r,\"\\n\") for r in bases]\n",
    "\n",
    "base_c = bases[0][\"base\"]\n",
    "base_f = bases[1][\"base\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"For the convolutional layers, base %s seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\" % bases[0][\"base\"])\n",
    "print(\"For the linear layers, base %s seems to allow us to use the fewest nodes with the lowest number of invalid configuration indices (sparcity).\" % bases[1][\"base\"])\n",
    "\n",
    "# print(\"We can use the \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def base_to_dec(num_list, base):\n",
    "    num_list = num_list[::-1]\n",
    "    num = 0\n",
    "    for k in range(len(num_list)):\n",
    "        dig = num_list[k]\n",
    "        dig = int(dig)\n",
    "        num += dig*(base**k)\n",
    "    return num\n",
    "\n",
    "def encode(config=[(24, 64),(13, 41)]):\n",
    "    iconv = c_comb.index(config[0])\n",
    "    ifull = f_comb.index(config[1])\n",
    "    \n",
    "    conv_hash = numberToBase(iconv,base_c)\n",
    "    full_hash = numberToBase(ifull,base_f)\n",
    "    \n",
    "    return [conv_hash,full_hash]\n",
    "\n",
    "def decode(hash=([1, 7, 5, 0], [2, 9, 7])):\n",
    "    conv = base_to_dec(hash[0], base_c)\n",
    "    full = base_to_dec(hash[1], base_f)\n",
    "\n",
    "    \n",
    "    return [c_comb[conv],f_comb[full]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    n_convs = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    n_fulls = trial.suggest_int(\"n_full_layers\", 1, 4)\n",
    "#         print(n_convs,n_fulls)\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(n_convs):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "            kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "#             print(\"post conv: \",pre_flat_size)\n",
    "            if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "#                 print(\"post pool: \",pre_flat_size)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "#     self.convolution = nn.Sequential(*layers)\n",
    "\n",
    "#     layers = []\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "#     self.flattening = nn.Sequential(*layers)\n",
    "\n",
    "#     layers = []\n",
    "#         print(\"pre_flat_size:\",pre_flat_size)\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(n_fulls):\n",
    "        out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "            p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "#     self.linearizing = nn.Sequential(*layers)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dynamically-generated nn that takes a 3-channel image and outputs a label\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, arch):\n",
    "#         super(Net, self).__init__()\n",
    "#         # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "# #         n_convs = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "# #         n_fulls = trial.suggest_int(\"n_full_layers\", 1, 4)\n",
    "# #         print(n_convs,n_fulls)\n",
    "#         layers = []\n",
    "#         pre_flat_size = 32\n",
    "#         in_channels = 3\n",
    "#         out_kernel = None\n",
    "\n",
    "#         for i in range(arch[\"n_conv_layers\"]):\n",
    "#             if pre_flat_size > 7:\n",
    "# #                 out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "#                 out_channels = arch[\"n_conv_channels_c%s\" % i]\n",
    "# #                 kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "#                 kernel_size = arch[\"kernel_size_c%s\" % i]\n",
    "#                 layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "#                 pre_flat_size = pre_flat_size - kernel_size+1\n",
    "#     #             print(\"post conv: \",pre_flat_size)\n",
    "# #                 if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "#                 if arch[\"has_max_pool_c%s\" % i] & pre_flat_size > 3:\n",
    "#                     layers.append(nn.MaxPool2d(2, 2))\n",
    "#                     pre_flat_size = int(pre_flat_size / 2)\n",
    "#     #                 print(\"post pool: \",pre_flat_size)\n",
    "#                 layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "#             in_channels = out_channels\n",
    "#             out_kernel = kernel_size\n",
    "            \n",
    "# #         self.convolution = nn.Sequential(*layers)\n",
    "        \n",
    "# #         layers = []\n",
    "        \n",
    "#         layers.append(nn.Flatten())\n",
    "        \n",
    "# #         self.flattening = nn.Sequential(*layers)\n",
    "        \n",
    "# #         layers = []\n",
    "# #         print(\"pre_flat_size:\",pre_flat_size)\n",
    "#         in_features = in_channels * pre_flat_size**2\n",
    "#         for i in range(arch[\"n_full_layers\"]):\n",
    "# #             out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "#             out_features = arch[\"n_l_units_l%s\" % i]\n",
    "#             layers.append(nn.Linear(in_features, out_features))\n",
    "#             layers.append(nn.ReLU())\n",
    "# #             if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "#             if arch[\"has_dropout_l%s\" % i]:\n",
    "#                 p = arch[\"dropout_l%s\" % i]\n",
    "#                 layers.append(nn.Dropout(p))\n",
    "#             layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "#             in_features = out_features\n",
    "        \n",
    "#         layers.append(nn.Linear(in_features, 10))\n",
    "#         layers.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "# #         self.linearizing = nn.Sequential(*layers)\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "# #         [print(layer) for layer in [self.convolution,self.flattening,self.linearizing]]\n",
    "        \n",
    "# #         print(\"New model: %s\" % hidden_layers)\n",
    "#     def forward(self, x):\n",
    "#         x = self.layers(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(arch):\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(arch[\"n_conv_layers\"]):\n",
    "        if pre_flat_size > 7:\n",
    "#                 out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "            out_channels = arch[\"n_conv_channels_c%s\" % i]\n",
    "#                 kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "            kernel_size = arch[\"kernel_size_c%s\" % i]\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "#             print(\"post conv: \",pre_flat_size)\n",
    "#                 if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "            if arch[\"has_max_pool_c%s\" % i] & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "#                 print(\"post pool: \",pre_flat_size)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "#         self.convolution = nn.Sequential(*layers)\n",
    "\n",
    "#         layers = []\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "#         self.flattening = nn.Sequential(*layers)\n",
    "\n",
    "#         layers = []\n",
    "#         print(\"pre_flat_size:\",pre_flat_size)\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(arch[\"n_full_layers\"]):\n",
    "#             out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "        out_features = arch[\"n_l_units_l%s\" % i]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "#             if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "        if arch[\"has_dropout_l%s\" % i]:\n",
    "            p = arch[\"dropout_l%s\" % i]\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "#         self.linearizing = nn.Sequential(*layers)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def define_device():\n",
    "    CANDIDATES = 2 # number of GPUs being used\n",
    "    cuda_tensor = torch.Tensor([[2,0],[0,2]])\n",
    "    print(str(torch.Tensor().cuda().device))\n",
    "    cuda_tensor = cuda_tensor.cuda()\n",
    "    print(cuda_tensor.get_device())\n",
    "    \n",
    "    it = [torch.Tensor([[1,0],[0,1]]) for i in range(CANDIDATES)]\n",
    "    index_tensors = [t.to(\"cuda:%s\" % i) for i,t in enumerate(it)]\n",
    "    \n",
    "    cpu_tensor = torch.Tensor([[1,0],[0,1]])\n",
    "    cpu_tensor = cpu_tensor.to(\"cpu\")\n",
    "    print(cpu_tensor.device)\n",
    "    \n",
    "    index_tensors.append(cpu_tensor)\n",
    "    \n",
    "    [print(t.device) for t in index_tensors]\n",
    "    [print(cuda_tensor*t) for t in index_tensors]\n",
    "define_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(non_arch_config,trial):\n",
    "#     non_arch_config.set_user_attr(\"train_params\",non_arch_config)\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "#     [print(x) for x in [lr,batch_size,epochs]]\n",
    "#     data_dir=d.absolute()\n",
    "    \n",
    "#     def cv_discrim(s): return 'conv_subindex_' in s\n",
    "#     def fc_discrim(s): return 'full_subindex_' in s\n",
    "#     cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "#     fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "    \n",
    "#     cfg = decode([cvs, fcs])\n",
    "    \n",
    "    \n",
    "#     net = Net(cfg)\n",
    "    net = define_model(trial) if type(trial) == optuna.trial.Trial else Net(trial.params)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "#     gpus = torch.cuda.device_count()\n",
    "#     if gpus:\n",
    "#         device = str(torch.Tensor().cuda().device)\n",
    "#         net = net.to(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "#     if gpus > 1:\n",
    "# #         torch.cuda.set_device(gpus-1)\n",
    "#         net = nn.DataParallel(net)\n",
    "#         nn.DataParallel(net, device_ids=range(gpus))\n",
    "# #         torch.distributed.init_process_group(\n",
    "# #             backend='nccl', world_size=gpus, rank=0#, init_method='...'\n",
    "# #         )\n",
    "# #         net = DistributedDataParallel(net, device_ids=[gpus-1], output_device=gpus-1)\n",
    "# #         device = net.output_device\n",
    "#     else:\n",
    "#         net = net.to(device)\n",
    "        \n",
    "#     torch.distributed.init_process_group(\n",
    "#         backend='nccl', world_size=N, init_method='...'\n",
    "#     )\n",
    "#     model = DistributedDataParallel(model, device_ids=[i], output_device=i)\n",
    "    \n",
    "#     if is_distributed_trainable():\n",
    "#         net = DistributedDataParallel(net)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             print(\"Input shape: \",inputs.shape)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "#             [print(y) for y in [outputs, labels]]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             # print statistics\n",
    "#             running_loss += loss.item()\n",
    "#             epoch_steps += 1\n",
    "#             if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "# #             if i % 4000 == 3999:  # print every 4000 mini-batches\n",
    "# #                 print(\"Model: %s, Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (cfg,epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "# #                 if str(loss) == 'nan': print(\"outputs: %s, labels: %s, loss: %s\" % (outputs, labels, loss))\n",
    "# #                 [print(\"%s: %s\" % n,v) for n,v in zip([\"outputs\", \"labels\", \"loss\"],[outputs, labels, loss])]\n",
    "#                 print(\"Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "#                 running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", trial.number, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "        \n",
    "        \n",
    "# #         trial.report([loss,accuracy], epoch)\n",
    "#         trial.report(loss,accuracy,epoch)\n",
    "\n",
    "# #         tune.report(accuracy=accuracy,loss=loss,model=net)\n",
    "#     arch_state_dict = trial.params# if type(trial) == optuna.multi_objective.trial.MultiObjectiveTrial else trial.params\n",
    "#     trial.set_user_attr(\"arch\", arch_state_dict)\n",
    "#     trial.set_user_attr(\"MODEL\", dict(net.state_dict()))\n",
    "# #     trial.set_user_attr(\"optimizer\", optimizer.state_dict())\n",
    "    \n",
    "#     trial.report([loss,accuracy],trial.number)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                net.state_dict()\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "    return [loss,accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model nn based on HPO\n",
    "def model_cifar(non_arch_config,arch_config):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "#     [print(x) for x in [lr,batch_size,epochs]]\n",
    "#     data_dir=d.absolute()\n",
    "    \n",
    "#     def cv_discrim(s): return 'conv_subindex_' in s\n",
    "#     def fc_discrim(s): return 'full_subindex_' in s\n",
    "#     cvs = [neuron_config[hp] for hp in list(filter(cv_discrim, neuron_config.keys()))]\n",
    "#     fcs = [neuron_config[hp] for hp in list(filter(fc_discrim, neuron_config.keys()))]\n",
    "    \n",
    "#     cfg = decode([cvs, fcs])\n",
    "    \n",
    "    \n",
    "#     net = Net(cfg)\n",
    "    print(arch_config)\n",
    "    net = Net(arch_config)# if type(trial) == optuna.multi_objective.trial.MultiObjectiveTrial else Net(trial.params)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "#     gpus = torch.cuda.device_count()\n",
    "#     if gpus:\n",
    "#         device = str(torch.Tensor().cuda().device)\n",
    "#         net = net.to(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "    if gpus > 1:\n",
    "# #         torch.cuda.set_device(gpus-1)\n",
    "        net = nn.DataParallel(net)\n",
    "#         nn.DataParallel(net, device_ids=range(gpus))\n",
    "# #         torch.distributed.init_process_group(\n",
    "# #             backend='nccl', world_size=gpus, rank=0#, init_method='...'\n",
    "# #         )\n",
    "# #         net = DistributedDataParallel(net, device_ids=[gpus-1], output_device=gpus-1)\n",
    "# #         device = net.output_device\n",
    "#     else:\n",
    "#         net = net.to(device)\n",
    "        \n",
    "#     torch.distributed.init_process_group(\n",
    "#         backend='nccl', world_size=N, init_method='...'\n",
    "#     )\n",
    "#     model = DistributedDataParallel(model, device_ids=[i], output_device=i)\n",
    "    \n",
    "#     if is_distributed_trainable():\n",
    "#         net = DistributedDataParallel(net)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             print(\"Input shape: \",inputs.shape)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "#             [print(y) for y in [outputs, labels]]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             # print statistics\n",
    "#             running_loss += loss.item()\n",
    "#             epoch_steps += 1\n",
    "#             if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "# #             if i % 4000 == 3999:  # print every 4000 mini-batches\n",
    "# #                 print(\"Model: %s, Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (cfg,epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "# #                 if str(loss) == 'nan': print(\"outputs: %s, labels: %s, loss: %s\" % (outputs, labels, loss))\n",
    "# #                 [print(\"%s: %s\" % n,v) for n,v in zip([\"outputs\", \"labels\", \"loss\"],[outputs, labels, loss])]\n",
    "#                 print(\"Epoch: %d, Mini-batch: %5d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "#                 running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", Test, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "        \n",
    "        \n",
    "#         trial.report([loss,accuracy], epoch)\n",
    "\n",
    "# #         tune.report(accuracy=accuracy,loss=loss,model=net)\n",
    "#     arch_state_dict = trial.params# if type(trial) == optuna.multi_objective.trial.MultiObjectiveTrial else trial.params\n",
    "#     trial.set_user_attr(\"arch\", arch_state_dict)\n",
    "# #     trial.set_user_attr(\"model\", net.state_dict())\n",
    "# #     trial.set_user_attr(\"optimizer\", optimizer.state_dict())\n",
    "    \n",
    "# #     trial.report([loss,accuracy],trial.number)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    _, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#determine configuration boundary for nn based on number of layers\n",
    "nodes_c = bases[0][\"nodes_req\"]\n",
    "nodes_f = bases[1][\"nodes_req\"]\n",
    "max_c = bases[0][\"max_necc_base_value\"]\n",
    "max_f = bases[1][\"max_necc_base_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_training_hyperparameters():\n",
    "    lr = {\n",
    "        \"name\":\"learning rate {10^(-⌊x⌋)\",\n",
    "#         \"bounds\":[1e-4,1e-1]\n",
    "        \"bounds\":[x for x in range(1,4)]\n",
    "    }\n",
    "    batch_size = {\n",
    "        \"name\":\"batch size {2^⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(6,9)]\n",
    "#         \"bounds\":[x for x in range(4,6)]\n",
    "    }\n",
    "    epochs = {\n",
    "        \"name\":\"epochs {10⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(2,6)]\n",
    "#         \"bounds\":[1,2]\n",
    "    }\n",
    "    \n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict,config_space_ray = {},{}\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            lr[\"name\"],\n",
    "            lr[\"bounds\"][0],\n",
    "            lr[\"bounds\"][-1],\n",
    "            log=True\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            batch_size[\"name\"], \n",
    "            batch_size[\"bounds\"]\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            epochs[\"name\"], \n",
    "            epochs[\"bounds\"]\n",
    "        ))\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_ray[lr[\"name\"]] = tune.loguniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_ray[batch_size[\"name\"]] = tune.choice(batch_size[\"bounds\"])\n",
    "    config_space_ray[epochs[\"name\"]] = tune.choice(categories=epochs[\"bounds\"])\n",
    "    \n",
    "    #start Dragonfly Search Space API\n",
    "    param_list = [\n",
    "        {\n",
    "            \"name\": lr[\"name\"], \n",
    "            \"type\": \"float\", \n",
    "            \"min\": lr[\"bounds\"][0], \n",
    "            \"max\": lr[\"bounds\"][-1]\n",
    "        },\n",
    "        {\n",
    "            \"name\": batch_size[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(2**x) for x in batch_size[\"bounds\"]])\n",
    "        },\n",
    "        {\n",
    "            \"name\": epochs[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(10*x) for x in epochs[\"bounds\"]])\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    #start BayesOpt Search Space API\n",
    "    config_space_dict[lr[\"name\"]] = tune.uniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_dict[batch_size[\"name\"]] = tune.uniform(lower=batch_size[\"bounds\"][0], upper=batch_size[\"bounds\"][-1])\n",
    "    config_space_dict[epochs[\"name\"]] = tune.uniform(lower=epochs[\"bounds\"][0], upper=epochs[\"bounds\"][-1])\n",
    "    \n",
    "    #start Discrete Search Search Space API\n",
    "    param_dict = {p[\"name\"]:p[\"bounds\"] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    #start PB2 Space API\n",
    "    min_max_param_dict = {p[\"name\"]:[p[\"bounds\"][0], p[\"bounds\"][-1]] for p in [lr,batch_size,epochs]}\n",
    "#     bounds = [\n",
    "#         [lr[\"name\"],[lr[\"bounds\"][0], lr[\"bounds\"][-1]]],\n",
    "#         [batch_size[\"name\"],[batch_size[\"bounds\"][0], batch_size[\"bounds\"][-1]]],\n",
    "#         [epochs[\"name\"], [epochs[\"bounds\"][0], epochs[\"bounds\"][-1]]]\n",
    "#     ]\n",
    "#     for [n,b] in bounds:\n",
    "#         param_dict[n] = [b[0],b[1]]\n",
    "\n",
    "#     print(discrete_param_dict)\n",
    "#     print(param_dict)\n",
    "    \n",
    "    return config_space_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(search_training_hyperparameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "def nas_report(study,trial):\n",
    "#     for n in [study,trial]:\n",
    "#         print(\"Start\")\n",
    "#         for i in inspect.getmembers(n):\n",
    "#             if not i[0].startswith('_'): \n",
    "#                 if not inspect.ismethod(i[1]):  \n",
    "#                     print(i)\n",
    "#         print(\"End\")\n",
    "#     best_session = study.get_pareto_front_trials()[0]\n",
    "    best_session = study.best_trials[0]\n",
    "#     print(\"Best trial stats so far for study: \", best_session[\"train_params\"])\n",
    "    print(\"Trial stats (#{}):    Loss={}    Accuracy={}\".format(trial.number,*(list(best_session.values))))\n",
    "    print(\"Best params so far (#{}):    {}\".format(best_session.number,best_session.params))\n",
    "#     print(trial.params.items())\n",
    "#     print(\"  Loss=%s    Accuracy=%s\" % tuple(trial.values))\n",
    "\n",
    "    finished_trials = list(filter(\n",
    "        (lambda trial: trial.state.is_finished()),\n",
    "        study.trials\n",
    "    ))\n",
    "\n",
    "    model_state = {}\n",
    "    with tune.checkpoint_dir(step=best_session.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state = torch.load(path)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "#         path = os.path.join(study.study_name, checkpoint_dir)\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                best_session.params,\n",
    "                model_state#,\n",
    "#                 finished_trials\n",
    "                \n",
    "#                 best_session.user_attrs[\"optimizer\"]\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "\n",
    "    \n",
    "#     print(\"Best trial:\")\n",
    "#     trial = study.best_trial if (type(study) == optuna.study.Study) else study.get_pareto_front_trials()[0]\n",
    "# #     return trial\n",
    "#     print(\"  Loss: %s  Accuracy: %s\" % tuple(trial.values))\n",
    "    \n",
    "#     print(\"  Params: \")\n",
    "#     for (key, value) in trial.params.items():\n",
    "#         print(\"    {}: {}\".format(key, value))\n",
    "#     tune.report(loss=loss,accuracy=accuracy,model=trial)\n",
    "    result_zip = zip([\"loss\",\"accuracy\"], list(best_session.values))\n",
    "#     tune.report(loss=trial.values[0], accuracy=trial.values[1], model=trial.params)\n",
    "    results = {p:v for p,v in result_zip}\n",
    "#     results[\"trial\"] = num_samples*10\n",
    "    tune.report(**results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neural_arch(non_arch_config,checkpoint_dir=None):\n",
    "\n",
    "#     from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, select_length, UCB, standardize, TV_SquaredExp\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"maximize\",\n",
    "#         sampler=TPESampler(multivariate=True),\n",
    "#         pruner=SuccessiveHalvingPruner(reduction_factor=2)\n",
    "#     )\n",
    "#     print(non_arch_config)\n",
    "#     training_hyperparameters = {hp:v for hp,v in zip([\"lr\",\"batch_size\",\"epochs\"],non_arch_config[\"point\"])}\n",
    "    \n",
    "#     storage = optuna.storages.RedisStorage(\n",
    "#         url=\"redis://127.0.0.1:6380\"\n",
    "#     )\n",
    "    \n",
    "#     print(optuna.logging.get_verbosity())\n",
    "    optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "#     print(optuna.logging.get_verbosity())\n",
    "\n",
    "#     optuna.logging.disable_default_handler()\n",
    "    \n",
    "#     study = optuna.multi_objective.create_study(\n",
    "    study = optuna.create_study(\n",
    "        directions=[\"minimize\",\"maximize\"],\n",
    "        study_name=str(non_arch_config),\n",
    "#         sampler=MOTPEMultiObjectiveSampler(),\n",
    "        sampler=BoTorchSampler(),\n",
    "        pruner=SuccessiveHalvingPruner(),\n",
    "        storage='sqlite:///na.db',\n",
    "#         storage=storage,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "#     storage.set_study_directions(0,[optuna.study.StudyDirection.MAXIMIZE])\n",
    "    \n",
    "#     print(\"num_samples: \", num_samples)\n",
    "#     n_trials = num_samples\n",
    "#     if checkpoint_dir:\n",
    "#         checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         arch, finished_trials = torch.load(checkpoint)\n",
    "#         for trial in finished_trials:\n",
    "#             study.add_trial(trial)\n",
    "#             n_trials -= 1\n",
    "#         print(\"Resumed trial list: \",study.get_trials())\n",
    "            \n",
    "    \n",
    "#     if checkpoint_dir:\n",
    "#         checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         model_state, optimizer_state = torch.load(checkpoint)\n",
    "#         net.load_state_dict(model_state)\n",
    "#         optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    \n",
    "    study.optimize(\n",
    "        partial(train_cifar, non_arch_config),\n",
    "        n_trials=oom,\n",
    "#         n_jobs=2,\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[nas_report]#,\n",
    "#         timeout=60\n",
    "    )\n",
    "    \n",
    "#     best_session = study.get_pareto_front_trials()[0]\n",
    "#     result_zip = zip([\"loss\",\"accuracy\",\"best trial\"], train_cifar(non_arch_config,best_session))\n",
    "# #     study.optimize((lambda trial: train_cifar(trial, training_hyperparameters, checkpoint_dir)), n_trials=num_samples, gc_after_trial=True)\n",
    "# #     study.optimize((lambda trial: train_cifar(trial, training_hyperparameters, target)), n_trials=1)\n",
    "# #     return {\"loss\":trial.values[0], \"accuracy\":trial.values[1], \"model\":7}#trial.params}\n",
    "#     results = {p:v for p,v in result_zip}\n",
    "# #     results[\"best trial\"] = num_samples\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoke_test = {\"learning rate {10^(-⌊x⌋)\": 2.0, \"batch size {2^⌊x⌋}\": 7.0, \"epochs {10⌊x⌋}\": 1.0}\n",
    "res = search_neural_arch(smoke_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import inspect \n",
    "res.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key=[\"batch_size\",\"epochs\",\"lr\"]\n",
    "val=[4,1,0.000522636]\n",
    "k_to_v = {k:v for k,v in zip(key,val)}\n",
    "res = search_neural_arch({\"point\":val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print(res[0].values)\n",
    "[print({i:v}) for (i,v) in list(res.items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mdl = Net(res[\"model\"])\n",
    "print(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform neuron configuration trials\n",
    "def search_neurons():\n",
    "#     ray.init(address=\"auto\")\n",
    "    neuron_config_space = search_training_hyperparameters()\n",
    "    \n",
    "#     param_dict = {\"name\": \"training_hyperparameters\", \"domain\": neuron_config_space}\n",
    "#     domain_config = load_config(param_dict)\n",
    "#     domain, domain_orderings = domain_config.domain, domain_config.domain_orderings\n",
    "\n",
    "#     # define the hpo search algorithm BO\n",
    "#     func_caller = CPFunctionCaller(None, domain, domain_orderings=domain_orderings)\n",
    "#     optimizer = CPGPBandit(func_caller, ask_tell_mode=True)\n",
    "# #     bo_search_alg = DragonflySearch(optimizer, metric=\"validation_mae\", mode=\"min\")\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "#     hpn = [p[\"name\"] for p in neuron_config_space]\n",
    "#     hpn = neuron_config_space.get_hyperparameter_names()\n",
    "    hpn = list(neuron_config_space.keys())\n",
    "#     hpn.append(\"model\")\n",
    "    \n",
    "    #pre-load data to avoid races\n",
    "    load_data()\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=oom,\n",
    "        reduction_factor=2,\n",
    "#         grace_period=3,\n",
    "        **experiment_metrics)\n",
    "#     scheduler = PB2(\n",
    "#         time_attr=\"training_iteration\",\n",
    "#         perturbation_interval=1,#max_time/10,\n",
    "# #         quantile_fraction=0.5,\n",
    "#         hyperparam_bounds=neuron_config_space,\n",
    "#         synch=True,\n",
    "#         **experiment_metrics)\n",
    "    search = BayesOptSearch(\n",
    "#         neuron_config_space,\n",
    "        **experiment_metrics)\n",
    "    search = ConcurrencyLimiter(\n",
    "        search,\n",
    "        max_concurrent=max_concurrent_trials)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=hpn,\n",
    "#         max_progress_rows=num_samples,\n",
    "        max_report_frequency=10,\n",
    "#         print_intermediate_tables=True,\n",
    "#         metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "        **experiment_metrics)\n",
    "#     distributed_run = DistributedTrainableCreator(\n",
    "#         search_neural_arch,\n",
    "# #         partial(search_neural_arch, data_dir=r.absolute()),\n",
    "#         num_workers=2,\n",
    "#         num_cpus_per_worker=1,\n",
    "#         num_gpus_per_worker=1,\n",
    "#         backend='nccl'\n",
    "#     )\n",
    "    result = tune.run(\n",
    "#         distributed_run,\n",
    "        search_neural_arch,\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "#         max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        config=neuron_config_space,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    \n",
    "# #     def cv_discrim(s): return 'conv_subindex_' in s\n",
    "# #     def fc_discrim(s): return 'full_subindex_' in s\n",
    "#     def other_discrim(s): return 'subindex' not in s\n",
    "# #     best_cvs = [best_trial.config[hp] for hp in list(filter(cv_discrim, best_trial.config.keys()))]\n",
    "# #     best_fcs = [best_trial.config[hp] for hp in list(filter(fc_discrim, best_trial.config.keys()))]\n",
    "#     best_other = [best_trial.config[hp] for hp in list(filter(other_discrim, best_trial.config.keys()))]\n",
    "\n",
    "# #     cfg = decode([best_cvs, best_fcs])\n",
    "    \n",
    "# #     conv_report = [\"Connolutional Layer %s: %s\" % (i,c) for i,c in enumerate(cfg[0],1)]\n",
    "# #     full_report = [\"Fully-connected Layer %s: %s\" % (i,f) for i,f in enumerate(cfg[1],1)]\n",
    "# #     other_report = [\"%s: %s\" % (hp,f) for (hp,f) in zip([\"Batch Size\",\"Epochs\",\"Learning Rate\"],best_other)]\n",
    "    \n",
    "# # #     best_trained_model = Net(best_trial.last_result[\"model\"])\n",
    "# #     best_trained_model = best_trial.last_result[\"model\"]\n",
    "#     res = best_trial\n",
    "#     return res\n",
    "\n",
    "#     best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best training hyperparameters: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    arch_state, model_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "\n",
    "# #     best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "#     best_trained_model = model_cifar(best_trial.config,arch_state)\n",
    "    best_trained_model = Net(arch_state)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "#         if gpus > 1:\n",
    "#             best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform test\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    model = search_layers(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
