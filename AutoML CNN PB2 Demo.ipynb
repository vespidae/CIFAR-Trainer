{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial # for trials\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "\n",
    "import os # for paths\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "import numpy as np # for accuracy math\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "# from ray.tune.integration.torch import is_distributed_trainable\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "# from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "# from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import BoTorchSampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "d = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count() # number of cpu cores\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus\n",
    "cpu_use = 1 # number of cpu cores to dedicate to 1 series of trials\n",
    "gpu_use = gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "oom = 6 if gpus else 2 # order of magnitude\n",
    "num_samples = 2 ** oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data into sets for loading\n",
    "def load_data(data_dir=d.absolute()):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset,testset = [torchvision.datasets.CIFAR10(root=data_dir, train=is_train, download=True, transform=transform) for is_train in [True,False]]\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    n_convs = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    n_fulls = trial.suggest_int(\"n_full_layers\", 1, 4)\n",
    "\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(n_convs):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = trial.suggest_int(\"n_conv_channels_c{}\".format(i), *[3**x for x in [2,5]])\n",
    "            kernel_size = trial.suggest_int(\"kernel_size_c{}\".format(i),2,5)\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if trial.suggest_int(\"has_max_pool_c{}\".format(i),0,1) & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(n_fulls):\n",
    "        out_features = trial.suggest_int(\"n_l_units_l{}\".format(i), *[2**x for x in [2,6]])\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if trial.suggest_int(\"has_dropout_l{}\".format(i),0,1):\n",
    "            p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(arch):\n",
    "    layers = []\n",
    "    pre_flat_size = 32\n",
    "    in_channels = 3\n",
    "    out_kernel = None\n",
    "\n",
    "    for i in range(arch[\"n_conv_layers\"]):\n",
    "        if pre_flat_size > 7:\n",
    "            out_channels = arch[\"n_conv_channels_c%s\" % i]\n",
    "            kernel_size = arch[\"kernel_size_c%s\" % i]\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            pre_flat_size = pre_flat_size - kernel_size+1\n",
    "            if arch[\"has_max_pool_c%s\" % i] & pre_flat_size > 3:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "                pre_flat_size = int(pre_flat_size / 2)\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        in_channels = out_channels\n",
    "        out_kernel = kernel_size\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    in_features = in_channels * pre_flat_size**2\n",
    "    for i in range(arch[\"n_full_layers\"]):\n",
    "        out_features = arch[\"n_l_units_l%s\" % i]\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        if arch[\"has_dropout_l%s\" % i]:\n",
    "            p = arch[\"dropout_l%s\" % i]\n",
    "            layers.append(nn.Dropout(p))\n",
    "        layers.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn on data\n",
    "def train_cifar(non_arch_config,trial):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "\n",
    "    net = define_model(trial) if type(trial) == optuna.trial.Trial else Net(trial.params)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", trial.number, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                net.state_dict()\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "    return [loss,accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model nn based on HPO\n",
    "def model_cifar(non_arch_config,arch_config):\n",
    "    loss,accuracy = 0,0\n",
    "    lr = 10**-(non_arch_config[\"learning rate {10^(-⌊x⌋)\"])\n",
    "    batch_size = 2**int(non_arch_config[\"batch size {2^⌊x⌋}\"])\n",
    "    epochs = 10*int(non_arch_config[\"epochs {10⌊x⌋}\"])\n",
    "    \n",
    "    print(arch_config)\n",
    "    net = Net(arch_config)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    net.to(device)\n",
    "    if gpus > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader,valloader = [torch.utils.data.DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2) for subset in [train_subset,val_subset]]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "            \n",
    "        loss = (val_loss / val_steps)\n",
    "        accuracy = (correct / total)\n",
    "        print(\"HP: \", non_arch_config,\"\\n\", \"Trial/Epoch: \", Test, \"/\", epoch, \"Loss/Accuracy: \", loss,\"/\",accuracy)\n",
    "        \n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    _, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_training_hyperparameters():\n",
    "    lr = {\n",
    "        \"name\":\"learning rate {10^(-⌊x⌋)\",\n",
    "        \"bounds\":[x for x in range(1,4)]\n",
    "    }\n",
    "    batch_size = {\n",
    "        \"name\":\"batch size {2^⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(6,9)]\n",
    "    }\n",
    "    epochs = {\n",
    "        \"name\":\"epochs {10⌊x⌋}\",\n",
    "        \"bounds\":[x for x in range(2,10)]\n",
    "    }\n",
    "    \n",
    "    config_space = CS.ConfigurationSpace()\n",
    "    config_space_dict,config_space_ray = {},{}\n",
    "    \n",
    "    #start ConfigSpace API\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            lr[\"name\"],\n",
    "            lr[\"bounds\"][0],\n",
    "            lr[\"bounds\"][-1],\n",
    "            log=True\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            batch_size[\"name\"], \n",
    "            batch_size[\"bounds\"]\n",
    "        ))\n",
    "    config_space.add_hyperparameter(\n",
    "        CS.CategoricalHyperparameter(\n",
    "            epochs[\"name\"], \n",
    "            epochs[\"bounds\"]\n",
    "        ))\n",
    "    \n",
    "    #start Ray Search Space API\n",
    "    config_space_ray[lr[\"name\"]] = tune.loguniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_ray[batch_size[\"name\"]] = tune.choice(batch_size[\"bounds\"])\n",
    "    config_space_ray[epochs[\"name\"]] = tune.choice(categories=epochs[\"bounds\"])\n",
    "    \n",
    "    #start Dragonfly Search Space API\n",
    "    param_list = [\n",
    "        {\n",
    "            \"name\": lr[\"name\"], \n",
    "            \"type\": \"float\", \n",
    "            \"min\": lr[\"bounds\"][0], \n",
    "            \"max\": lr[\"bounds\"][-1]\n",
    "        },\n",
    "        {\n",
    "            \"name\": batch_size[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(2**x) for x in batch_size[\"bounds\"]])\n",
    "        },\n",
    "        {\n",
    "            \"name\": epochs[\"name\"], \n",
    "            \"type\": \"discrete_numeric\", \n",
    "            \"items\": \":\".join([str(10*x) for x in epochs[\"bounds\"]])\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    #start BayesOpt Search Space API\n",
    "    config_space_dict[lr[\"name\"]] = tune.uniform(lr[\"bounds\"][0],lr[\"bounds\"][-1])\n",
    "    config_space_dict[batch_size[\"name\"]] = tune.uniform(lower=batch_size[\"bounds\"][0], upper=batch_size[\"bounds\"][-1])\n",
    "    config_space_dict[epochs[\"name\"]] = tune.uniform(lower=epochs[\"bounds\"][0], upper=epochs[\"bounds\"][-1])\n",
    "    \n",
    "    #start Discrete Search Search Space API\n",
    "    param_dict = {p[\"name\"]:p[\"bounds\"] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    #start PB2 Space API\n",
    "    min_max_param_dict = {p[\"name\"]:[p[\"bounds\"][0], p[\"bounds\"][-1]] for p in [lr,batch_size,epochs]}\n",
    "    \n",
    "    return config_space_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "def nas_report(study,trial):\n",
    "    best_session = study.best_trials[0]\n",
    "    print(\"Trial stats (#{}):    Loss={}    Accuracy={}\".format(trial.number,*(list(best_session.values))))\n",
    "    print(\"Best params so far (#{}):    {}\".format(best_session.number,best_session.params))\n",
    "\n",
    "    finished_trials = list(filter(\n",
    "        (lambda trial: trial.state.is_finished()),\n",
    "        study.trials\n",
    "    ))\n",
    "\n",
    "    model_state = {}\n",
    "    with tune.checkpoint_dir(step=best_session.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state = torch.load(path)\n",
    "\n",
    "    with tune.checkpoint_dir(step=trial.number) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save(\n",
    "            (\n",
    "                best_session.params,\n",
    "                model_state\n",
    "            ),\n",
    "            path\n",
    "        )\n",
    "\n",
    "    \n",
    "    result_zip = zip([\"loss\",\"accuracy\"], list(best_session.values))\n",
    "    results = {p:v for p,v in result_zip}\n",
    "    tune.report(**results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neural_arch(non_arch_config,checkpoint_dir=None):\n",
    "\n",
    "    optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions=[\"minimize\",\"maximize\"],\n",
    "        study_name=str(non_arch_config),\n",
    "        sampler=BoTorchSampler(),\n",
    "        pruner=SuccessiveHalvingPruner(),\n",
    "#         storage='sqlite:///na.db',\n",
    "        storage=\"mysql://root@localhost/example\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        partial(train_cifar, non_arch_config),\n",
    "        n_trials=oom,\n",
    "        n_jobs=4,\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[nas_report]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_pod = {}\n",
    "# perform neuron configuration trials\n",
    "def search_neurons():\n",
    "    neuron_config_space = search_training_hyperparameters()\n",
    "    \n",
    "    experiment_metrics = dict(metric=\"accuracy\", mode=\"max\")\n",
    "\n",
    "    hpn = list(neuron_config_space.keys())\n",
    "    \n",
    "    #pre-load data to avoid races\n",
    "    load_data()\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=oom,\n",
    "        reduction_factor=2,\n",
    "#         grace_period=3,\n",
    "        **experiment_metrics)\n",
    "    search = BayesOptSearch(\n",
    "        **experiment_metrics)\n",
    "    search = ConcurrencyLimiter(\n",
    "        search,\n",
    "        max_concurrent=max_concurrent_trials)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        parameter_columns=hpn,\n",
    "#         max_progress_rows=num_samples,\n",
    "        max_report_frequency=10,\n",
    "        **experiment_metrics)\n",
    "    result = tune.run(\n",
    "        search_neural_arch,\n",
    "        verbose=3,\n",
    "        name=\"neurons\",\n",
    "        local_dir=r.absolute(),\n",
    "        resources_per_trial={\"cpu\": cpu_use, \"gpu\": gpu_use},\n",
    "        max_failures=3,\n",
    "        num_samples=num_samples,\n",
    "        config=neuron_config_space,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search,\n",
    "        queue_trials=True,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    escape_pod = best_trial\n",
    "    \n",
    "\n",
    "    print(\"Best training hyperparameters: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    first, second = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "    \n",
    "    arch_state,model_state = {},{}\n",
    "    if (type(second) == tuple): \n",
    "        arch_state,model_state = second\n",
    "    else:\n",
    "        arch_state,model_state = first,second\n",
    "\n",
    "    best_trained_model = Net(arch_state)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\n"
     ]
    }
   ],
   "source": [
    "print(\"Resource usage can be viewed at port http://127.0.0.1:8265/ or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/62.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 2/8 CPUs, 0.24/1 GPUs, 0.0/30.62 GiB heap, 0.0/10.55 GiB objects (0/1.0 accelerator_type:G)<br>Result logdir: /home/grottesco/Source/CIFAR-Trainer/ray_results/neurons<br>Number of trials: 10/64 (8 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  learning rate {10^(-⌊x⌋)</th><th style=\"text-align: right;\">  batch size {2^⌊x⌋}</th><th style=\"text-align: right;\">  epochs {10⌊x⌋}</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>search_neural_arch_d032f250</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                   1.39935</td><td style=\"text-align: right;\">             6.91214</td><td style=\"text-align: right;\">         7.49623</td></tr>\n",
       "<tr><td>search_neural_arch_d054ac74</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                   1.0929 </td><td style=\"text-align: right;\">             7.02847</td><td style=\"text-align: right;\">         6.1469 </td></tr>\n",
       "<tr><td>search_neural_arch_c7c15ada</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   2.46399</td><td style=\"text-align: right;\">             6.74908</td><td style=\"text-align: right;\">         8.655  </td></tr>\n",
       "<tr><td>search_neural_arch_c7c49c9a</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   1.31199</td><td style=\"text-align: right;\">             7.19732</td><td style=\"text-align: right;\">         3.09213</td></tr>\n",
       "<tr><td>search_neural_arch_c7c588b2</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   2.20223</td><td style=\"text-align: right;\">             6.11617</td><td style=\"text-align: right;\">         8.06323</td></tr>\n",
       "<tr><td>search_neural_arch_c7c69798</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   2.93982</td><td style=\"text-align: right;\">             7.41615</td><td style=\"text-align: right;\">         2.14409</td></tr>\n",
       "<tr><td>search_neural_arch_c7c79af8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   1.36365</td><td style=\"text-align: right;\">             7.66489</td><td style=\"text-align: right;\">         3.48637</td></tr>\n",
       "<tr><td>search_neural_arch_c7c9e8f8</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   2.04951</td><td style=\"text-align: right;\">             6.36681</td><td style=\"text-align: right;\">         4.1297 </td></tr>\n",
       "<tr><td>search_neural_arch_c7cb182c</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   2.22371</td><td style=\"text-align: right;\">             6.86389</td><td style=\"text-align: right;\">         4.0386 </td></tr>\n",
       "<tr><td>search_neural_arch_c7d3c576</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">                   1.73272</td><td style=\"text-align: right;\">             6.27899</td><td style=\"text-align: right;\">         4.04501</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 10<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>search_neural_arch_c7c15ada</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c15ada_1_batch size {2^⌊x⌋}=6.7491,epochs {10⌊x⌋}=8.655,learning rate {10^(-⌊x⌋)=2.464_2021-08-25_07-08-30/error.txt   </td></tr>\n",
       "<tr><td>search_neural_arch_c7c49c9a</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c49c9a_2_batch size {2^⌊x⌋}=7.1973,epochs {10⌊x⌋}=3.0921,learning rate {10^(-⌊x⌋)=1.312_2021-08-25_07-08-30/error.txt  </td></tr>\n",
       "<tr><td>search_neural_arch_c7c588b2</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c588b2_3_batch size {2^⌊x⌋}=6.1162,epochs {10⌊x⌋}=8.0632,learning rate {10^(-⌊x⌋)=2.2022_2021-08-25_07-08-30/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_c7c69798</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c69798_4_batch size {2^⌊x⌋}=7.4161,epochs {10⌊x⌋}=2.1441,learning rate {10^(-⌊x⌋)=2.9398_2021-08-25_07-08-30/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_c7c79af8</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c79af8_5_batch size {2^⌊x⌋}=7.6649,epochs {10⌊x⌋}=3.4864,learning rate {10^(-⌊x⌋)=1.3636_2021-08-25_07-08-30/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_c7c9e8f8</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7c9e8f8_6_batch size {2^⌊x⌋}=6.3668,epochs {10⌊x⌋}=4.1297,learning rate {10^(-⌊x⌋)=2.0495_2021-08-25_07-08-30/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_c7cb182c</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7cb182c_7_batch size {2^⌊x⌋}=6.8639,epochs {10⌊x⌋}=4.0386,learning rate {10^(-⌊x⌋)=2.2237_2021-08-25_07-08-30/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_c7d3c576</td><td style=\"text-align: right;\">           4</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_c7d3c576_8_batch size {2^⌊x⌋}=6.279,epochs {10⌊x⌋}=4.045,learning rate {10^(-⌊x⌋)=1.7327_2021-08-25_07-08-30/error.txt   </td></tr>\n",
       "<tr><td>search_neural_arch_d032f250</td><td style=\"text-align: right;\">           3</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_d032f250_9_batch size {2^⌊x⌋}=6.9121,epochs {10⌊x⌋}=7.4962,learning rate {10^(-⌊x⌋)=1.3993_2021-08-25_07-08-44/error.txt </td></tr>\n",
       "<tr><td>search_neural_arch_d054ac74</td><td style=\"text-align: right;\">           2</td><td>/home/grottesco/Source/CIFAR-Trainer/ray_results/neurons/search_neural_arch_d054ac74_10_batch size {2^⌊x⌋}=7.0285,epochs {10⌊x⌋}=6.1469,learning rate {10^(-⌊x⌋)=1.0929_2021-08-25_07-08-45/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 07:08:51,283\tERROR trial_runner.py:607 -- Trial search_neural_arch_d054ac74: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15132, ip=10.109.31.194)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15132, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "    self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "    return strategy.create(*args, **kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "    dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "    return __import__(\"MySQLdb\")\n",
      "ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15132, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "    storage = storages.get_storage(storage)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "    return _CachedStorage(RDBStorage(storage))\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "    raise ImportError(\n",
      "ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "2021-08-25 07:08:51,285\tINFO trial_runner.py:781 -- Trial search_neural_arch_d054ac74: Attempting to restore trial state from last checkpoint.\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m <ipython-input-11-ae7e80119b3b>:8: ExperimentalWarning: BoTorchSampler is experimental (supported from v2.4.0). The interface can change in the future.\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m 2021-08-25 07:08:52,843\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return strategy.create(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return __import__(\"MySQLdb\")\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     storage = storages.get_storage(storage)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return _CachedStorage(RDBStorage(storage))\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     raise ImportError(\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return strategy.create(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return __import__(\"MySQLdb\")\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/opt/anaconda/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     storage = storages.get_storage(storage)\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     return _CachedStorage(RDBStorage(storage))\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m     raise ImportError(\n",
      "\u001b[2m\u001b[36m(pid=15137)\u001b[0m ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m <ipython-input-11-ae7e80119b3b>:8: ExperimentalWarning: BoTorchSampler is experimental (supported from v2.4.0). The interface can change in the future.\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m 2021-08-25 07:08:52,867\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return strategy.create(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return __import__(\"MySQLdb\")\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     storage = storages.get_storage(storage)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return _CachedStorage(RDBStorage(storage))\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     raise ImportError(\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return strategy.create(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return __import__(\"MySQLdb\")\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/opt/anaconda/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     storage = storages.get_storage(storage)\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     return _CachedStorage(RDBStorage(storage))\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m   File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m     raise ImportError(\n",
      "\u001b[2m\u001b[36m(pid=15129)\u001b[0m ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "2021-08-25 07:08:53,041\tERROR trial_runner.py:607 -- Trial search_neural_arch_d054ac74: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15137, ip=10.109.31.194)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15137, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "    self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "    return strategy.create(*args, **kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "    dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "    return __import__(\"MySQLdb\")\n",
      "ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15137, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "    storage = storages.get_storage(storage)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "    return _CachedStorage(RDBStorage(storage))\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "    raise ImportError(\n",
      "ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "2021-08-25 07:08:53,060\tERROR trial_runner.py:607 -- Trial search_neural_arch_d032f250: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15129, ip=10.109.31.194)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15129, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 140, in __init__\n",
      "    self.engine = create_engine(self.url, **self.engine_kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/__init__.py\", line 520, in create_engine\n",
      "    return strategy.create(*args, **kwargs)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/engine/strategies.py\", line 87, in create\n",
      "    dbapi = dialect_cls.dbapi(**dbapi_args)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\", line 118, in dbapi\n",
      "    return __import__(\"MySQLdb\")\n",
      "ModuleNotFoundError: No module named 'MySQLdb'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=15129, ip=10.109.31.194)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"<ipython-input-11-ae7e80119b3b>\", line 5, in search_neural_arch\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/study.py\", line 810, in create_study\n",
      "    storage = storages.get_storage(storage)\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/__init__.py\", line 27, in get_storage\n",
      "    return _CachedStorage(RDBStorage(storage))\n",
      "  File \"/home/grottesco/envs/bl-ed/lib/python3.8/site-packages/optuna/storages/_rdb/storage.py\", line 142, in __init__\n",
      "    raise ImportError(\n",
      "ImportError: Failed to import DB access module for the specified storage URL. Please install appropriate one.\n",
      "2021-08-25 07:08:53,062\tINFO trial_runner.py:842 -- Blocking for next trial...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for search_neural_arch_d054ac74:\n",
      "  {}\n",
      "  \n",
      "Result for search_neural_arch_d032f250:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 07:08:54,063\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:08:55,067\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:08:56,069\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:08:57,071\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:08:58,074\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:08:59,078\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:00,081\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:01,084\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:02,088\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:03,092\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:04,095\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:05,100\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:06,102\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:07,106\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:08,110\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:09,114\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:10,118\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:11,121\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:12,125\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:13,128\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:14,131\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:15,134\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:16,136\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:17,140\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:18,144\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:19,146\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:20,149\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:21,154\tINFO trial_runner.py:842 -- Blocking for next trial...\n",
      "2021-08-25 07:09:22,157\tINFO trial_runner.py:842 -- Blocking for next trial...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97e1e1f81862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8bac03fdeda9>\u001b[0m in \u001b[0;36msearch_neurons\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmax_report_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         **experiment_metrics)\n\u001b[0;32m---> 29\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0msearch_neural_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m             self._callbacks.on_step_begin(\n\u001b[1;32m    350\u001b[0m                 iteration=self._iteration, trials=self._trials)\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mnext_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext_trial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start_trial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_get_next_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPENDING\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mwait_for_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_trial_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"choose_trial_to_run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scheduler_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_trial_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/bl-ed/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_update_trial_queue\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Blocking for next trial...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = search_neurons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
