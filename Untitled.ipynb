{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brave-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # for trials\n",
    "import numpy as np # for accuracy math\n",
    "import os # for paths\n",
    "import torch # for nn instantiation\n",
    "import torch.nn as nn # for nn objects\n",
    "import torch.nn.functional as F # for forward method\n",
    "import torch.optim as optim # for optimization\n",
    "from torch.utils.data import random_split # for train/test split\n",
    "import torchvision # for data transforms\n",
    "import torchvision.transforms as transforms # for transform methods\n",
    "import ray\n",
    "from ray import tune # for trialing\n",
    "# from ray.tune import CLIReporter # for trial reporting\n",
    "from ray.tune import JupyterNotebookReporter # for trial reporting\n",
    "from ray.tune.integration.torch import is_distributed_trainable\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from ray.tune.integration.torch import DistributedTrainableCreator\n",
    "from ray.tune.integration.torch import distributed_checkpoint_dir\n",
    "from ray.tune.schedulers import ASHAScheduler # for trial scheduling\n",
    "# from ray.tune.schedulers import HyperBandForBOHB # for trial scheduling\n",
    "# from ray.tune.suggest.bohb import TuneBOHB # for trial selection/pruning\n",
    "# from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "# from dragonfly.opt.gp_bandit import CPGPBandit\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# from dragonfly import load_config\n",
    "# from dragonfly.exd.experiment_caller import CPFunctionCaller, EuclideanFunctionCaller\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import GPy\n",
    "import sklearn\n",
    "from ray.tune.schedulers import pb2\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "import ConfigSpace as CS # for configuration bounds\n",
    "from collections import OrderedDict # for dynamic configuration definition\n",
    "from pathlib import Path # for OS agnostic path definition\n",
    "\n",
    "# import itertools package \n",
    "import itertools \n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from itertools import product\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# allow configuration copying\n",
    "from copy import deepcopy\n",
    "\n",
    "import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "from optuna.multi_objective.samplers import MOTPEMultiObjectiveSampler\n",
    "# from optuna.pruners import SuccessiveHalvingPruner\n",
    "\n",
    "# from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, select_length, UCB, standardize, TV_SquaredExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set data and checkpoint locations\n",
    "# p = Path('.')\n",
    "# d = p / 'data'\n",
    "# r = p / 'ray_results'\n",
    "# l = p / 'checkpoints' / 'layers'\n",
    "# n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# # set computation location(s)\n",
    "# gpus = torch.cuda.device_count()\n",
    "# device = \"cuda:0\" if gpus else \"cpu\"\n",
    "\n",
    "# # set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# # cpu_use must be grater than zero\n",
    "# cpu_use = 1 if gpus else 0.5\n",
    "# gpu_use = 0.25 if gpus else 0\n",
    "\n",
    "# # set experiment hyperparameters\n",
    "# num_samples = 2 ** (6 if gpus else 4)\n",
    "# max_time = 10 * (4 if gpus else 1)\n",
    "# gpus_per_trial = 0.5 if gpus else 0\n",
    "\n",
    "# set data and checkpoint locations\n",
    "p = Path('.')\n",
    "dt = p / 'data'\n",
    "r = p / 'ray_results'\n",
    "l = p / 'checkpoints' / 'layers'\n",
    "n = p / 'checkpoints' / 'layers'\n",
    "\n",
    "# set computation location(s)\n",
    "cpus = os.cpu_count()\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "# set number or fraction of processing units (per training worker) you'd like to utilize, if any at all\n",
    "# cpu_use must be grater than zero\n",
    "max_concurrent_trials = cpus#int(cpus/2) if cpus > 1 else cpus\n",
    "cpu_use = 2#2 / gpus if gpus else 1\n",
    "gpu_use = 0.5#gpus/max_concurrent_trials if gpus else 0\n",
    "\n",
    "# set experiment hyperparameters\n",
    "# oom = 2 if gpus else 3 # order of magnitude\n",
    "num_samples = 2 / gpu_use#** oom\n",
    "max_time = 10# * oom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-barbados",
   "metadata": {},
   "source": [
    "Since the neuron configuration we want is dependent upon the number of layers we have, we need to work flatten the feature space a bit. We can reduce the high-dminesional setups to a slightly less high-dminesional string of base-n nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature space for hashing\n",
    "c_min = 3**2\n",
    "c_max = 3**5\n",
    "k_min = 2\n",
    "k_max = 5\n",
    "m_min = 0\n",
    "m_max = 1\n",
    "f_min = 2**2\n",
    "f_max = 2**6\n",
    "d_min = 5\n",
    "d_max = 11\n",
    "\n",
    "c = c_max - c_min # convolutional layer options\n",
    "k = k_max - k_min # kernel options\n",
    "m = m_max - m_min # max pool layer options\n",
    "f = f_max - f_min # fully connected layer options\n",
    "d = d_max - d_min # dropout layer options\n",
    "\n",
    "# conv = set(range(c_max)) - set(range(c_min))\n",
    "# full = set(range(f_max)) - set(range(f_min))\n",
    "conv = list(range(c_max)[c_min:])\n",
    "kern = list(range(k_max)[k_min:])\n",
    "maxp = list(range(m_max)[m_min:])\n",
    "full = list(range(f_max)[f_min:])\n",
    "drop = list(range(d_max)[d_min:])\n",
    "\n",
    "# c_comb = list(combinations_with_replacement(conv,2))\n",
    "c_comb = []\n",
    "k_comb = []\n",
    "m_comb = []\n",
    "f_comb = []\n",
    "d_comb = []\n",
    "\n",
    "for layers in range(1,4):\n",
    "    cm = list(product(conv,kern,maxp,repeat=1))\n",
    "    c_comb += list(combinations_with_replacement(cm,layers))\n",
    "# print(conv)\n",
    "# print(maxp)\n",
    "# print(c_comb)\n",
    "pd.DataFrame(c_comb).to_csv(\"c_comb.csv\")\n",
    "for layers in range(1,2):\n",
    "    m_comb += list(combinations_with_replacement(maxp,layers))\n",
    "for layers in range(1,5):\n",
    "    fd = list(product(full,drop))\n",
    "    f_comb += list(combinations_with_replacement(fd,layers))\n",
    "# print(full)\n",
    "# print(drop)\n",
    "# print(f_comb)\n",
    "pd.DataFrame(f_comb).to_csv(\"f_comb.csv\")\n",
    "for layers in range(1,2):\n",
    "    d_comb += list(combinations_with_replacement(drop,layers))\n",
    "#     print(\"Fully connected layer %s range: %s\" % (layers,len(f_comb)) )\n",
    "#     print(\"\\n\")\n",
    "# [pd.DataFrame(comb).to_csv(\"%s.csv\" % name) for name,comb in zip([\"c_comb\",\"m_comb\",\"f_comb\",\"d_comb\"],[c_comb,m_comb,f_comb,d_comb])]\n",
    "\n",
    "# for conversion from dec to whatever we end up using\n",
    "# most to least significant digit\n",
    "def numberToBase(n, b):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    digits = []\n",
    "    while n:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    rev = digits[::-1]\n",
    "    return rev\n",
    "\n",
    "def feature_spacing():\n",
    "    \n",
    "    # create empty list to store the \n",
    "    # combinations \n",
    "#     conv_combinations = list(combinations([c_comb,m_comb],2))\n",
    "#     print(\"np.shape(conv_combinations[0])\",np.shape(conv_combinations[0]))\n",
    "# #     pd.DataFrame(conv_combinations).to_csv('conv_combinations.csv')\n",
    "#     full_combinations = list(combinations([f_comb,d_comb],2))\n",
    "#     print(\"np.shape(full_combinations[0])\",np.shape(full_combinations[0]))\n",
    "#     pd.DataFrame(full_combinations).to_csv('full_combinations.csv')\n",
    "    unique_combinations = list(product(c_comb,f_comb))\n",
    "    print(\"np.shape(unique_combinations)\",np.shape(unique_combinations))\n",
    "    pd.DataFrame(unique_combinations).to_csv('unique_combinations.csv')\n",
    "#     [pd.DataFrame(comb).to_csv(\"%s.csv\" % name) for name,comb in zip([\"conv_combinations\",\"full_combinations\",\"unique_combinations\"],[conv_combinations,full_combinations,unique_combinations])]\n",
    "    total_uniques = len(unique_combinations)\n",
    "    total_points = total_uniques**2\n",
    "    total_cvs = len(c_comb)\n",
    "    total_krn = len(k_comb)\n",
    "    total_mxp = len(m_comb)\n",
    "    total_fcs = len(f_comb)\n",
    "    total_drp = len(d_comb)\n",
    "    \n",
    "    columns = [\"base\",\"nodes_req\",\"sparcity\",\"sparcity_pcnt\",\"denoise_pcnt\"]\n",
    "    values = [1,total_uniques,total_points - total_uniques,(total_points - total_uniques) / total_points,0]\n",
    "    \n",
    "    cf = []\n",
    "    \n",
    "    for layer in [total_cvs,total_mxp,total_fcs,total_drp]:#,total_uniques]:\n",
    "        results = {\n",
    "            \"base\": [1],\n",
    "            \"nodes_req\": [total_uniques],\n",
    "            \"sparcity\": [total_points - total_uniques],\n",
    "            \"max_necc_base_value\":[0],\n",
    "            \"nodes+_req\": [0],\n",
    "            \"subsparcity\": [0],\n",
    "            \"unexplained\":[0],\n",
    "            \"sparcity_pcnt\": [(total_points - total_uniques) / total_points * 100],\n",
    "            \"subsparcity_pcnt\": [0],\n",
    "            \"denoise_pcnt\":[0],\n",
    "            \"complexity\":[0]\n",
    "        }\n",
    "\n",
    "        report = pd.DataFrame(results)\n",
    "    \n",
    "        for base in range(2,101):\n",
    "            results[\"base\"] = [base]\n",
    "            results[\"nodes_req\"] = [math.ceil(math.log(layer,(base)))]\n",
    "            results[\"nodes+_req\"] = [math.floor(math.log(layer,(base)))]\n",
    "            \n",
    "            results[\"sparcity\"] = [base**math.ceil(math.log(layer,base)) - layer]\n",
    "            results[\"subsparcity\"] = [-(base**math.floor(math.log(layer,base)) - layer)]\n",
    "            \n",
    "            results[\"sparcity_pcnt\"] = [(base**math.ceil(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.ceil(math.log(layer,(base))))*100]\n",
    "            results[\"subsparcity_pcnt\"] = [-((base**math.floor(math.log(layer,(base))) - base**math.log(layer,(base)))/(base**math.floor(math.log(layer,(base))))*100)]\n",
    "            \n",
    "#             results[\"max_necc_base_value\"] = [numberToBase((results[\"base\"][0]**results[\"nodes+_req\"][0]+results[\"subsparcity\"][0]),results[\"base\"][0])]\n",
    "            results[\"max_necc_base_value\"] = [numberToBase(layer,base)]\n",
    "            results[\"unexplained\"] = [(-(base**math.floor(math.log(layer,base)) - layer))*(math.floor(math.log(layer,(base))))]\n",
    "            \n",
    "            results[\"denoise_pcnt\"] = [math.floor(((total_points-(math.ceil(math.log(layer,base)))**2)/total_points)*100)]\n",
    "        \n",
    "            results[\"complexity\"] = [results[\"nodes_req\"][0]*(results[\"sparcity\"][0]+1)]\n",
    "\n",
    "            report = report.append(pd.DataFrame(results))\n",
    "            \n",
    "            \n",
    "        report.index = [x for x in range(1, len(report.values)+1)]\n",
    "        report.drop([1],axis=0,inplace=True)\n",
    "        report.sort_values([\"nodes+_req\",\"sparcity\",\"unexplained\",\"subsparcity\",\"sparcity_pcnt\",\"base\"],inplace=True)\n",
    "        \n",
    "        cf.append(report.iloc[0])\n",
    "    \n",
    "    return cf\n",
    "\n",
    "bases = feature_spacing()\n",
    "[print(r,\"\\n\") for r in bases]\n",
    "\n",
    "base_c = bases[0][\"base\"]\n",
    "base_m = bases[1][\"base\"]\n",
    "base_f = bases[2][\"base\"]\n",
    "base_d = bases[3][\"base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-floating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
